<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[算法-二分查找]]></title>
    <url>%2Fposts%2F2018-09-30-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE.html</url>
    <content type="text"><![CDATA[1、二分查找 查找中间索引，判断是否是要查找的数 &nbsp;&nbsp;1）是，返回中间索引； &nbsp;&nbsp;2）否，判断中间索引的数比目标数大还是小？ &nbsp;&nbsp;&nbsp;&nbsp;a) 比目标数大：则目标数在左侧，将最大数设置为中间索引-1； &nbsp;&nbsp;&nbsp;&nbsp;b) 比目标数小：则目标数在右侧，将最小数设置为中间索引+1； &nbsp;&nbsp;计算中间索引，再次判断中间索引是否是要查找的数，进行循环 &nbsp;&nbsp;当最小数大于最大数的时候，说明没有找到，返回-1。 12345678910111213141516171819202122232425262728293031323334import java.util.*;/*** 二分查找*/public class ArraySelectDemo01&#123; public static void main(String[] args)&#123; int[] arr = &#123;11,33,55,66,88,99&#125;; int index = getIndex(arr, 100); System.out.println(index); &#125; public static int getIndex(int[] arr, int value)&#123; int min = 0; int max = arr.length - 1; int mid = (min + max) &gt;&gt;&gt; 1; while(arr[mid] != value)&#123; if(arr[mid] &gt; value)&#123; max = mid - 1; &#125;else&#123; min = mid + 1; &#125; if(min &gt; max)&#123; return -1; &#125; mid = (min + max) &gt;&gt;&gt; 1; &#125; return mid; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-基数排序]]></title>
    <url>%2Fposts%2F2018-09-25-%E7%AE%97%E6%B3%95-%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(下)]]></title>
    <url>%2Fposts%2F2018-09-23-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8B).html</url>
    <content type="text"><![CDATA[前言 快速排序 堆排序]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>快速排序</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(上)]]></title>
    <url>%2Fposts%2F2018-09-20-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8A).html</url>
    <content type="text"><![CDATA[前言 冒泡排序 插入排序 选择排序]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>冒泡排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-递归]]></title>
    <url>%2Fposts%2F2018-09-18-%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92.html</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-队列]]></title>
    <url>%2Fposts%2F2018-09-16-%E7%AE%97%E6%B3%95-%E9%98%9F%E5%88%97.html</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-栈]]></title>
    <url>%2Fposts%2F2018-09-15-%E7%AE%97%E6%B3%95-%E6%A0%88.html</url>
    <content type="text"><![CDATA[前言]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-怎样写好链表代码]]></title>
    <url>%2Fposts%2F2018-09-13-%E7%AE%97%E6%B3%95-%E6%80%8E%E6%A0%B7%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81.html</url>
    <content type="text"><![CDATA[上一节讲了链表相关的基础知识，有人可能会说基础知识我都掌握了，但是写链表代码还是很费劲怎么办？确实是这样的，想要写好链表代码并不是容易的事，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。 为什么链表代码这么难写？究竟怎么样才能比较轻松的写出正确的链表代码呢？ 只要愿意投入时间，我觉得大多数人都是可以学会的。比如，如果你真能花一整天或者一个周末，就去写链表反转这一个代码，多写几次，知道能毫不费力的写出bug free的代码，这个坎儿还会很难跨吗？ 当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要掌握一些技巧和方法。下面我总结了几个写链表的代码技巧，如果能熟练掌握这几个技巧，叫上主动和坚持，轻松拿下链表代码完全没有问题。 理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以要想写好链表代码，首先就要理解好指针。 有些语言有“指针”的概念，比如C语言，有些语言没有指针，取而代之的是“引用”，比如Java、Python等。不管是指针还是引用，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。 接下来，我会拿C语言中的指针来讲解。如果你用的是Java或者其他语言也没关系，把它理解成引用就可以了。 实际上，对于指针的理解，只需要记住下面这句话就可以了：将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。 在编写链表代码的时候，经常会有这样的代码：p-&gt;next = q，这行代码是说p结点中的next指针存储了q结点的内存地址。还有一个更复杂的，也是写链表代码经常用到的：p-&gt;next = p-&gt;next-&gt;next，意思是说p结点的next指针存储了p结点的下下一个结点的内存地址。 掌握了指针或者引用的概念，应该可以很轻松的看懂链表代码。 警惕指针丢失和内存泄露不知道你有没有这样的感觉，写链表代码的时候指针指来指去，一会就不知道指针到哪里了。所以我们在写代码的时候，一定不要弄丢了指针。 如上图所示，当我们在a结点和b结点之间插入结点c，假设当前指针p指向结点a。如果我们将代码写成下面这个样子，就会发生指针丢失和内存泄露。 12p-&gt;next = c; // 将p的next指针指向c结点c-&gt;next = p-&gt;next; //将c结点next指针指向b结点 当p-&gt;next指针在完成第一步操作之后，已经不再指向b结点了，而是指向结点c，因此，第二行代码相当于将c-&gt;next指针指向了自己。因此整个链表断裂成了两半，从结点b之后的所有结点都无法访问了。 对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露，所以，我们在插入结点时，一定要注意操作的顺序。要先将c结点的next指针指向b，再将a结点的next指针指向c，这样才不会丢失指针，导致内存泄露。 利用哨兵简化实现难度首先，我们回顾一下单链表的插入、删除操作。如果我们在结点p之后插入一个结点，只需要下面两行代码就可以了。 12new_node-&gt;next = p-&gt;next; p-&gt;next = new_node; 但是当我们向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以从这段代码可以看出，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不同的。 1234if (head == null)&#123; head = new_node;&#125; 同样再来看一下链表的删除操作，如果要删除p结点的后继点点，我们只需要一行代码就可以搞定： 1p-&gt;next = p-&gt;next-&gt;next； 但是如果要删除链表的最后一个结点，这样的代码就不行了。跟插入类似，我们也需要对这种情况特殊处理。代码如下： 1234if (head-&gt;next == null)&#123; head = null;&#125; 可以看出，针对链表的插入、删除操作，需要对第一个结点的插入和最后一个结点的删除情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。那如何来解决这个问题呢？ 这时上面提到的哨兵就出场了。现实中的哨兵，解决的是国家之间的边界问题。同理我们这里的哨兵也是解决“边界问题的”，不直接参与业务逻辑。 还记得如何表示一个空链表呢？head=null表示链表中没有结点了，其中head表示头结点指针，指向链表中的第一个结点。 如果我们引入哨兵结点，在任何时候，不管链表是不是为空，head指针都会一直指向这个哨兵结点。我们把这种有哨兵的链表叫做带头链表，相反，没有哨兵结点的链表叫做不带头链表。 如下我画了一个带头链表，可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑。 实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这里用C语言实现一个简单的例子，不涉及语法方面的高级知识，你可以类比其他语言。 代码一： 123456789101112131415161718// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; int i=0; // 这里有两个比较操作： i&lt;n 和 a[i] == key while(i&lt;n)&#123; if(a[i] == key)&#123; retrun i; &#125; ++i; &#125; retrun -1;&#125; 代码二： 12345678910111213141516171819202122232425262728293031323334// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度// 为了更好的解释，这里举了个例子来说明// a = &#123;4,2,3,5,9,6&#125; key = 7int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; // 这里因为要将a[n-1]设为哨兵，所以特殊处理这个值 if(a[n-1] == key)&#123; return n-1; &#125; // 临时变量保存a[n-1]，以便之后恢复，这里temp = 6 char temp = a[n-1]; // 把key值放到数组a[n-1]，此时a=&#123;4,2,3,5,9,7&#125; a[n-1] = key; int i=0; // 此时while循环比起代码一，少了i&lt;n这个比较操作 while(a[i] == key)&#123; ++i; &#125; // 将数组a[n-1] 恢复为原来的值 a[n-1] = temp; // 如果i = n-1，说明数组中没有要找的key if(i == n-1)&#123; return -1; &#125; // 否则，说明找到了key，位置为i else&#123; return i; &#125;&#125; 对比两段代码，在字符串a很长的时候，比如几万、几十万，你觉得那段代码执行更快呢？答案是代码二。因为两端代码中执行次数最多的就是while循环那一部分。在第二段代码中，我们通过一个哨兵a[n-1]=key，成功省掉了一个比较语句，不要小看了这一句，当积累上万次、几十万次的时候，累积的时间就很明显了。 当然，这里只是说明哨兵的作用，写代码的时候千万不要写成第二段代码那样，可读性太差了，大部分情况下，我们并不需要追求如此极致的性能。 重点留意边界条件处理软件开发中，代码在以下边界或者异常情况下，最容易产生bug。链表代码也不例外，要实现没有bug的链表代码，一定要在编写的过程中以及编写完成后，检查边界条件是否考虑全面，以及边界条件下代码是否能运行。 我经常用来检查链表代码是否正确执行的边界条件有这么几个：]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-链表]]></title>
    <url>%2Fposts%2F2018-09-12-%E7%AE%97%E6%B3%95-%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[前言 今天我们来聊聊“链表 LinkedList”这个数据结构，学习链表有什么用呢，我们先来讨论一个经典的链表使用场景，那就是LRU缓存淘汰算法。 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被占满时，那些数据应该被清理出去，那些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有这么三种：先进先出策略FIFO(First In First Out)、最少使用策略LFU(Least Frequently Used)、最近最少使用策略LRU(Least Recently Used)。 今天我们的问题是，怎样用链表来实现一个LRU缓存淘汰策略？ 链表及其结构 相比数组，链表是一种稍微复杂一点的数据结构，掌握起来也要比数组要困难一些。数组和链表是两个非常基础、非常常用的数据结构。所以要掌握甚至精通，同时理解其思想。 我们先从底层存储结构来看一下二者的区别： 为了直观的对比，我画了一张图，从图中可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高，如果我们申请一个100MB大小的内存空间，当内存中没有连续的、足够大的内存空间时，即便剩余的总空间大于100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，他通过“指针”将一组零散的内存块连接起来使用，所以申请一块大小是100MB的链表，根本不会有问题。 链表的结构五花八门，今天我们着重介绍三种最常用的链表结构：单链表、双向链表、循环链表。 单链表首先来看最简单、最常用的单链表。我们刚讲到，链表是用指针将一组零散的内存块串联在一起，其中，我们把内存块称为链表的“结点”。为了使所有的节点串联起来，每个链表的结点出了需要保存数据之外，还需要记录链上下一个结点的地址，如图所示，我们把这个记录下一个结点指针地址的指针叫做后继指针 next。 从上面单链表的结构图中，可以发现，单链表中有两个结点是比较特殊的，分别是第一个节点和最后一个结点，我们习惯性的把第一个结点称为头结点，最后一个节点称为尾结点。其中头结点用来记录链表的基地址，我们可以通过它遍历得到整个链表。而尾结点的特殊之处在于，指针不是指向下一个结点，二是指向了一个空地址null，表示这是链表的最后一个结点。 与数组一样，链表也支持数据的插入、查找、删除操作。我们知道在进行数组的插入、删除操作时，为了保持内存的连续性，需要做大量的数据搬移操作，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要保持内存的连续性而搬移结点，因为链表本身的存储空间就不是连续的。所以在链表中插入删除一个数据是非常快的。 为了方便理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度为O(1)。 但是有利就有弊，链表想要随机访问第K个元素就没有数组那么高效了。因为链表中的数据并非是连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就可以直接计算出对应的内存地址，而是需要一个一个结点依次遍历，直到找到对应的结点。 你可以把链表想象成一个队伍，每个人都知道自己前面的人是谁，所以当我们希望知道排在第K为的人是谁的时候，就需要从第一个人开始，一个一个往下数。所以链表随机访问的性能没有数组好，时间复杂度为O(n)。 好了，单链表了解了，下面来看看另外两个复杂的链表：循环链表和双向链表。 循环链表循环链表是一种特殊的单链表。实际上，循环链表也很简单，它和单链表唯一的区别就在尾结点。我们知道，单链表的尾结点是指向空地址，表示这是最后的节点了，而循环链表的尾结点的指针是指向链表的头结点。从下图中可以看出，循环链表想一个环一样首尾相连，所以叫循环链表。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环形结构特点时，就特别适合采用循环链表，比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表的话，代码就会简洁很多。 双线链表接下来再看一个稍微复杂，在实际的软件开发中，也更加常见的链表结构：双向链表。 单链表只有一个方向，节点只有一个后继指针，next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。 从上图可以看出，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单向链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表的操作灵活性。那相比单向链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的删除、插入操作比单链表要简单、高效。 你可能会说，单链表的插入、删除操作的时间复杂度都已经是O(1)了，双向链表还能怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法的书籍也是这么说得，但是这种说法实际上是不准确的，或者说是有先觉条件的。 我们再来分析一下链表的两个操作，先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值的”结点 删除给定指针指向的结点 对于第一种情况，不管是单链表还是双向链表，为了查找到值等于某个给定值的结点，都需要从头开始一个一个依次遍历对比，知道找到值等于给定值的结点，再通过前面讲的指针操作将其删除。 尽管单纯的删除操作时间复杂度都是O(1)，但是遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。 对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道前驱结点，而单链表并不支持直接获取前驱结点，所以为了找到前驱结点，我们还是要从头结点开始遍历链表，知道p-&gt;next = q，说明p是q的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大优势，双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。 除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查找的效率也要比单向链表高一些。因为我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是向前查找还是往后查找，所以平均只需要查找一半的数据。 现在，有没有觉得双向链表比单向链表更加高效呢？这就是问什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器，如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。 实际上，这里有一个更重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足时，如果我们更追求代码的执行速度，我们就可以选择空间复杂度相对较高，但时间复杂度相对较低的算法和数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单机片中，这个时候，就要反过来用时间换空间的涉及思路。 还是开篇缓存的例子，缓存实际上就是利用了空间换时间的例子。虽然我们将数据存放在磁盘上，会比较节省内存，但是每次查询数据都要查询一遍磁盘，会比较慢。但是我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次查询数据的速度就大大提高了。 所以对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)进行优化；而消耗过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？ 了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不需要我多讲，你应该知道双向循环链表长什么样子了吧？ 链表 VS 数组性能大比拼 通过前面的学习，你应该知道，数组和链表是两种截然不同的内存组织方式，正是因为内存存储的区别，他们插入、删除、随机访问的时间复杂度正好相反。 时间复杂度 数组 链表 插入删除 O(n) O(1) 随机访问 O(1) O(n) 不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就能决定使用那哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存并不好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，可能没有足够的连续内存空间分配给它，导致“内存不足”。如果声明的数组过小，则可能出现不够用的情况，这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然的支持动态扩容，我觉得这也是它与数组最大的区别。 你可能会说，Java中也有ArrayList容器，也可以支持动态扩容啊？我们上一节已经讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将原数组拷贝过去，而数据拷贝的操作是非常耗时的。 我举一个稍微极端的例子。如果我们用ArrayList存储了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上，听起来是不是就很耗时。 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的内存空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是java语言，就有可能会导致频繁的GC(Garbage Collection 垃圾回收)。 所以在实际的开发项目中，要根据不同的项目情况，权衡究竟是选择数组还是链表。 解答开篇 好了，我们现在回过头来看，如何基于链表实现LRU缓存淘汰算法？ 我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新数据被访问时，我们从链表头部开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，再插入到链表的头部。 如果此数据没有缓存在链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 这样我们就实现了一个LRU缓存，是不是很简单。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。 实际上，我们可以继续优化这个实现思路，比如引入哈希表(hash table)来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。这个优化方案，等讲到哈希表的时候再讲。 基于链表的实现思路，实际上还可以用数组来实现LRU缓存淘汰策略。如何利用数组实现LRU缓存淘汰策略？ 内容小结 今天我们讲了一种跟数组“相反”的数据结构，链表。他跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。 和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过在具体的软件开发中，要对数组和链表的各种性能进行对比，综合来使用两者中的一个。 课后思考 如何判断一个字符串是否是回文字符串呢？今天的思考题就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那如何来判断是一个回文串呢？相应的时间空间复杂度是多少。 本章代码：github]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-数组]]></title>
    <url>%2Fposts%2F2018-09-10-%E7%AE%97%E6%B3%95-%E6%95%B0%E7%BB%84.html</url>
    <content type="text"><![CDATA[前言 提到数组，我想你肯定不陌生，甚至还会自信的说他很简单。 是的，在每一种编程语言中，基本都会有数组这种数据类型。尽管数组看起来非常基础、简单，但是我估计很多人都没有理解这个基础数据结构的精髓。 在大部分的数据结构中，数组都是从0开始编号的，但是为什么数组要从0开始，而不是1开始呢？从1开始不是更符合人类的思维习惯吗？下面我们通过本篇文章来认识这个问题。 数组如何实现随机访问？ 什么是数组呢？数组是一种线性表结构，它用一组连续的内存空间，来存储一组具有相同数据类型的数据。 这里有几个关键词： 第一是线性表。顾名思义，线性表就是数据像一条线一样的结构。每个线性表上的数据最多只有前后两个方向。除了数组，链表、队列、栈等也是线性表结构。 与线性表相对应的概念是非线性表，比如二叉树、堆、图，之所以叫非线性，是因为在非线性表中，数据之间并不是简单的前后关系。 第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，所以才有一个堪称杀手锏的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如在数组中插入、删除一个数据，为了保证连续性，就需要做大量的数据搬移工作。 说到数据的随机访问，那么数组是如何实现很具下标随机访问数组元素的吗？ 我们拿一个长度为10的int类型的数组int[] a = new int[10] 来举例。在如下图中，假设计算机给数组a[10] 分配了一块连续的内存空间000-039，其中首地址为000。 我们知道计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问某个数组元素时，它会通过寻址公式，计算出该元素的内存地址。 $$ a[i]\_address = base\_address + i * data\_type\_size $$ 其中base address表示数组的基地址，data_type_size表示数组中的每个元素的大小，在这个例子中，数组中存储的int类型，所以data_type_size就是4个字节。 很多人在面试中回答数组和链表的区别都会这么说：“链表适合插入、删除，时间复杂度为 O(1)；数组适合查找，查找时间复杂度为O(1)”。实际上这种表述是不准确的。数组是适合查找操作，但是查找的复杂度并不是O(1)，即便是排好序的数组，用二分查找时间复杂度也是$O(logN)$。所以正确的表述应该是数组的随机访问的复杂度是O(1)。 低效的“插入”和“删除”前面我们提到，数组为了保持内存数据的连续性，会导致插入、删除操作比较低效，现在我们就来看看究竟为什么会导致低效？ 插入操作假设数组的长度为n，现在需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，我们需要将k-n这部分的元素都往后顺挪一位。 如果是在数组的末尾插入元素，那就不需要移动数据，时间复杂度为O(1)；但是如果在数组开头插入一个元素，那所有的元素都需要后移一位，所以最坏时间复杂度为O(n)；因为在每个位置插入元素的概率是一样的，所以平均时间复杂度为$ (1+2+3+…+n)/n = O(n) $ 。 所以对于插入的时间复杂度：最好的O(1)，最坏O(n)，平均O(n)。 如果数组中的元素是有序的，并且插入新元素也要保证数组有序，那么就必须按照刚才的方法移动数据。但是如果数组中存储的数据没有任何规律，只是被当来存储数据的集合，那么如果在k处插入一个数据，可以将k处的数据移到数组的末尾，然后替换k处数据为要插入的数据，这种插入处理技巧可以将时间复杂度降为O(1)。 删除操作跟插入数据类似，如果要删除第k个位置的数据，为了保持内存的连续性，也需要搬迁数据，不然数组中间就会出现断层，内存就不连续了。 和插入类似，如果删除数组末尾的数据，则是最好时间复杂度为O(1)；如果删除开头的数据，则最坏时间复杂度为O(n)，平均情况时间复杂度也为O(n)。 实际上，在某些特殊场景下，我们并不一定追求数组中数据的连续性，如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？ 我们继续来看一个例子，数组a[10]中存储了8个元素：a,b,c,d,e,f,g,h。现在我们要依次删除a,b,c这三个元素。 为了避免d,e,f,g这几个数据会被搬移三次，我们可以先记录下已删除的数据，每次的删除并不是真正的搬移数据，只是记录数据已经被删除，当数组没有更多空间存储数据事，我们再进行一次真正的删除操作，这样就大大减少了删除数据之后导致的数据迁移。 如果你了解JVM，会发现，这不就是JVM的标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或算法，而是要学习他背后的思想和处理技巧，这些东西才是最优价值的。如果你细心留意，不管是在开发还是在架构设计中，总能找到某些数据结构和算法的影子。 警惕数组越界问题了解数组的几个基本操作后，再来看看数据的访问越界问题。 这里以一段C语言代码为例来进行说明： 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(i; i&lt;=3; i++)&#123; arr[i] = 0; printf("hello world\n"); &#125; return 0;&#125; 你发现问题了吗？这段代码并不是打印三行”hello world”，而是会无限打印”hello world”，这是为什么呢？ 我们知道数组大小为3，分别为a[0]、a[1]、a[2]，而我们代码因为书写错误，for循环结束条件错写为了i&lt;=3而非i&lt;3，所以当i=3时，数组访问越界。 我们知道，在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。而根据我们前面讲的寻址公式，a[3]也会被定位到一个某块不属于数组的内存地址上，而在C语言的内存管理中，在局部变量分配空间的顺序是跟变量的声明顺序直接相关，同时按照内存由高到低的顺序进行空间分配，所以在内存布局中，i变量的地址刚好是在数组arr之后的一个字，所以在循环体中，将arr[3]赋值为0，实际上却是将计数器i的值设为0，这就导致了该函数的死循环。 关于C语言中编译器关于变量的内存分配顺序可以看此篇文章理解一下: https://blog.csdn.net/liuhuiyi/article/details/7526889 数组越界在C语言中是一种未决行为，并没有规定数组访问越界编译器应该如何处理。因为数组访问的本质就是访问一段连续的内存地址，只要数组通过偏移计算得到的内存地址是可用的，那么程序就不会报错。 所以在这种情况下，一般会出现莫名其妙的错误，而且很多计算机病毒也是利用了代码中数组越界可以访问到非法地址的漏洞，来攻击系统，所以代码中一定要警惕数组的越界访问。 但并非所有的编程语言都想C一样，将数组越界检查交给程序员来做，像Java、Python本身就会做越界检查，比如java会抛出java.lang.ArrayIndexOutOfBoundsException的异常，Python会有IndexError: list index out of range的错误。 容器能否完全代替数组?针对数组类型，很多语言提供了容器类。比如在java中提供了ArrayList、C++ STL中的vector等。那么在项目开发中，什么时候适合用数组，什么时候适合用容器呢？ 以java中ArrayList为例，ArrayList最大的优势就是可以将很多数组操作封装，比如数组的插入、删除等。另外，它还支持动态扩容，当存储空间不够时，它会自动扩容为原来的1.5倍。 不过由于扩容操作涉及内存申请和数据搬移，是比较耗时的，因此如果事先能确定存储数据的大小，最好在创建ArrayList时实现指定数据的大小。 作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有时候用数组会更合适些。 1、Java ArrayList无法存储基本类型，需要封装为Long、Integer等包装类类型，因此存在一定的拆装箱上的性能损耗，如果特别关注性能，或者要使用基本类型，则可以选择数组。 2、如果事先知道数据的大小，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以使用数组。 对于业务开发，直接使用容器就足够了，省时省力，毕竟一丢丢的性能损耗，不会影响到系统整体的性能，但是如果做一些非常底层的开发，这个时候数组就会优于容器，成为首选。 解答开篇为什么数组的索引是从0开始，而不是从1开始呢？ 从数组存储的内存模型来看，”下标”即索引最确切的定义应该是”偏移(offset)”，如果用arr表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要根据如下公式计算即可$$ a[k]\_address = base\_address + k * type\_size $$ 但是如果数组从1开始计数，那我们计算a[k]的内存地址计算公式就会变为：$$ a[k]\_address = base\_address + (k-1) * type\_size $$ 对比两个公式，从1开始的话，每次随机访问数组元素就多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是非常基础的操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作指令，数组选择了从小标从0开始，而不是从1开始。 不过解释的再多，我认为都算不上压倒性的证明，说数组编号非从0开始不可，最主要的原因可能是历史原因。 C语言设计者用0开始计数数组下标之后，Java、JavaScript等高级语言都效仿了C语言，或者说为了在一定程度上减少C语言程序学习Java的成本，继续沿用了从0开始计数的习惯。但是仍有很多语言中数组并不是从0开始的，比如Matlab。甚至还有一些语言支持负数下标，比如python。 思考题1、在数组的删除操作中，提到了JVM的标记清除垃圾回收算法的核心理念，如果熟悉Java、JVM，回顾下JVM的标记清除垃圾回收算法。2、上面讲到一维数组的寻址公式，类比一下，二维数组的内存寻址公式是怎么样的？ JVM标记清除垃圾回收算法：分为两个阶段，标记和清除。在大多数主流的虚拟机中采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有GC ROOTS，将所有GC ROOTS可达对象标记为存活，只有当标记工作完成后，才会进行清理工作。 该算法最大的问题是会产生连续的内存空间碎片，同时标记和回收的效率都不高，但是对于只有少量垃圾产生时可以采用此种算法。 二维数组的寻址公式： 根据上图,对于一个二维数组int arr[m][n]，arr[i][j]的寻址公式为：$$ arr[i][j]\_address = base\_address + (i + n*j)*data\_type\_size $$]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-最好、最坏、平均、均摊时间复杂度]]></title>
    <url>%2Fposts%2F2018-09-09-%E7%AE%97%E6%B3%95-%E6%9C%80%E5%A5%BD%E3%80%81%E6%9C%80%E5%9D%8F%E3%80%81%E5%B9%B3%E5%9D%87%E3%80%81%E5%9D%87%E6%91%8A%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.html</url>
    <content type="text"><![CDATA[前言 前面我们讲过复杂度的大O表示法和几个分析技巧，还举了一些复杂度分析的例子，掌握了这些内容，对于复杂度分析这个知识点，已经达到及格线了。 这篇会着重讲一下复杂度分析的四个复杂度分析方面的知识： 最好时间情况复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。 最好、最坏时间复杂度 我们先用学过的知识试着分析以下代码的时间复杂度： 1234567891011int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; &#125; &#125; return pos;&#125; 上面代码实现的功能是在一个无序数组中，查找变量target的位置，如果找不到就返回-1，按照前面的分析方法，该段代码的时间复杂度为O(n)。 但是我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，优化一下这段代码： 123456789101112int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; break; &#125; &#125; return pos;&#125; 但是这时候问题来了，优化完之后，时间复杂度还是O(n)吗？ 因为要查找的变量target可能出现在数组的任何位置，如果要查找的target刚好出现在数组的开始位置，那么就不需要遍历剩余的数据，此时时间复杂度为O(1)。但是如果数组中不存在变量target，或者在最后一位，那我们就需要把整个数组都遍历一遍，时间复杂度就成了O(n)，所以这段代码在不同情况下时间复杂度是不同的。 为了表示代码在不同情况下的时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况复杂度、平均时间复杂度。 顾名思义，最好情况时间复杂度就是，在最理想情况下，执行这段代码的时间复杂度。如上例中，在最理想情况下，查找的变量target刚好在第一个，这时候对应的时间复杂度就是最好情况时间复杂度。 同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度，上例中，如果数组中没有要查找的变量target，我们需要把整个数组遍历一遍，所以最坏情况下对应的时间复杂度就是最坏情况复杂度。 平均时间复杂度我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率并不大。为了更好的表示平均情况下的时间复杂度，我们引入一个概念：平均情况时间复杂度，简称平均时间复杂度。 平均时间复杂度又该怎么分析呢？我们还是借助上面的例子。 要查找的变量target在数组中的位置，有n+1中情况： 在数组0 ~ n-1位置 n种情况和不在数组中1个情况。我们把每种情况下，需要遍历的元素个数累加起来，然后在除以n+1，就可以得到需要遍历的元素个数的平均值，即： $$ \frac{1+2+3+…+n+n}{n+1} = \frac{n(n + 3)}{2(n + 1)} $$ 我们知道，时间复杂度大O标记法中，可以省略掉系数、低阶、常量，所以上面的时间复杂度为O(n)。 这个结论虽然是正确的，但是计算过程稍微有点问题。我们刚讲的这n+1中情况，出现的概率并不一样。下面结合概率论的知识分析一下。 我们知道，要查找的变量x，要么在数组中，要么不再数组中，我们假设这两个概率分布为$\frac{1}{2}$。 不在数组中时，时间复杂度为: $n\times\frac{1}{2}$; 在数组中时，因为数组大小为n，出现在任何一个位置的可能性都是一样的，所以每个位置的概率就是:$\frac{1}{2n}$, 因此在数组中时的时间复杂度为：$(1+2+3+…+n)\times\frac{1}{2n} $。 那平均时间复杂度就是：$(1+2+3+…+n)\times\frac{1}{2n} + n\times\frac{1}{2} = \frac{3n+1}{4} = O(n)$。 这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度也叫做加权平均时间复杂度或者期望时间复杂度。 实际上，在大多情况下我们并不需要区分最好、最坏、平均时间复杂度三种情况，很多时候我们只用一个复杂度就可以满足需求了。只有同一代码在不同的情况下，时间复杂度有量级的差距，我们才会使用三种复杂度表示法来区分。 均摊时间复杂度目前为止，我们应该已经掌握了算法复杂度分析的大部分内容了，下面来认识一个更高级的概念：均摊时间复杂度，以及它对应的分析方法摊还分析。 均摊时间复杂度听起来跟平均时间复杂度有点像，对于初学者来说，这两个概念很容易弄混。前面说过，大部分情况下不需要区分最好、最坏、平均时间复杂度，只有某些特殊情况才需要平均时间复杂度，而均摊时间复杂度比它的应用场景比它更特殊、更有限。 还是以一个例子来说明(别太在意例子，只是为了说明)： 1234567891011121314151617int[] arr = new int[n];int size = 0；void insert(int val)&#123; // 如果数组满了 if(count == arr.length)&#123; int sum = 0; for(int i=0; i&lt;arr.length;i++)&#123; sum = sum + arr[i]; &#125; arr[0] = sum; count = 1; &#125; // 数组赋值 arr[count] = val; ++count;&#125; 先简单解释一下这段代码的功能，这段代码实现了一个往数组中插入数据的功能，如果数组有空闲空间，直接插入即可。如果数组满了，将数组中的数据求和，清空数组，将求和之后的数据放入数组的第一个位置，然后再将新的数据插入。 那这段代码的时间复杂度是多少呢？我们可以先利用上面讲的三种分析方法来分析一下。 最理想情况下，数组有空闲空间，直接插入数据就可以，所以最好时间复杂度为O(1)；最坏情况下，数组中没有空闲空间了，我们需要先进行一次数组遍历求和，在做数据插入，所以最坏情况时间复杂度为O(n)；平均情况时间复杂度，我们还是用概率论的方法来分析，假设数组长度为n，根据插入位置不同，可以分为n种情况，每种情况的时间复杂度为O(1)，另外还有一种特殊情况，就是数组没有空闲时间时，时间复杂度为O(n)，而且这n+1中情况出现的概率是一样的，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 我们来比较一下这个例子中insert函数和上面findArray的不同。首先，findArray在极端情况下，复杂度才为O(1)，大部分情况都为O(n)，而insert函数大部分情况时间复杂度都为O(1)，只有特殊情况时间复杂度才为O(n)，这是第一个区别。第二个不同的地方，对于insert函数来说，O(1)和O(n)的时间复杂度出现的频率是非常有规律的，而且有一定的时序关系，一般都是一个O(n)插入之后，跟n-1个O(1)的插入操作，循环往复。 针对这样一种情况，我们并不需要像平均复杂度分析那样，计算所有输入情况和发生的概率，计算加权平均值。 我们引入一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字叫：摊还时间复杂度。 那么究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？ 我们还是以这个insert函数为例，每一次O(n)的插入操作，后面都会跟n-1次O(1)插入操作，所以我们把耗时最多的操作均摊到n-1次耗时少的操作上，均摊下来，这一组连续操作的均摊时间复杂度就为O(1)，这就是均摊分析法的大致思路。 均摊时间复杂度和摊还分析应用场景比较特殊，所以不会经常用到，这里简单总结一下他们的应用场景。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块分析，看看是否能将时间复杂度高的操作，均摊到其他时间复杂度低的操作上。在一般的能运用均摊时间复杂度的场景中，均摊时间复杂度是等于最好时间复杂度的。 思考题：根据今天学习的几个复杂度分析的方法，来分析一下下面这个add()函数的时间复杂度。 1234567891011121314151617181920int[] arr = new int[10];int len = 10;int i=0;void add(int element)&#123; // 数组空间满了 if(i&gt;=len)&#123; // 数组扩容 int new_arr = new int[len*2]; // 把数组拷贝到新数组 for(int j=0; i&lt;len; j++)&#123; new_arr[j] = arr[j]; &#125; arr = new_arr; len = len*2; &#125; // 添加到数组中 arr[i] = element; ++i;&#125; 在最理想情况下，数组中有空闲空间，可以直接添加到数组中，时间复杂度为O(1)；最坏情况下，数组中没有空闲空间，先进行一次扩容操作，在进行遍历给新数组赋值，时间复杂度为O(n)，所以最坏时间复杂度为O(n)。 平均时间复杂度，可以分为有空闲空间和没有空闲空间两种，有空间空间有n中情况，所以每种情况出现的概率为$\frac{1}{n+1}$，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 均摊时间复杂度，可以看出本例是符合均摊时间复杂度的场景的，在一次O(n)时间复杂度操作后都会跟n-1次O(1)时间复杂度操作，所以将O(n)时间复杂度的操作均摊到n-1次O(1)时间复杂度操作上，最终均摊时间复杂度为O(1)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-复杂度分析]]></title>
    <url>%2Fposts%2F2018-09-08-%E7%AE%97%E6%B3%95-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言 我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快、更省存储空间。那如何来衡量算法的“快”和“省”呢？这就要用到复杂度分析：时间、空间复杂度分析。复杂度分析是整个算法学习的精髓，掌握了它，数据结构和算法的内容基本就掌握了一半。 为什么需要复杂度分析 有人说，我只要把代码跑一遍，通过统计、监控，就可以得到算法执行的时间和占用的那内存，为什么还要做复杂度分析呢？ 1、首先，这种评估方法确实是准确的，但是这种方法是”事后统计法”，是有非常大的局限性。 2、测试结果非常依赖测试环境，同样一段代码，在不同的CPU可能执行的时间会差很多，比如Intel Core i9就比i3运行的快，同样在不同的两台机器上也可能会出现代码执行不一样的情况。 3、对于不同的数据集，如果数据的有序程度不一样，那么对数据进行同一种算法运算，也可能会得到不同的结果。除此之外，数据规模的大小也可能对算法产生影响。 因此我们需要一个不用具体的测试数据来测试，就可以粗略估计算法的执行效率的方法，这就是时间、空间复杂度分析所解决的问题。 大O复杂度表示法 算法的执行效率，粗略的讲，就是算法执行的时间，但是如何能在不运行的情况下，得到一段代码的运行时间呢？ 这里举一个简单的例子，求解1，2，3……n 的累加和，以下为一个简单的代码实现： 1234567int sum(int n)&#123; int sum = 0; for (int i=1; i&lt;=n; i++)&#123; sum += i; &#125; return sum;&#125; 从CPU的角度看，每一行代码都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行个数、执行时间都不尽相同，但是我们只是粗略的估计，因此这里假设每行代码执行的时间都相同，那么在此基础上，这段代码执行的时间可以进行如下计算： 第二行代码执行时间为time，第三、四行代码执行的时间为 $ 2 \times n \times time $，所以此段代码执行的时间为$ (2n + 1)\times time $ ，可以看出这段代码执行时间T(n)与每行代码的执行次数成正比。 按照这个思路，再对以下代码段进行分析： 12345678int sum(int n)&#123; int sum = 0; for(int i=1; i &lt;= n; i++)&#123; for(int j=1; j &lt;= n; j++)&#123; sum += i*j; &#125; &#125;&#125; 假设每行代码执行的时间依然为time，那么这段代码执行的时间是多少呢？ 第二行代码的执行时间依然为time，第三行代码执行的次数为n次，所以需要的时间为$ n*time $,内层循环第四、五行代码都执行了$ n*n $次,需要的时间为$ 2*n^2*time $。所以此段代码总的执行时间为$(n + 1 + 2n^2)*time $。 尽管不知道time的具体值，但是通过这两段代码的分析过程，得出一个非常重要的规律： 所有的代码执行时间T(n)与每行代码的执行次数成正比$$ T(n) = O(f(n)) $$ 其中 $T(n)$ 表示代码执行的时间; n表示数据规模大小; $ f(n) $ 表示每行代码执行次数的总和，因为是一个公式，所以用$ f(n) $ 表示。公式中的O表示代码执行时间 $ T(n) $ 与 $ f(n) $ 成正比。 所以在第一个例子中 $ T(n) = O(2n + 1) $ ，第二个例子中 $ T(n) = O(2n^2 + n + 1)$ , 这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，所以也叫做渐进时间复杂度，简称时间复杂度。 在时间复杂度公式中，如果n很大时，公式中的低阶、常量、系数三部分并不影响增长趋势，所以可以先忽略。所以上述两个例子的时间复杂度就可以记为： $ T(n) = O(n) $； $ T(n) = O(n^2) $; 时间复杂度分析 前面介绍了大 O 时间复杂度的由来和表示方法，那如何分析一段代码的时间复杂度呢？ 1、只关注循环次数最多的一段代码在大 O 表示法中，只是表示一种趋势，通常我们会忽略公式中的常量、低阶、系数，因此只需要记录一个最大的量级就可以了，所以我们在分析一个算法时，只关注循环次数执行次数最多的那一段代码就行了。 2、加法法则：总复杂度等于量级最大的那段代码的复杂度如果一段代码中出现多个循环，那么总的时间复杂度就是各个循环相加得到的，但是往往会忽略低阶、常量，因此只取量级最大的那段代码就可以了。 注意：当一段代码循环次数是一个常量，比如循环10000、1000000次，只要是一个已知的常量数，且不随数据规模变化，那么该循环照样是一个常量级别的执行时间。 3、乘法法则: 嵌套代码的时间复杂度等于嵌套内外代码复杂度的乘积比如第二个例子中如果但看外层循环的时间复杂度是 $ O(n) $；内层循环的时间复杂度也是 $O(n)$， 因此总共的时间复杂度就是 $ T(n) = O(n) * O(n) = O(n^2) $ 几种常见时间复杂度 1、$O(1)$O(1) 只是常量级时间复杂度的一种表示方法，并不是指执行了一行代码。只要代码的执行时间不随n的增大而增大，这样的代码时间复杂度都可以记为O(1)。一般情况下，只要代码中不出现循环、递归等，即使有成千上万行代码，时间复杂度也是O(1)。 2、$ O(logN)、O(N*logN) $对数阶的时间复杂度非常常见，同时也是最难分析的一种。 1234int i = 1;while(i &lt;= n)&#123; i = i * 2;&#125; 在上述代码中，变量i从1取值，第二次为2，第三次为4，第四次为8……,所以i的取值规律为 $$ 2^0 \&nbsp;&nbsp;&nbsp;&nbsp; 2^1 \&nbsp;&nbsp; 2^2 \&nbsp;&nbsp; 2^3 … 2^k… 2^x $$ 当$2^x = n$ 时，循环结束，而循环的次数即为x，所以时间复杂度也为$ O(x=\log_2 N) $。 如果把代码改为如下。那时间复杂度是多少呢？ 1234int i = 1;while(i &lt;= n)&#123; i = i * 3;&#125; 根据上面的思路，很容易看出这段代码的时间复杂度为$ O(log_3N) $ 。 实际上，不管是以2为底，还是以3为底，亦或是以10为底，我们都把对数阶的时间复杂度记为$ O(logN) $，为什么呢？ 我们知道对数之间是可以互相转化的，$ log_3n$ 就可以转换为$ log_32*log_2N $，所以$ O(log_32) = O(C * log_2N) $，其中$ C = log_32 $ 是一个常量，基于前面的结论： 在采用大O标记复杂度的时候，可以忽略系数，即$ O(C*f(n)) = O(f(n)) $。因此在对数阶时间复杂度的表示方法里，我们忽略的底，统一表示为$O(logN)$。 如果理解了$O(logN)$，那么$O(nlogN)$就很容易了，根据前面所说的乘法法则，如果一段代码的时间复杂度是$O(logN)$，如果循环执行了 n 次，那么该代码的时间复杂度就是$O(nlogN)$。而且$O(nlogN)$是一种非常常见的时间复杂度，归并排序、快速排序的时间复杂度都是$O(nlogN)$。 2、$ O(m+n)、O(m*n) $我们再来讲跟前面都不一样的时间复杂度，代码的时间复杂度由两个数据规模来决定。 123456789101112int func(int m, int n)&#123; int sum1 = 0; for(int i=1; i&lt;=m; i++)&#123; sum1 += i; &#125; int sum1 = 0; for(int j=1; j&lt;=m; j++)&#123; sum1 += j; &#125; return sum1+sum2;&#125; 从代码中看出，m和n表示两个不同的数据规模，我们无法事先评估m和n的量级大小，所以我们在分析复杂度时，就不能简单用加法法则忽略一个，因此上面代码的时间复杂度为$O(m + n)$， 针对这种情况，加法原则就不正确了，我们将加法原则改为：$ T1(m) + T2(n) = O(f(m) + g(n)) $，但是乘法法则继续有效：$ T1(m) + T2(n) = O(f(m) * f(n)) $。 空间复杂度 前面讲过，时间复杂度的全称是渐近时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度的全称就是渐进空间复杂度，表示算法的存储空间与数据规模的增长关系。 还是拿具体的例子说明(仅供测试,一般没人这么写) 12345678void func(int n)&#123; int i = 0; int[] a = new int[n]; for(i; i&lt;n; i++)&#123; a[i] = i*1; print(a[i]); &#125;&#125; 和分析时间复杂度一样，我们看到第二行申请了一个空间变量i，但是它是常量阶的，跟数据规模n无关，所以可以忽略，第三行申请了一个大小为n的int数组，除此之外，该代码没有占据更多的空间O(n). 我们常见的空间复杂度就是$O(1)、O(n)、O(n^2)$，像$ O(logN)、O(nlogN) $ 这样的对数阶复杂度平时都用不到。空间复杂度分析相对时间复杂度要简单得多。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-安装及配置]]></title>
    <url>%2Fposts%2F2018-09-07-hexo-%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[前言曾几何时，你是否也想有个自己的博客，抒发自己的心情，总结自己的得失，与人分享喜悦、哀伤、愤怒、忧愁，那么这篇文章你就必须看了，非常简单搭建一个自己的开源博客。 一、预备1、安装Nodejs及npm Nodejs下载地址： 官网下载地址：https://nodejs.org/zh-cn/download/ 2、安装Git Git下载地址： 官网下载地址：https://git-scm.com/download/ 安装完成后，执行如下命令，可以显示版本号就算安装成功了 12345678$ node -vv9.11.1$ npm -v6.3.0$ git --versiongit version 2.17.0.windows.1 二、安装hexo进入命令行，执行如下命令: 1234567891011121、全局安装hexo$ npm install hexo -g2、创建hexo工作目录$ mkdir hexo-blog$ cd hexo-blog3、初始化工作目录$ hexo init4、本地启动hexo$ hexo serve 到此一个hexo博客已经搭建完成了，可以访问 http://localhost:4000/ 查看博客的效果。 当然现在你就可以开始写博客了，默认的配置足够你写作、发表文章了，但是默认的东西有些并不符合自己的要求和审美。所以下面对hexo进行一些配置，以符合自己的要求。 三、hexo配置hexo的配置文件在根目录下_config.yml文件中。本文仅列举几项，其余配置可以参照hexo官网文档进行配置，当然，有兴趣可以参照我的配置 网站配置：12345678# Sitetitle: Aries' blog 网站标题subtitle: 副标题description: 我不生产知识，我只是知识的搬运工。 网站一句话描述keywords: 关键词author: 无名万物 作者language: zh-CN 语言timezone: Asia/Shanghai 时区 文章配置：1234url: http://blog.renhj.org 网站urlroot: / 文章根路径permalink: posts/:year-:month-:day-:title.html 文章urlpermalink_defaults: 四、创建新文章你可以通过以下命令来创建一篇新文章1hexo new [layout] &lt;title&gt; 命令中指令文章的布局，默认为post，可以通过修改_config.yml中的default_layout来修改默认布局，当然也可以在文章Front-Matter上添加布局. 当然也可以新建一个草稿： draft，这种布局在建立时会保存到source/_drafts文件夹，也可以通过publish来将草稿移动到正式文件夹。 12345# 新建草稿文章$ hexo new draft &lt;title&gt;# 将文章正式发布$ hexo publish [layout] &lt;title&gt; Front-matter Front-matter是文章最上方以--- 分割的区域，用于指定个别文件的变量 12345678910---layout: 指定文章的布局属性title： 文章标题data：建立日期updated： 更新日期comments： 是否开启文章的评论功能(如果有的话)tags： 标签categories：分类permalink： 覆盖文章的网址--- 修改美化默认的主题是有点丑，可以去hexo的主题商店 找一个自己喜欢的、漂亮的主题。 本人找的是网上比较流行的nexT的主题，即本博客所使用的主题：hexo nexT主题，更多的配置可以参照nexT官网的配置或者其他文章进行配置。本文就不再这里赘述的，具体效果可以看本博客的。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>nexT</tag>
        <tag>Github Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Markdown来写文章]]></title>
    <url>%2Fposts%2F2018-09-06-%E7%94%A8Markdown%E6%9D%A5%E5%86%99%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[MarkdownMarkdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成丰富的HTML页面。 Markdown用一些简单的符号标识不同的标题，将某些文章标记为”粗体“或者斜体，下面就来一起学习一下。 语法1、标题 不同的标题采用不等个数的#号来进行标记，如下所示： 1234# 一级标题## 二级标题### 三级标题#### 四级标题 2、代码块 在需要高亮的代码块的前一行及后一行使用三个反引号“`”，同时第一行反引号后面表面代码块所使用的语言, 如下： ```pyhtonprint (“Hello World!”)``` 3、特殊字符 123**粗体***斜体*&gt; 引用内容]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
