<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[漏洞靶场Vulhub使用]]></title>
    <url>%2Fposts%2F2018-11-01-%E6%BC%8F%E6%B4%9E%E9%9D%B6%E5%9C%BAVulhub%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言 Vulhub是一个面向大众的开源漏洞靶场，采用docker进行搭建，但是无需docker知识，简单执行两条命令即可编译、运行一个完整的靶场环境。该项目旨在让漏洞复现变得更加简单，让安全研究人员更专注于漏洞本身。 安装 我在Centos7上进行的如下步骤，如果在其他类型的机器上，可以参照进行各个环境的安装 123456# 安装gityum install git# 安装docker并启动dockeryum install docker &amp;&amp; systemctl start docker# 安装docker-composeyum install docker-compose 由于该漏洞环境镜像均来自于Dockerhub/Github/软件官网，所以在国内访问可能会存在速度慢、丢包等问题，导致环境地洞太卡，影响正常使用，请自行解决翻墙问题，或者采用加速器进行加速。 docker-compose用户组合服务和内网 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(下)]]></title>
    <url>%2Fposts%2F2018-09-25-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8B).html</url>
    <content type="text"><![CDATA[前言 上一节着重分析了几种常用排序算法的原理、时间复杂度、空间复杂度、稳定性等。今天会接触三种时间复杂度为O(n)的排序算法：桶排序、基数排序、计数排序。因为这些排序算法的时间复杂度是线性的，所以把这类排序算法叫做线性排序。之所以能做到线性的时间复杂度，是因为这三种算法是基于非比较的排序算法，都不涉及元素之间的比较操作。 这几种算法理解起来都不难，时间、空间复杂度分析起来也很简单，但是对要排序的数据要求很苛刻，所以今天学习掌握这些排序算法的适用场景。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(中)]]></title>
    <url>%2Fposts%2F2018-09-23-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%AD).html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({}); 前言 上一节讲到冒泡排序、插入排序、选择排序这三种排序算法，他们的时间复杂度都是$O(n^2)$，比较高，适合小规模的排序。今天讲两种时间复杂度为$O(nlogN)$的排序算法，归并排序和快速排序。这两种算法适合大规模的数据排序，比上一节的三种算法更常用。 归并排序和快速排序都用到了分治思想，非常巧妙，我们可以借鉴这个思想，来解决非排序的问题，比如：如何在O(n)时间复杂度内查找一个无序数组中的第K大元素？，这就要用到今天讲的内容。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>快速排序</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序(上)]]></title>
    <url>%2Fposts%2F2018-09-20-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F(%E4%B8%8A).html</url>
    <content type="text"><![CDATA[前言 排序对一个程序员来说，可能都不会陌生。大部分编程语言中，也都提供了排序函数。在平成的项目中，也经常会用到排序。排序非常重要，所以会分几节详细讲一讲经典的排序算法。 排序算法太多了，可能有的连名字都没有听说过，比如猴子排序、睡眠排序、面条排序等等。这里只列举众多排序算法众多的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。按照时间复杂度把他们分成了三类，分上中下三节来讲。 排序算法 时间复杂度 是否基于比较 上 冒泡、插入、选择 $ O(n^2) $ √ 中 快排、归并 $ O(nlogN) $ √ 下 桶、计数、基数 $ O(n) $ × (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>冒泡排序</tag>
        <tag>插入排序</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-递归]]></title>
    <url>%2Fposts%2F2018-09-18-%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92.html</url>
    <content type="text"><![CDATA[前言 推荐注册返佣金这个功能我想你应该不陌生吧？现在很多app都有这个功能。这个功能中，用户A推荐用户B注册，用户B又推荐了用户C注册，我们可以说C的“最终推荐人”为用户A，用户B的“最终推荐人”也为用户A，用户A没哟“最终推荐人”。 一般来说，我们会通过数据库记录这种推荐关系，在数据库表中，我们可以记录两行数据，其中actor_id表示用户id，referrer_id表示推荐人id。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-队列]]></title>
    <url>%2Fposts%2F2018-09-16-%E7%AE%97%E6%B3%95-%E9%98%9F%E5%88%97.html</url>
    <content type="text"><![CDATA[前言 我们知道，CPU资源是有限的，任务的处理逻辑与线程个数并不是正相关。相反，过多的线程反而会导致CPU频繁切换，处理性能下降。所以，线程池的大小一般都是综合考虑要处理任务的特点与硬件环境，来事先设置的。 当我们向一个固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是如何实现的？ 其实，这些问题并不复杂，其底层的数据结构就是今天的内容，队列(queue)。 如何理解队列 队列这个概念非常好理解，你可以把它想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队。先进者先出，这就是典型的队列。 我们知道，栈只支持两个操作：入栈push()和出栈pop()，队列和栈非常类似，支持的操作只有：入队enqueue()，将一个数据方法队尾，出对dequeue()，从队头取出一个数据。 所以，队列跟栈一样，也是一种操作受限的线性表数据结构。 队列的概念很好理解，基本操作也很容易掌握。作为一种非常基础的数据结构，队列的应用也非常广泛。特别是一些具有额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多片底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形存储，都用到了循环队列；java.concurent并发包中用到了ArrayBlockingQueue来实现公平锁等。 顺序队列和循环队列 我们知道了，队列跟栈一样，也是一种抽象的数据结构。它具有先进先出的特性，支持在队尾插入元素，在对头删除元素，那么究竟该如何实现一个队列呢？ 跟栈一样，队列可以用数组实现，也可以用链表实现。用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。同样，用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。 先来看下基于数组的实现方法。我这里采用java语言进行实现，不会涉及高级语法。 12345678910111213141516171819202122232425262728293031323334353637// 基于数组实现的队列public class ArrayQueue&lt;T&gt;&#123; // 数组items private T[] items; // 队列大小 private int size=0; private int capacity; // head表示队头下标，tail表示队尾下标 private int head=0; private int tail=0; public ArrayQueue()&#123; this(10); // 队列默认容量给10 &#125; public ArrayQueue(int capacity)&#123; this.items = new T[capacity]; this.capacity = capacity; &#125; public boolean enqueue(T val)&#123; if(size == capacity)&#123;return false;&#125; // 队列满了 items[size] = val; size ++ ; tail = size; return true; &#125; public T dequeue()&#123; if (size == 0) &#123; return; &#125; T res = items[head]; head++; return res; &#125;&#125; (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-栈]]></title>
    <url>%2Fposts%2F2018-09-15-%E7%AE%97%E6%B3%95-%E6%A0%88.html</url>
    <content type="text"><![CDATA[前言 浏览器的前进、后退功能，我想你肯定很熟悉吧？ 当你依次访问完一连串页面a-b-c-d之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面c-b-a。当后退到a页面之后，点击前进按钮，可以重新进入页面b-c-d。但是如果进入页面b之后，点击了两一个页面，那就无法通过前进后退页面进入c-d了。 假如你是浏览器的开发设计者，你会如何实现这个功能呢？带着这个问题，我们来看一下“栈”这个数据结构。 (adsbygoogle = window.adsbygoogle || []).push({}); 如何理解栈？ 关于栈，举一个非常贴切的例子。比如叠盘子，我们放盘子的时候都是从下往上一个一个放。取的时候，我们也是从上往下一个一个取，不能从中间抽取。先进者后出，后进者先出，这就是典型的栈结构。 从栈的操作特性上来看，栈是一种操作受限的线性表，只允许在一端插入和删除数据。 我第一次接触这种数据结构的时候，就对它存在的意义产生了很大的疑惑。因为相比数组和链表，栈带给我的只有限制，并没有任何优势。那我直接使用数组或者链表就好了？为什么还要用这个“操作受限”的数据结构呢？ 事实上，从功能上来说，数组和链表确实可以代替栈，但是你要知道，特定的数据结构是对特定场景的抽象，而且数组和链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然就更容易出错。 当某个数据集合只涉及在一端插入和删除数据时，并且满足先进后出、后进先出的特性，我们就应该用栈这种数据结构。 如何实现一个栈？ 从刚才栈的定义里可以看出，栈主要包含两个操作，入栈和出栈。也就是在在栈顶插入一个数据和从栈顶删除一个数据。理解了栈的定义之后，我们来看一看如何用代码实现一个栈。 实际上，栈可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫做顺序栈，用链表实现的栈，我们叫做链式栈。 基于数组实现的顺序栈我这里用Java实现一个基于数组的顺序栈，基于链表的实现，可以自己写一下。 12345678910111213141516171819202122232425262728293031323334353637383940// 基于数组实现的链式栈public class ArrayStack&lt;T&gt; implements stack&lt;T&gt; &#123; private final Object [] DEFAULT_ARRAY = new Object[10]; private final int DEFAULT_CAP = 10; private Object[] data; private int cap; private int size; public ArrayStack() &#123; this.cap = DEFAULT_CAP; this.size = 0; this.data = DEFAULT_ARRAY; &#125; public ArrayStack(int cap)&#123; if (cap &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ cap); this.cap = cap; this.data = new Object[cap]; &#125; public void push(T val) &#123; if (size&lt;cap)&#123; // 数组满了 data[size] = val; size++; &#125;else &#123; throw new Runtime("stack is full!") // 可以动态扩容的stack // Object[] objects = new Object[cap*2]; // System.arraycopy(data, 0, objects, 0, size); // data = objects; // data[size] = val; // size ++; &#125; &#125; public T pop() &#123; if (size == 0) return null; T result = (T) data[size-1]; size--; return result; &#125;&#125; 了解了定义和基本操作，那它的操作时间、空间复杂度是多少呢？ 不管是链式栈还是顺序栈，我们存储数据需要一个大小为n的数组就够了。在入栈和出栈的过程中，只需要一两个临时变量存储空间，因此时间复杂度是O(1)。 注意这里存储数据需要一个大小为n的数组，并不是说空间复杂度是O(n)，因为这n个空间是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。 时间复杂度分析：不管是入栈、出栈，都只涉及栈顶个别数据的操作，因此时间复杂度为O(1)。 支持动态扩容的顺序栈刚才那个基于数组实现的顺序栈，是一个固定大小的栈，也就是说，在初始化后需要实现指定栈的大小，当栈满之后，就无法在王栈里添加数据了，尽管链式栈的大小不受限，但是要存储next指针，内存消耗相对较多。那我们如何实现一个可以支持动态扩容的栈呢？ 还记得，在数组那一节，要如何来实现一个支持动态扩容的数组吗？当数组空间不足时，我们重新申请一块更大的内存，将原来数组中的数据拷贝过去，这样就实现了一个支持动态扩容的数组。 所以，如果实现一个支持动态扩容的栈，我们只需要底层依赖一个支持动态扩容的数组就可以了。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新的数组中。 实际上，支持动态扩容的顺序栈，我们开发中并不经常用到。这块我们复习一下复杂度分析方法。现在我们来分析一下支持动态扩容的顺序栈的入栈、出栈时间复杂度。 对于出栈操作来说，不会涉及到内存的重新申请和数据搬移，所以出栈的时间复杂度仍然是O(1)。但是对于入栈操作来说，情况就不一样了，当栈中有空闲空间时，入栈操作时间复杂度为O(1)，当栈中没有空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。 也就是说，对于入栈操作来说，最好时间复杂度为O(1)，最坏情况时间复杂度为O(n)。那平均情况下的时间复杂度是多少呢？还记得时间复杂度分析方法中的摊还分析法吗？这个入栈操作的平均情况的时间按复杂度正好可以用摊还分析法来分析。 为了分析方便，我们先做一些假设和定义： 栈空间不够时，我们重新申请一个是原来大小两倍的数组； 为了简化分析，假设只有入栈操作没有出栈操作； 定义不涉及内存搬移操作的入栈操作为simple-push操作，时间复杂度为O(1)。 如果当前栈大小为K，并且已满，当在有新的的数据要入栈时，就需要重新申请2倍大小的内存，并且做K个数据的搬移操作，然后在入栈。但是，接下来的K-1次入栈操作，我们都不需要在重新申请内存和搬移数据，所以这k-1次都只需要一次simple-push操作就可以完成。如下图： 从上图看出，这K次入栈操作，总共涉及了K个数据的搬移，以及K次simple-push操作。讲K个数据搬移均摊到K次入栈操作，那每个入栈操作只需要一个数据搬移和一个simpel-push操作。以此类推，入栈操作的时间复杂度为O(1)。 通过这个例子分析，也验证了前面讲的，均摊时间复杂度一般都等于最好时间复杂度。因为在大部分情况下，入栈操作的时间复杂度都是O(1)，只有在个别情况才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下耗时就接近O(1)。 栈的应用场景 栈在函数调用中的应用前面讲的都比较偏理论，我们现在来看，栈在软件工程中的实际应用。栈作为一个比较基础的数据结构，应用场景还是蛮多的。其中比较经典的一个应用场景就是函数调用栈。 我们知道，操作系统给每个线程分配了一块独立的内存空间，这块内存空间被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。为了更好理解函数调用栈，一起来看一下这段代码的执行过程。 123456789101112131415int main()&#123; int a = 1; int ret = 0; int res = 0; ret = add(3,5); res = a + ret; printf("%d", res); return 0;&#125;int add(int x, int y)&#123; int sum = 0; sum = x + y; return sum;&#125; 从代码中我们可以看出，main函数调用了add函数，获取计算结果，并且与临时变量a相加，最后打印res的值，为了清晰的看到这个过程的函数栈里对应的入栈、出栈过程，我这里画了一张函数栈图： 栈在表达式求值中的应用我们再来看一个栈的常见应用场景，编译器如何利用栈实现表达式求值。 这里我们用一个只包含加减乘除四则运算的表达式来解释，比如：34+13*9+44-12/3。对于这个四则运算，我们人脑可以很快算出来，但是对于计算机来说，理解这个表达式本身就是个挺难的事。如果是你，你会怎么实现一个表达式求值的功能呢？ 实际上，编译器就是通过两个栈来实现的。其中一个是保存操作数的栈，另一个保存运算符的栈。我们从左往右遍历表达式，当遇到数字，我们直接压入操作数栈。当遇到运算符，就与运算符的栈顶元素进行比较。如果运算符比当前栈顶元素的优先级高，就直接压入运算符栈中，如果比栈顶元素的优先级低或者相同，就将当前栈顶元素取出，再从操作数栈中取出两个操作数，然后进行运算，再把计算完的结果压入操作数栈，继续比较。 这里用一个简单的例子：3+5*8-6 我将这个表达式的计算过程画成一个图，结合图来理解刚才的计算过程。 栈在括号匹配中的应用出了用栈来实现表达式求值，我们还可以借助栈来检查表达式中的括号是否匹配。 我们同样简化一下背景，假设表达式只包含三种括号，圆括号()、方括号[]、花括号{}，并且他们可以任意嵌套。比如{[{}]}、[([]){()}]等都为合法格式，而{[}()或[{(}]为非法格式。那现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？ 这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从做到右一次扫描字符串。当扫描到左括号时，则将其压入栈中，当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如”(“和”)”匹配、”[“和”]”匹配、”{“和”}”匹配，则继续扫描剩下的字符串。如果扫描过程中，遇到不能匹配的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成后，如果栈为空，则说明字符串为合法格式；否则说明有为匹配的左括号，为非法格式。 解答开篇 好了，理解了栈的概念和应用，再回头看看开篇的问题。如何实现浏览器的前进、后退功能？学过栈之后，就可以用两个栈完美的解决这个问题了。 我们使用两个栈X、Y，把首次浏览的页面压入栈X，当点击后退按钮时，依次从栈X中出栈，并将出栈的数据依次放入栈Y。当我们点击前进按钮时，依次取出栈Y中的数据，并放入栈X。当X中没有数据时，说明没有页面可以后退了。当Y中没有数据时，说明没有页面可以点击前进按钮进行浏览了。 当我们依次浏览了a、b、c三个页面，我们依次把a、b、c压入栈，这个时候，两个栈的数据就是如下这个样子： 当我们通过浏览器的后退按钮，从页面c后退到页面a之后，我们依次把c、b从栈X中弹出，并且依次放入栈Y中，这个时候栈中的数据就是如下： 这时候，又想看页面b，于是点击前进按钮回到b页面，我们就把b再从栈Y中取出，放入X，此时栈中数据如下： 总结 栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它的最大特点。栈既可以通过数组来实现，也可以通过链表来实现。不管是数组实现的栈，还是链表实现的栈，他们的入栈、出栈时间复杂度都为O(1)。在基于数组实现的动态扩容的顺序栈中，时间复杂度均为O(1)，重点是入栈时间复杂度中关于摊还分析法的掌握。 思考 1、再讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用”栈”这种数据结构来保存临时变量呢？用其他数据结构可以吗？2、我们知道，JVM内存管理中有个“堆栈”的概念。栈内存用来白村局部变量和方法调用，堆内存用来存储java中的对象。那JVM里面的“栈”和我们这里的“栈”一样吗？不一样的话，为什么叫“栈”呢？]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-怎样写好链表代码]]></title>
    <url>%2Fposts%2F2018-09-13-%E7%AE%97%E6%B3%95-%E6%80%8E%E6%A0%B7%E5%86%99%E5%A5%BD%E9%93%BE%E8%A1%A8%E4%BB%A3%E7%A0%81.html</url>
    <content type="text"><![CDATA[上一节讲了链表相关的基础知识，有人可能会说基础知识我都掌握了，但是写链表代码还是很费劲怎么办？确实是这样的，想要写好链表代码并不是容易的事，尤其是那些复杂的链表操作，比如链表反转、有序链表合并等，写的时候非常容易出错。 为什么链表代码这么难写？究竟怎么样才能比较轻松的写出正确的链表代码呢？ 只要愿意投入时间，我觉得大多数人都是可以学会的。比如，如果你真能花一整天或者一个周末，就去写链表反转这一个代码，多写几次，知道能毫不费力的写出bug free的代码，这个坎儿还会很难跨吗？ 当然，自己有决心并且付出精力是成功的先决条件，除此之外，我们还需要掌握一些技巧和方法。下面我总结了几个写链表的代码技巧，如果能熟练掌握这几个技巧，叫上主动和坚持，轻松拿下链表代码完全没有问题。 理解指针或引用的含义事实上，看懂链表的结构并不是很难，但是一旦把它和指针混在一起，就很容易让人摸不着头脑。所以要想写好链表代码，首先就要理解好指针。 有些语言有“指针”的概念，比如C语言，有些语言没有指针，取而代之的是“引用”，比如Java、Python等。不管是指针还是引用，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。 接下来，我会拿C语言中的指针来讲解。如果你用的是Java或者其他语言也没关系，把它理解成引用就可以了。 实际上，对于指针的理解，只需要记住下面这句话就可以了：将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。 在编写链表代码的时候，经常会有这样的代码：p-&gt;next = q，这行代码是说p结点中的next指针存储了q结点的内存地址。还有一个更复杂的，也是写链表代码经常用到的：p-&gt;next = p-&gt;next-&gt;next，意思是说p结点的next指针存储了p结点的下下一个结点的内存地址。 掌握了指针或者引用的概念，应该可以很轻松的看懂链表代码。 警惕指针丢失和内存泄露不知道你有没有这样的感觉，写链表代码的时候指针指来指去，一会就不知道指针到哪里了。所以我们在写代码的时候，一定不要弄丢了指针。 如上图所示，当我们在a结点和b结点之间插入结点c，假设当前指针p指向结点a。如果我们将代码写成下面这个样子，就会发生指针丢失和内存泄露。 12p-&gt;next = c; // 将p的next指针指向c结点c-&gt;next = p-&gt;next; //将c结点next指针指向b结点 当p-&gt;next指针在完成第一步操作之后，已经不再指向b结点了，而是指向结点c，因此，第二行代码相当于将c-&gt;next指针指向了自己。因此整个链表断裂成了两半，从结点b之后的所有结点都无法访问了。 对于有些语言来说，比如C语言，内存管理是由程序员负责的，如果没有手动释放结点对应的内存空间，就会产生内存泄露，所以，我们在插入结点时，一定要注意操作的顺序。要先将c结点的next指针指向b，再将a结点的next指针指向c，这样才不会丢失指针，导致内存泄露。 利用哨兵简化实现难度首先，我们回顾一下单链表的插入、删除操作。如果我们在结点p之后插入一个结点，只需要下面两行代码就可以了。 12new_node-&gt;next = p-&gt;next; p-&gt;next = new_node; 但是当我们向一个空链表中插入第一个结点，刚刚的逻辑就不能用了。我们需要进行下面这样的特殊处理，其中head表示链表的头结点。所以从这段代码可以看出，对于单链表的插入操作，第一个结点和其他结点的插入逻辑是不同的。 1234if (head == null)&#123; head = new_node;&#125; 同样再来看一下链表的删除操作，如果要删除p结点的后继点点，我们只需要一行代码就可以搞定： 1p-&gt;next = p-&gt;next-&gt;next； 但是如果要删除链表的最后一个结点，这样的代码就不行了。跟插入类似，我们也需要对这种情况特殊处理。代码如下： 1234if (head-&gt;next == null)&#123; head = null;&#125; 可以看出，针对链表的插入、删除操作，需要对第一个结点的插入和最后一个结点的删除情况进行特殊处理。这样代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错。那如何来解决这个问题呢？ 这时上面提到的哨兵就出场了。现实中的哨兵，解决的是国家之间的边界问题。同理我们这里的哨兵也是解决“边界问题的”，不直接参与业务逻辑。 还记得如何表示一个空链表呢？head=null表示链表中没有结点了，其中head表示头结点指针，指向链表中的第一个结点。 如果我们引入哨兵结点，在任何时候，不管链表是不是为空，head指针都会一直指向这个哨兵结点。我们把这种有哨兵的链表叫做带头链表，相反，没有哨兵结点的链表叫做不带头链表。 如下我画了一个带头链表，可以发现，哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑。 实际上，这种利用哨兵简化编程难度的技巧，在很多代码实现中都有用到，比如插入排序、归并排序、动态规划等。这里用C语言实现一个简单的例子，不涉及语法方面的高级知识，你可以类比其他语言。 代码一： 123456789101112131415161718// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; int i=0; // 这里有两个比较操作： i&lt;n 和 a[i] == key while(i&lt;n)&#123; if(a[i] == key)&#123; retrun i; &#125; ++i; &#125; retrun -1;&#125; 代码二： 12345678910111213141516171819202122232425262728293031323334// 在数组a中，查找key，返回key所在的位置，其中n代表数组，a代表长度// 为了更好的解释，这里举了个例子来说明// a = &#123;4,2,3,5,9,6&#125; key = 7int find(char* a, int n, char key)&#123; // 边界条件处理，如果a为空，或者n&lt;=0 if(a == null || n&lt;=0)&#123; return -1; &#125; // 这里因为要将a[n-1]设为哨兵，所以特殊处理这个值 if(a[n-1] == key)&#123; return n-1; &#125; // 临时变量保存a[n-1]，以便之后恢复，这里temp = 6 char temp = a[n-1]; // 把key值放到数组a[n-1]，此时a=&#123;4,2,3,5,9,7&#125; a[n-1] = key; int i=0; // 此时while循环比起代码一，少了i&lt;n这个比较操作 while(a[i] == key)&#123; ++i; &#125; // 将数组a[n-1] 恢复为原来的值 a[n-1] = temp; // 如果i = n-1，说明数组中没有要找的key if(i == n-1)&#123; return -1; &#125; // 否则，说明找到了key，位置为i else&#123; return i; &#125;&#125; 对比两段代码，在字符串a很长的时候，比如几万、几十万，你觉得那段代码执行更快呢？答案是代码二。因为两端代码中执行次数最多的就是while循环那一部分。在第二段代码中，我们通过一个哨兵a[n-1]=key，成功省掉了一个比较语句，不要小看了这一句，当积累上万次、几十万次的时候，累积的时间就很明显了。 当然，这里只是说明哨兵的作用，写代码的时候千万不要写成第二段代码那样，可读性太差了，大部分情况下，我们并不需要追求如此极致的性能。 重点留意边界条件处理软件开发中，代码在以下边界或者异常情况下，最容易产生bug。链表代码也不例外，要实现没有bug的链表代码，一定要在编写的过程中以及编写完成后，检查边界条件是否考虑全面，以及边界条件下代码是否能运行。 我经常用来检查链表代码是否正确执行的边界条件有这么几个： 如果链表为空时，代码是否能正常工作？ 如果一个链表只包含了一个结点，代码能否正常工作？ 如果链表只包含两个结点时，代码能否正常工作？ 代码逻辑在处理头结点和尾结点时，是否能正常工作？ 当你写完链表代码之后，除了看下你写的代码在正常情况下能否工作，还要看下在上面我列举的杰哥边界条件下，代码能否正常工作。 当然边界条件不止我列举的这些，针对不同的场景，可能还有特定的边界条件，需要自己去思考，不过套路都是一样的。 其实，不光是写链表代码，在写任何代码的时候，千万不要只是实现业务正常情况下的功能就行了，一定要多想想会遇到哪些边界情况或者异常情况，遇到了应该如何应对，这样写出来的代码才够健壮。 举列画图，辅助思考对于稍微复杂的链表操作，比如前面我们提到的单链表反转，指针一会指这，一会指那，总感觉脑容量不够，想不清楚。这时候可以采用举列法和画图法，来进行辅助分析。 你可以找一个具体的例子，把它画在纸上，释放一些脑容量，留更多的给逻辑思考，这样就会感觉思路清晰很多。比如往单链表中插入一个结点，可以先把各种情况都举一个例子，画出插入前和插入后的链表变化，如图所示： 看着图写代码，是不是简单多了。而且当我们写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易发现代码中的Bug。 多写多练，没有捷径如果你已经理解并掌握了这些方法，但是手写代码还是会出现各种各样的错误，也不要着急，多写多练。把常见的链表操作多写几遍，出问题就一点点调试，熟能生巧。 下面我精选了5个常见的链表操作，这要把这几个操作写熟练，不熟就多练几遍，保证之后不会在害怕写链表代码。 单链表反转 链表中环的检测 两个有序链表合并 删除链表倒数第n个结点 求链表的中间结点 我觉得，写链表代码是最考验逻辑思维能力的，因为链表到处都是指针的操作，边界条件的处理，一个不慎就会产生bug。链表代码写的好坏，可以看出一个人写代码是否细心，考虑问题是否全面，思维是否缜密，所以很多面试都喜欢让人手写链表代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class SingleLinkedList&lt;T&gt;&#123; private Node&lt;T&gt; head; private int size; public SingleLinkedList()&#123; this.head = new Node&lt;T&gt;(null); &#125; public void reverse()&#123; // 反转单链表 if (head.next == null) &#123; return ; &#125; Node&lt;T&gt; p, q, r; p = head.next; q = p.next; p.next = null; while(q!=null)&#123; r = q.next; q.next = p; p = q; q = r; &#125; head.next = p; &#125; private class Node&lt;T&gt; &#123; T val; Node&lt;T&gt; next; Node(T val)&#123; this.val = val; &#125; &#125; @Override public String toString()&#123; if (head.next == null) &#123; return "[]"; &#125; StringBuilder sb = new StringBuilder(); sb.append("["); Node&lt;T&gt; p = head.next; while(p.next != null)&#123; sb.append(p.val).append(", "); p = p.next; &#125; sb.append(p.val).append("]"); return sb.toString(); &#125; public static void main(String[] args)&#123; SingleLinkedList&lt;String&gt; sl = new SingleLinkedList&lt;&gt;(); sl.add("1"); sl.add("2"); sl.add("3"); System.out.println(sl); // [1, 2, 3] System.out.println(sl.get(0));// 1 sl.delete("3"); System.out.println(sl); // [1, 2] sl.add("3"); sl.reverse(); System.out.println("第一次反转后："+sl); // [3, 2, 1] sl.reverse(); System.out.println("第二次反转后："+sl); // [1, 2, 3] &#125;&#125; (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-链表]]></title>
    <url>%2Fposts%2F2018-09-12-%E7%AE%97%E6%B3%95-%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[前言 今天我们来聊聊“链表 LinkedList”这个数据结构，学习链表有什么用呢，我们先来讨论一个经典的链表使用场景，那就是LRU缓存淘汰算法。 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被占满时，那些数据应该被清理出去，那些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有这么三种：先进先出策略FIFO(First In First Out)、最少使用策略LFU(Least Frequently Used)、最近最少使用策略LRU(Least Recently Used)。 今天我们的问题是，怎样用链表来实现一个LRU缓存淘汰策略？ 链表及其结构 相比数组，链表是一种稍微复杂一点的数据结构，掌握起来也要比数组要困难一些。数组和链表是两个非常基础、非常常用的数据结构。所以要掌握甚至精通，同时理解其思想。 我们先从底层存储结构来看一下二者的区别： 为了直观的对比，我画了一张图，从图中可以看到，数组需要一块连续的内存空间来存储，对内存的要求比较高，如果我们申请一个100MB大小的内存空间，当内存中没有连续的、足够大的内存空间时，即便剩余的总空间大于100MB，仍然会申请失败。 而链表恰恰相反，它并不需要一块连续的内存空间，他通过“指针”将一组零散的内存块连接起来使用，所以申请一块大小是100MB的链表，根本不会有问题。 链表的结构五花八门，今天我们着重介绍三种最常用的链表结构：单链表、双向链表、循环链表。 单链表首先来看最简单、最常用的单链表。我们刚讲到，链表是用指针将一组零散的内存块串联在一起，其中，我们把内存块称为链表的“结点”。为了使所有的节点串联起来，每个链表的结点出了需要保存数据之外，还需要记录链上下一个结点的地址，如图所示，我们把这个记录下一个结点指针地址的指针叫做后继指针 next。 从上面单链表的结构图中，可以发现，单链表中有两个结点是比较特殊的，分别是第一个节点和最后一个结点，我们习惯性的把第一个结点称为头结点，最后一个节点称为尾结点。其中头结点用来记录链表的基地址，我们可以通过它遍历得到整个链表。而尾结点的特殊之处在于，指针不是指向下一个结点，二是指向了一个空地址null，表示这是链表的最后一个结点。 与数组一样，链表也支持数据的插入、查找、删除操作。我们知道在进行数组的插入、删除操作时，为了保持内存的连续性，需要做大量的数据搬移操作，所以时间复杂度是O(n)。而在链表中插入或者删除一个数据，我们并不需要保持内存的连续性而搬移结点，因为链表本身的存储空间就不是连续的。所以在链表中插入删除一个数据是非常快的。 为了方便理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度为O(1)。 但是有利就有弊，链表想要随机访问第K个元素就没有数组那么高效了。因为链表中的数据并非是连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就可以直接计算出对应的内存地址，而是需要一个一个结点依次遍历，直到找到对应的结点。 你可以把链表想象成一个队伍，每个人都知道自己前面的人是谁，所以当我们希望知道排在第K为的人是谁的时候，就需要从第一个人开始，一个一个往下数。所以链表随机访问的性能没有数组好，时间复杂度为O(n)。 好了，单链表了解了，下面来看看另外两个复杂的链表：循环链表和双向链表。 循环链表循环链表是一种特殊的单链表。实际上，循环链表也很简单，它和单链表唯一的区别就在尾结点。我们知道，单链表的尾结点是指向空地址，表示这是最后的节点了，而循环链表的尾结点的指针是指向链表的头结点。从下图中可以看出，循环链表想一个环一样首尾相连，所以叫循环链表。 和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环形结构特点时，就特别适合采用循环链表，比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表的话，代码就会简洁很多。 双线链表接下来再看一个稍微复杂，在实际的软件开发中，也更加常见的链表结构：双向链表。 单链表只有一个方向，节点只有一个后继指针，next指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。 从上图可以看出，双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单向链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表的操作灵活性。那相比单向链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持O(1)时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的删除、插入操作比单链表要简单、高效。 你可能会说，单链表的插入、删除操作的时间复杂度都已经是O(1)了，双向链表还能怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法的书籍也是这么说得，但是这种说法实际上是不准确的，或者说是有先觉条件的。 我们再来分析一下链表的两个操作，先来看删除操作。在实际的软件开发中，从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值的”结点 删除给定指针指向的结点 对于第一种情况，不管是单链表还是双向链表，为了查找到值等于某个给定值的结点，都需要从头开始一个一个依次遍历对比，知道找到值等于给定值的结点，再通过前面讲的指针操作将其删除。 尽管单纯的删除操作时间复杂度都是O(1)，但是遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。 对于第二种情况，我们已经找到了要删除的结点，但是删除某个结点q需要知道前驱结点，而单链表并不支持直接获取前驱结点，所以为了找到前驱结点，我们还是要从头结点开始遍历链表，知道p-&gt;next = q，说明p是q的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以针对第二种情况，单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大优势，双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。 除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查找的效率也要比单向链表高一些。因为我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是向前查找还是往后查找，所以平均只需要查找一半的数据。 现在，有没有觉得双向链表比单向链表更加高效呢？这就是问什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉Java语言，你肯定用过LinkedHashMap这个容器，如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。 实际上，这里有一个更重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足时，如果我们更追求代码的执行速度，我们就可以选择空间复杂度相对较高，但时间复杂度相对较低的算法和数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单机片中，这个时候，就要反过来用时间换空间的涉及思路。 还是开篇缓存的例子，缓存实际上就是利用了空间换时间的例子。虽然我们将数据存放在磁盘上，会比较节省内存，但是每次查询数据都要查询一遍磁盘，会比较慢。但是我们通过缓存技术，事先将数据加载在内存中，虽然会比较耗费内存空间，但是每次查询数据的速度就大大提高了。 所以对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)进行优化；而消耗过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗。你还能想到其他时间换空间或者空间换时间的例子吗？ 了解了循环链表和双向链表，如果把这两种链表整合在一起就是一个新的版本：双向循环链表。我想不需要我多讲，你应该知道双向循环链表长什么样子了吧？ 链表 VS 数组性能大比拼 通过前面的学习，你应该知道，数组和链表是两种截然不同的内存组织方式，正是因为内存存储的区别，他们插入、删除、随机访问的时间复杂度正好相反。 时间复杂度 数组 链表 插入删除 O(n) O(1) 随机访问 O(1) O(n) 不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就能决定使用那哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存并不好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，可能没有足够的连续内存空间分配给它，导致“内存不足”。如果声明的数组过小，则可能出现不够用的情况，这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然的支持动态扩容，我觉得这也是它与数组最大的区别。 你可能会说，Java中也有ArrayList容器，也可以支持动态扩容啊？我们上一节已经讲过，当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了，就会申请一个更大的空间，将原数组拷贝过去，而数据拷贝的操作是非常耗时的。 我举一个稍微极端的例子。如果我们用ArrayList存储了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList会申请一个1.5GB的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上，听起来是不是就很耗时。 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的内存空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是java语言，就有可能会导致频繁的GC(Garbage Collection 垃圾回收)。 所以在实际的开发项目中，要根据不同的项目情况，权衡究竟是选择数组还是链表。 解答开篇 好了，我们现在回过头来看，如何基于链表实现LRU缓存淘汰算法？ 我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新数据被访问时，我们从链表头部开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，再插入到链表的头部。 如果此数据没有缓存在链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 这样我们就实现了一个LRU缓存，是不是很简单。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为O(n)。 实际上，我们可以继续优化这个实现思路，比如引入哈希表(hash table)来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)。这个优化方案，等讲到哈希表的时候再讲。 基于链表的实现思路，实际上还可以用数组来实现LRU缓存淘汰策略。如何利用数组实现LRU缓存淘汰策略？ 内容小结 今天我们讲了一种跟数组“相反”的数据结构，链表。他跟数组一样，也是非常基础、非常常用的数据结构。不过链表要比数组稍微复杂，从普通链表衍生出来好几种链表结构，比如双向链表、循环链表、双向循环链表。 和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过在具体的软件开发中，要对数组和链表的各种性能进行对比，综合来使用两者中的一个。 课后思考 如何判断一个字符串是否是回文字符串呢？今天的思考题就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那如何来判断是一个回文串呢？相应的时间空间复杂度是多少。 (adsbygoogle = window.adsbygoogle || []).push({}); 本章代码：GitHub 带头单链表代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211import java.util.NoSuchElementException;public class SinglyLinkedList&lt;T&gt;&#123; private Node&lt;T&gt; head; private int size; public SinglyLinkedList()&#123; this.head = new Node&lt;&gt;(null); &#125; // 链表头部插入值 private void linkFirst(Node&lt;T&gt; newNode)&#123; newNode.next = head.next; head.next = newNode; size++; &#125; // 链表尾部插入值 public void linkLast(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkLast(newNode); &#125; private void linkLast(Node&lt;T&gt; newNode)&#123; Node&lt;T&gt; p = head; while (p.next!=null)&#123; p=p.next; &#125; p.next = newNode; size++; &#125; // 获取头部值 public T getFirst()&#123; if (head.next == null)&#123; throw new NoSuchElementException(); &#125; return head.next.val; &#125; // 获取尾部值 public T getLast()&#123; Node&lt;T&gt; p = head.next; while (p.next!=null)&#123; p = p.next; &#125; return p.val; &#125; // 添加 public void add(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkLast(newNode); &#125; // 在某处索引插入 public void add(int index, T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); Node&lt;T&gt; p = node(index); insert(p, newNode); &#125; private void insert(Node&lt;T&gt; p, Node&lt;T&gt; newNode)&#123; Node&lt;T&gt; q = head; while (q!=null &amp;&amp; q.next!=p)&#123; q = q.next; &#125; if (q == null)&#123; return; &#125; newNode.next = p; q.next = newNode; &#125; // 根据值删除某个节点 public boolean delete(T val)&#123; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; !p.next.val.equals(val))&#123; p = p.next; &#125; if (p.next== null)&#123; return false; &#125; p.next = p.next.next; return true; &#125; // 根据索引删除某结点 public T delete(int index)&#123; Node&lt;T&gt; deleteNode = node(index); return deleteNode(deleteNode); &#125; private T deleteNode(Node&lt;T&gt; deleteNode)&#123; final T element = deleteNode.val; Node&lt;T&gt; p = head; while (p.next!= null &amp;&amp; p.next != deleteNode)&#123; p = p.next; &#125; if (p.next == null)&#123; return null; &#125; p.next = deleteNode.next; return element; &#125; // 根据索引获取值 public T get(int index)&#123; if (index &gt;= size || index &lt; 0)&#123; throw new IndexOutOfBoundsException("Index: "+index + ", Size: "+size); &#125; return node(index).val; &#125; // 通过value 查找对应的索引 public int indexOf(T val)&#123; int index = 0; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; p.next.val!=val)&#123; p = p.next; index ++; &#125; if (p.next == null)&#123; index = -1; &#125; return index; &#125; public boolean contains(T val)&#123; Node&lt;T&gt; p = head; while (p.next !=null &amp;&amp; p.next.val!=val)&#123; p = p.next; &#125; return p.next != null; &#125; private Node&lt;T&gt; node(int index)&#123; if (index &gt;= size || index &lt; 0)&#123; throw new IndexOutOfBoundsException("Index: "+index + ", Size: "+size); &#125; Node&lt;T&gt; p = head.next; int i=0; while (i&lt;size)&#123; if (i == index)&#123; break; &#125; p = p.next; ++i; &#125; return p; &#125; public void push(T val)&#123; Node&lt;T&gt; newNode = new Node&lt;&gt;(val); linkFirst(newNode); &#125; public T pop()&#123; return unlinkedFirst(); &#125; private T unlinkedFirst()&#123; Node&lt;T&gt; first = head.next; if (first == null)&#123; throw new RuntimeException("没有元素"); &#125; return unlinkedFirst(first); &#125; private T unlinkedFirst(Node&lt;T&gt; node)&#123; final T element = node.val; head.next = head.next.next; node.next = null; node.val = null; size--; return element; &#125; // 单链表反转 public void reverse()&#123; // 链表为空或者链表只有一个元素时 if (head.next == null || size &lt;=1 )&#123; return; &#125; Node&lt;T&gt; p = head.next; Node&lt;T&gt; q = p.next; Node&lt;T&gt; r; p.next = null; while (q !=null)&#123; r = q.next; q.next = p; p = q; q = r; &#125; head.next = p; &#125; public int size()&#123; return size; &#125; // 打印链表 example: [1, 2, 3] @Override public String toString() &#123; if (head.next == null)&#123; return "[]"; &#125; StringBuilder sb = new StringBuilder(); sb.append("["); Node&lt;T&gt; p = head.next; while (p.next!=null)&#123; sb.append(p.val).append(", "); p = p.next; &#125; sb.append(p.val).append("]"); return sb.toString(); &#125; public static class Node&lt;T&gt;&#123; private T val; private Node&lt;T&gt; next; Node(T val)&#123; this.val = val; &#125; &#125;&#125; 基于链表的LRU缓存代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public interface LRUCache&lt;T&gt; &#123; void put(T val); T get(T val); int Size();&#125;class ListLRUCache&lt;T&gt; implements LRUCache&lt;T&gt; &#123; private SinglyLinkedList&lt;T&gt; lruList; private static final int DEFAULT_CAP=10; // 缓存容量 private int cap; // 缓存使用大小 private int size; public ListLRUCache()&#123; this(DEFAULT_CAP); &#125; public ListLRUCache(int cap)&#123; this.cap = cap; this.lruList = new SinglyLinkedList&lt;&gt;(); &#125; @Override public void put(T value) &#123; // 1、缓存满了 // 如果该列表中没有该数据 if (size == cap)&#123; // 1、缓存满了 // 删除最后一个节点 lruList.delete(size-1); // 将该数据插入到链表头部 lruList.push(value); &#125;else &#123; // 2、缓存未满 // 直接在列表头部插入该数据 lruList.push(value); size++; &#125; &#125; @Override public T get(T val) &#123; T result = null; if (lruList.contains(val))&#123; // 在list中,从list中获取该数据 int index = lruList.indexOf(val); result = lruList.get(index); System.out.println("从缓存中获取"); // 将该节点插入到链表头部 lruList.delete(index); lruList.push(val); &#125;else&#123; // 如果该列表中没有该数据 System.out.println("缓存中没有该数据！"); if (size == cap)&#123; // 1、缓存满了 // 删除最后一个节点 lruList.delete(size-1); // 将该数据插入到链表头部 lruList.push(val); System.out.println("缓存已满！将该数据插入到缓存"); &#125;else &#123; // 2、缓存未满 // 直接在列表头部插入该数据 lruList.push(val); size++; System.out.println("将该数据直接插入到缓存"); &#125; // 如果有数据库，该数据从数据库中获取 result = val; &#125; return result; &#125; public int Size()&#123; return size; &#125;&#125; 字符串是否是回文字符串：12]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-数组]]></title>
    <url>%2Fposts%2F2018-09-10-%E7%AE%97%E6%B3%95-%E6%95%B0%E7%BB%84.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({}); 前言 提到数组，我想你肯定不陌生，甚至还会自信的说他很简单。 是的，在每一种编程语言中，基本都会有数组这种数据类型。尽管数组看起来非常基础、简单，但是我估计很多人都没有理解这个基础数据结构的精髓。 在大部分的数据结构中，数组都是从0开始编号的，但是为什么数组要从0开始，而不是1开始呢？从1开始不是更符合人类的思维习惯吗？下面我们通过本篇文章来认识这个问题。 数组如何实现随机访问？ 什么是数组呢？数组是一种线性表结构，它用一组连续的内存空间，来存储一组具有相同数据类型的数据。 这里有几个关键词： 第一是线性表。顾名思义，线性表就是数据像一条线一样的结构。每个线性表上的数据最多只有前后两个方向。除了数组，链表、队列、栈等也是线性表结构。 与线性表相对应的概念是非线性表，比如二叉树、堆、图，之所以叫非线性，是因为在非线性表中，数据之间并不是简单的前后关系。 第二个是连续的内存空间和相同类型的数据。正是因为这两个限制，所以才有一个堪称杀手锏的特性：“随机访问”。但有利就有弊，这两个限制也让数组的很多操作变得非常低效，比如在数组中插入、删除一个数据，为了保证连续性，就需要做大量的数据搬移工作。 说到数据的随机访问，那么数组是如何实现很具下标随机访问数组元素的吗？ 我们拿一个长度为10的int类型的数组int[] a = new int[10] 来举例。在如下图中，假设计算机给数组a[10] 分配了一块连续的内存空间000-039，其中首地址为000。 我们知道计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问某个数组元素时，它会通过寻址公式，计算出该元素的内存地址。 $$ a[i]\_address = base\_address + i * data\_type\_size $$ 其中base address表示数组的基地址，data_type_size表示数组中的每个元素的大小，在这个例子中，数组中存储的int类型，所以data_type_size就是4个字节。 很多人在面试中回答数组和链表的区别都会这么说：“链表适合插入、删除，时间复杂度为 O(1)；数组适合查找，查找时间复杂度为O(1)”。实际上这种表述是不准确的。数组是适合查找操作，但是查找的复杂度并不是O(1)，即便是排好序的数组，用二分查找时间复杂度也是$O(logN)$。所以正确的表述应该是数组的随机访问的复杂度是O(1)。 低效的“插入”和“删除”前面我们提到，数组为了保持内存数据的连续性，会导致插入、删除操作比较低效，现在我们就来看看究竟为什么会导致低效？ 插入操作假设数组的长度为n，现在需要将一个数据插入到数组中的第k个位置。为了把第k个位置腾出来，我们需要将k-n这部分的元素都往后顺挪一位。 如果是在数组的末尾插入元素，那就不需要移动数据，时间复杂度为O(1)；但是如果在数组开头插入一个元素，那所有的元素都需要后移一位，所以最坏时间复杂度为O(n)；因为在每个位置插入元素的概率是一样的，所以平均时间复杂度为$ (1+2+3+…+n)/n = O(n) $ 。 所以对于插入的时间复杂度：最好的O(1)，最坏O(n)，平均O(n)。 如果数组中的元素是有序的，并且插入新元素也要保证数组有序，那么就必须按照刚才的方法移动数据。但是如果数组中存储的数据没有任何规律，只是被当来存储数据的集合，那么如果在k处插入一个数据，可以将k处的数据移到数组的末尾，然后替换k处数据为要插入的数据，这种插入处理技巧可以将时间复杂度降为O(1)。 删除操作跟插入数据类似，如果要删除第k个位置的数据，为了保持内存的连续性，也需要搬迁数据，不然数组中间就会出现断层，内存就不连续了。 和插入类似，如果删除数组末尾的数据，则是最好时间复杂度为O(1)；如果删除开头的数据，则最坏时间复杂度为O(n)，平均情况时间复杂度也为O(n)。 实际上，在某些特殊场景下，我们并不一定追求数组中数据的连续性，如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？ 我们继续来看一个例子，数组a[10]中存储了8个元素：a,b,c,d,e,f,g,h。现在我们要依次删除a,b,c这三个元素。 为了避免d,e,f,g这几个数据会被搬移三次，我们可以先记录下已删除的数据，每次的删除并不是真正的搬移数据，只是记录数据已经被删除，当数组没有更多空间存储数据事，我们再进行一次真正的删除操作，这样就大大减少了删除数据之后导致的数据迁移。 如果你了解JVM，会发现，这不就是JVM的标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或算法，而是要学习他背后的思想和处理技巧，这些东西才是最优价值的。如果你细心留意，不管是在开发还是在架构设计中，总能找到某些数据结构和算法的影子。 警惕数组越界问题了解数组的几个基本操作后，再来看看数据的访问越界问题。 这里以一段C语言代码为例来进行说明： 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(i; i&lt;=3; i++)&#123; arr[i] = 0; printf("hello world\n"); &#125; return 0;&#125; 你发现问题了吗？这段代码并不是打印三行”hello world”，而是会无限打印”hello world”，这是为什么呢？ 我们知道数组大小为3，分别为a[0]、a[1]、a[2]，而我们代码因为书写错误，for循环结束条件错写为了i&lt;=3而非i&lt;3，所以当i=3时，数组访问越界。 我们知道，在C语言中，只要不是访问受限的内存，所有的内存空间都是可以自由访问的。而根据我们前面讲的寻址公式，a[3]也会被定位到一个某块不属于数组的内存地址上，而在C语言的内存管理中，在局部变量分配空间的顺序是跟变量的声明顺序直接相关，同时按照内存由高到低的顺序进行空间分配，所以在内存布局中，i变量的地址刚好是在数组arr之后的一个字，所以在循环体中，将arr[3]赋值为0，实际上却是将计数器i的值设为0，这就导致了该函数的死循环。 关于C语言中编译器关于变量的内存分配顺序可以看此篇文章理解一下: https://blog.csdn.net/liuhuiyi/article/details/7526889 数组越界在C语言中是一种未决行为，并没有规定数组访问越界编译器应该如何处理。因为数组访问的本质就是访问一段连续的内存地址，只要数组通过偏移计算得到的内存地址是可用的，那么程序就不会报错。 所以在这种情况下，一般会出现莫名其妙的错误，而且很多计算机病毒也是利用了代码中数组越界可以访问到非法地址的漏洞，来攻击系统，所以代码中一定要警惕数组的越界访问。 但并非所有的编程语言都想C一样，将数组越界检查交给程序员来做，像Java、Python本身就会做越界检查，比如java会抛出java.lang.ArrayIndexOutOfBoundsException的异常，Python会有IndexError: list index out of range的错误。 容器能否完全代替数组?针对数组类型，很多语言提供了容器类。比如在java中提供了ArrayList、C++ STL中的vector等。那么在项目开发中，什么时候适合用数组，什么时候适合用容器呢？ 以java中ArrayList为例，ArrayList最大的优势就是可以将很多数组操作封装，比如数组的插入、删除等。另外，它还支持动态扩容，当存储空间不够时，它会自动扩容为原来的1.5倍。 不过由于扩容操作涉及内存申请和数据搬移，是比较耗时的，因此如果事先能确定存储数据的大小，最好在创建ArrayList时实现指定数据的大小。 作为高级语言编程者，是不是数组就无用武之地了呢？当然不是，有时候用数组会更合适些。 1、Java ArrayList无法存储基本类型，需要封装为Long、Integer等包装类类型，因此存在一定的拆装箱上的性能损耗，如果特别关注性能，或者要使用基本类型，则可以选择数组。 2、如果事先知道数据的大小，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以使用数组。 对于业务开发，直接使用容器就足够了，省时省力，毕竟一丢丢的性能损耗，不会影响到系统整体的性能，但是如果做一些非常底层的开发，这个时候数组就会优于容器，成为首选。 解答开篇为什么数组的索引是从0开始，而不是从1开始呢？ 从数组存储的内存模型来看，”下标”即索引最确切的定义应该是”偏移(offset)”，如果用arr表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]表示偏移k个type_size的位置，所以计算a[k]的内存地址只需要根据如下公式计算即可$$ a[k]\_address = base\_address + k * type\_size $$ 但是如果数组从1开始计数，那我们计算a[k]的内存地址计算公式就会变为：$$ a[k]\_address = base\_address + (k-1) * type\_size $$ 对比两个公式，从1开始的话，每次随机访问数组元素就多了一次减法指令。数组作为非常基础的数据结构，通过下标随机访问数组元素又是非常基础的操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作指令，数组选择了从小标从0开始，而不是从1开始。 不过解释的再多，我认为都算不上压倒性的证明，说数组编号非从0开始不可，最主要的原因可能是历史原因。 C语言设计者用0开始计数数组下标之后，Java、JavaScript等高级语言都效仿了C语言，或者说为了在一定程度上减少C语言程序学习Java的成本，继续沿用了从0开始计数的习惯。但是仍有很多语言中数组并不是从0开始的，比如Matlab。甚至还有一些语言支持负数下标，比如python。 思考题1、在数组的删除操作中，提到了JVM的标记清除垃圾回收算法的核心理念，如果熟悉Java、JVM，回顾下JVM的标记清除垃圾回收算法。2、上面讲到一维数组的寻址公式，类比一下，二维数组的内存寻址公式是怎么样的？ JVM标记清除垃圾回收算法：分为两个阶段，标记和清除。在大多数主流的虚拟机中采用可达性分析算法来判断对象是否存活，在标记阶段，会遍历所有GC ROOTS，将所有GC ROOTS可达对象标记为存活，只有当标记工作完成后，才会进行清理工作。 该算法最大的问题是会产生连续的内存空间碎片，同时标记和回收的效率都不高，但是对于只有少量垃圾产生时可以采用此种算法。 二维数组的寻址公式： 根据上图,对于一个二维数组int arr[m][n]，arr[i][j]的寻址公式为：$$ arr[i][j]\_address = base\_address + (i + n*j)*data\_type\_size $$ (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-最好、最坏、平均、均摊时间复杂度]]></title>
    <url>%2Fposts%2F2018-09-09-%E7%AE%97%E6%B3%95-%E6%9C%80%E5%A5%BD%E3%80%81%E6%9C%80%E5%9D%8F%E3%80%81%E5%B9%B3%E5%9D%87%E3%80%81%E5%9D%87%E6%91%8A%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6.html</url>
    <content type="text"><![CDATA[前言 前面我们讲过复杂度的大O表示法和几个分析技巧，还举了一些复杂度分析的例子，掌握了这些内容，对于复杂度分析这个知识点，已经达到及格线了。 这篇会着重讲一下复杂度分析的四个复杂度分析方面的知识： 最好时间情况复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。 最好、最坏时间复杂度 我们先用学过的知识试着分析以下代码的时间复杂度： 1234567891011int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; &#125; &#125; return pos;&#125; 上面代码实现的功能是在一个无序数组中，查找变量target的位置，如果找不到就返回-1，按照前面的分析方法，该段代码的时间复杂度为O(n)。 但是我们在数组中查找一个数据，并不需要每次都把整个数组都遍历一遍，优化一下这段代码： 123456789101112int findArray(int[] arr, int n, int target)&#123; int i = 0; int pos = -1; for(i; i&lt;n; i++)&#123; if(arr[i] = target)&#123; pos = i; break; &#125; &#125; return pos;&#125; 但是这时候问题来了，优化完之后，时间复杂度还是O(n)吗？ 因为要查找的变量target可能出现在数组的任何位置，如果要查找的target刚好出现在数组的开始位置，那么就不需要遍历剩余的数据，此时时间复杂度为O(1)。但是如果数组中不存在变量target，或者在最后一位，那我们就需要把整个数组都遍历一遍，时间复杂度就成了O(n)，所以这段代码在不同情况下时间复杂度是不同的。 为了表示代码在不同情况下的时间复杂度，我们需要引入三个概念：最好情况时间复杂度、最坏情况复杂度、平均时间复杂度。 顾名思义，最好情况时间复杂度就是，在最理想情况下，执行这段代码的时间复杂度。如上例中，在最理想情况下，查找的变量target刚好在第一个，这时候对应的时间复杂度就是最好情况时间复杂度。 同理，最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度，上例中，如果数组中没有要查找的变量target，我们需要把整个数组遍历一遍，所以最坏情况下对应的时间复杂度就是最坏情况复杂度。 平均时间复杂度我们都知道，最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率并不大。为了更好的表示平均情况下的时间复杂度，我们引入一个概念：平均情况时间复杂度，简称平均时间复杂度。 平均时间复杂度又该怎么分析呢？我们还是借助上面的例子。 要查找的变量target在数组中的位置，有n+1中情况： 在数组0 ~ n-1位置 n种情况和不在数组中1个情况。我们把每种情况下，需要遍历的元素个数累加起来，然后在除以n+1，就可以得到需要遍历的元素个数的平均值，即： $$ \frac{1+2+3+…+n+n}{n+1} = \frac{n(n + 3)}{2(n + 1)} $$ 我们知道，时间复杂度大O标记法中，可以省略掉系数、低阶、常量，所以上面的时间复杂度为O(n)。 这个结论虽然是正确的，但是计算过程稍微有点问题。我们刚讲的这n+1中情况，出现的概率并不一样。下面结合概率论的知识分析一下。 我们知道，要查找的变量x，要么在数组中，要么不再数组中，我们假设这两个概率分布为$\frac{1}{2}$。 不在数组中时，时间复杂度为: $n\times\frac{1}{2}$; 在数组中时，因为数组大小为n，出现在任何一个位置的可能性都是一样的，所以每个位置的概率就是:$\frac{1}{2n}$, 因此在数组中时的时间复杂度为：$(1+2+3+…+n)\times\frac{1}{2n} $。 那平均时间复杂度就是：$(1+2+3+…+n)\times\frac{1}{2n} + n\times\frac{1}{2} = \frac{3n+1}{4} = O(n)$。 这个值就是概率论中的加权平均值，也叫做期望值，所以平均时间复杂度也叫做加权平均时间复杂度或者期望时间复杂度。 实际上，在大多情况下我们并不需要区分最好、最坏、平均时间复杂度三种情况，很多时候我们只用一个复杂度就可以满足需求了。只有同一代码在不同的情况下，时间复杂度有量级的差距，我们才会使用三种复杂度表示法来区分。 均摊时间复杂度目前为止，我们应该已经掌握了算法复杂度分析的大部分内容了，下面来认识一个更高级的概念：均摊时间复杂度，以及它对应的分析方法摊还分析。 均摊时间复杂度听起来跟平均时间复杂度有点像，对于初学者来说，这两个概念很容易弄混。前面说过，大部分情况下不需要区分最好、最坏、平均时间复杂度，只有某些特殊情况才需要平均时间复杂度，而均摊时间复杂度比它的应用场景比它更特殊、更有限。 还是以一个例子来说明(别太在意例子，只是为了说明)： 1234567891011121314151617int[] arr = new int[n];int size = 0；void insert(int val)&#123; // 如果数组满了 if(count == arr.length)&#123; int sum = 0; for(int i=0; i&lt;arr.length;i++)&#123; sum = sum + arr[i]; &#125; arr[0] = sum; count = 1; &#125; // 数组赋值 arr[count] = val; ++count;&#125; 先简单解释一下这段代码的功能，这段代码实现了一个往数组中插入数据的功能，如果数组有空闲空间，直接插入即可。如果数组满了，将数组中的数据求和，清空数组，将求和之后的数据放入数组的第一个位置，然后再将新的数据插入。 那这段代码的时间复杂度是多少呢？我们可以先利用上面讲的三种分析方法来分析一下。 最理想情况下，数组有空闲空间，直接插入数据就可以，所以最好时间复杂度为O(1)；最坏情况下，数组中没有空闲空间了，我们需要先进行一次数组遍历求和，在做数据插入，所以最坏情况时间复杂度为O(n)；平均情况时间复杂度，我们还是用概率论的方法来分析，假设数组长度为n，根据插入位置不同，可以分为n种情况，每种情况的时间复杂度为O(1)，另外还有一种特殊情况，就是数组没有空闲时间时，时间复杂度为O(n)，而且这n+1中情况出现的概率是一样的，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 我们来比较一下这个例子中insert函数和上面findArray的不同。首先，findArray在极端情况下，复杂度才为O(1)，大部分情况都为O(n)，而insert函数大部分情况时间复杂度都为O(1)，只有特殊情况时间复杂度才为O(n)，这是第一个区别。第二个不同的地方，对于insert函数来说，O(1)和O(n)的时间复杂度出现的频率是非常有规律的，而且有一定的时序关系，一般都是一个O(n)插入之后，跟n-1个O(1)的插入操作，循环往复。 针对这样一种情况，我们并不需要像平均复杂度分析那样，计算所有输入情况和发生的概率，计算加权平均值。 我们引入一种更加简单的分析方法：摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字叫：摊还时间复杂度。 那么究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？ 我们还是以这个insert函数为例，每一次O(n)的插入操作，后面都会跟n-1次O(1)插入操作，所以我们把耗时最多的操作均摊到n-1次耗时少的操作上，均摊下来，这一组连续操作的均摊时间复杂度就为O(1)，这就是均摊分析法的大致思路。 均摊时间复杂度和摊还分析应用场景比较特殊，所以不会经常用到，这里简单总结一下他们的应用场景。 对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块分析，看看是否能将时间复杂度高的操作，均摊到其他时间复杂度低的操作上。在一般的能运用均摊时间复杂度的场景中，均摊时间复杂度是等于最好时间复杂度的。 思考题：根据今天学习的几个复杂度分析的方法，来分析一下下面这个add()函数的时间复杂度。 1234567891011121314151617181920int[] arr = new int[10];int len = 10;int i=0;void add(int element)&#123; // 数组空间满了 if(i&gt;=len)&#123; // 数组扩容 int new_arr = new int[len*2]; // 把数组拷贝到新数组 for(int j=0; i&lt;len; j++)&#123; new_arr[j] = arr[j]; &#125; arr = new_arr; len = len*2; &#125; // 添加到数组中 arr[i] = element; ++i;&#125; 在最理想情况下，数组中有空闲空间，可以直接添加到数组中，时间复杂度为O(1)；最坏情况下，数组中没有空闲空间，先进行一次扩容操作，在进行遍历给新数组赋值，时间复杂度为O(n)，所以最坏时间复杂度为O(n)。 平均时间复杂度，可以分为有空闲空间和没有空闲空间两种，有空间空间有n中情况，所以每种情况出现的概率为$\frac{1}{n+1}$，所以根据加权平均的计算方法，求得平均时间复杂度为：$ 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} + 1\times\frac{1}{n+1} +….+ 1\times\frac{1}{n+1} + n\times\frac{1}{n+1} = O(1) $。 均摊时间复杂度，可以看出本例是符合均摊时间复杂度的场景的，在一次O(n)时间复杂度操作后都会跟n-1次O(1)时间复杂度操作，所以将O(n)时间复杂度的操作均摊到n-1次O(1)时间复杂度操作上，最终均摊时间复杂度为O(1)。 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-复杂度分析]]></title>
    <url>%2Fposts%2F2018-09-08-%E7%AE%97%E6%B3%95-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[(adsbygoogle = window.adsbygoogle || []).push({}); 前言 我们都知道，数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行的更快、更省存储空间。那如何来衡量算法的“快”和“省”呢？这就要用到复杂度分析：时间、空间复杂度分析。复杂度分析是整个算法学习的精髓，掌握了它，数据结构和算法的内容基本就掌握了一半。 为什么需要复杂度分析 有人说，我只要把代码跑一遍，通过统计、监控，就可以得到算法执行的时间和占用的那内存，为什么还要做复杂度分析呢？ 1、首先，这种评估方法确实是准确的，但是这种方法是”事后统计法”，是有非常大的局限性。 2、测试结果非常依赖测试环境，同样一段代码，在不同的CPU可能执行的时间会差很多，比如Intel Core i9就比i3运行的快，同样在不同的两台机器上也可能会出现代码执行不一样的情况。 3、对于不同的数据集，如果数据的有序程度不一样，那么对数据进行同一种算法运算，也可能会得到不同的结果。除此之外，数据规模的大小也可能对算法产生影响。 因此我们需要一个不用具体的测试数据来测试，就可以粗略估计算法的执行效率的方法，这就是时间、空间复杂度分析所解决的问题。 大O复杂度表示法 算法的执行效率，粗略的讲，就是算法执行的时间，但是如何能在不运行的情况下，得到一段代码的运行时间呢？ 这里举一个简单的例子，求解1，2，3……n 的累加和，以下为一个简单的代码实现： 1234567int sum(int n)&#123; int sum = 0; for (int i=1; i&lt;=n; i++)&#123; sum += i; &#125; return sum;&#125; 从CPU的角度看，每一行代码都执行着类似的操作：读数据-运算-写数据。尽管每行代码对应的CPU执行个数、执行时间都不尽相同，但是我们只是粗略的估计，因此这里假设每行代码执行的时间都相同，那么在此基础上，这段代码执行的时间可以进行如下计算： 第二行代码执行时间为time，第三、四行代码执行的时间为 $ 2 \times n \times time $，所以此段代码执行的时间为$ (2n + 1)\times time $ ，可以看出这段代码执行时间T(n)与每行代码的执行次数成正比。 按照这个思路，再对以下代码段进行分析： 12345678int sum(int n)&#123; int sum = 0; for(int i=1; i &lt;= n; i++)&#123; for(int j=1; j &lt;= n; j++)&#123; sum += i*j; &#125; &#125;&#125; 假设每行代码执行的时间依然为time，那么这段代码执行的时间是多少呢？ 第二行代码的执行时间依然为time，第三行代码执行的次数为n次，所以需要的时间为$ n*time $,内层循环第四、五行代码都执行了$ n*n $次,需要的时间为$ 2*n^2*time $。所以此段代码总的执行时间为$(n + 1 + 2n^2)*time $。 尽管不知道time的具体值，但是通过这两段代码的分析过程，得出一个非常重要的规律： 所有的代码执行时间T(n)与每行代码的执行次数成正比$$ T(n) = O(f(n)) $$ 其中 $T(n)$ 表示代码执行的时间; n表示数据规模大小; $ f(n) $ 表示每行代码执行次数的总和，因为是一个公式，所以用$ f(n) $ 表示。公式中的O表示代码执行时间 $ T(n) $ 与 $ f(n) $ 成正比。 所以在第一个例子中 $ T(n) = O(2n + 1) $ ，第二个例子中 $ T(n) = O(2n^2 + n + 1)$ , 这就是大O时间复杂度表示法。大O时间复杂度实际上并不具体表示代码真正执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，所以也叫做渐进时间复杂度，简称时间复杂度。 在时间复杂度公式中，如果n很大时，公式中的低阶、常量、系数三部分并不影响增长趋势，所以可以先忽略。所以上述两个例子的时间复杂度就可以记为： $ T(n) = O(n) $； $ T(n) = O(n^2) $; 时间复杂度分析 前面介绍了大 O 时间复杂度的由来和表示方法，那如何分析一段代码的时间复杂度呢？ 1、只关注循环次数最多的一段代码在大 O 表示法中，只是表示一种趋势，通常我们会忽略公式中的常量、低阶、系数，因此只需要记录一个最大的量级就可以了，所以我们在分析一个算法时，只关注循环次数执行次数最多的那一段代码就行了。 2、加法法则：总复杂度等于量级最大的那段代码的复杂度如果一段代码中出现多个循环，那么总的时间复杂度就是各个循环相加得到的，但是往往会忽略低阶、常量，因此只取量级最大的那段代码就可以了。 注意：当一段代码循环次数是一个常量，比如循环10000、1000000次，只要是一个已知的常量数，且不随数据规模变化，那么该循环照样是一个常量级别的执行时间。 3、乘法法则: 嵌套代码的时间复杂度等于嵌套内外代码复杂度的乘积比如第二个例子中如果但看外层循环的时间复杂度是 $ O(n) $；内层循环的时间复杂度也是 $O(n)$， 因此总共的时间复杂度就是 $ T(n) = O(n) * O(n) = O(n^2) $ 几种常见时间复杂度 1、$O(1)$O(1) 只是常量级时间复杂度的一种表示方法，并不是指执行了一行代码。只要代码的执行时间不随n的增大而增大，这样的代码时间复杂度都可以记为O(1)。一般情况下，只要代码中不出现循环、递归等，即使有成千上万行代码，时间复杂度也是O(1)。 2、$ O(logN)、O(N*logN) $对数阶的时间复杂度非常常见，同时也是最难分析的一种。 1234int i = 1;while(i &lt;= n)&#123; i = i * 2;&#125; 在上述代码中，变量i从1取值，第二次为2，第三次为4，第四次为8……,所以i的取值规律为 $$ 2^0 \&nbsp;&nbsp;&nbsp;&nbsp; 2^1 \&nbsp;&nbsp; 2^2 \&nbsp;&nbsp; 2^3 … 2^k… 2^x $$ 当$2^x = n$ 时，循环结束，而循环的次数即为x，所以时间复杂度也为$ O(x=\log_2 N) $。 如果把代码改为如下。那时间复杂度是多少呢？ 1234int i = 1;while(i &lt;= n)&#123; i = i * 3;&#125; 根据上面的思路，很容易看出这段代码的时间复杂度为$ O(log_3N) $ 。 实际上，不管是以2为底，还是以3为底，亦或是以10为底，我们都把对数阶的时间复杂度记为$ O(logN) $，为什么呢？ 我们知道对数之间是可以互相转化的，$ log_3n$ 就可以转换为$ log_32*log_2N $，所以$ O(log_32) = O(C * log_2N) $，其中$ C = log_32 $ 是一个常量，基于前面的结论： 在采用大O标记复杂度的时候，可以忽略系数，即$ O(C*f(n)) = O(f(n)) $。因此在对数阶时间复杂度的表示方法里，我们忽略的底，统一表示为$O(logN)$。 如果理解了$O(logN)$，那么$O(nlogN)$就很容易了，根据前面所说的乘法法则，如果一段代码的时间复杂度是$O(logN)$，如果循环执行了 n 次，那么该代码的时间复杂度就是$O(nlogN)$。而且$O(nlogN)$是一种非常常见的时间复杂度，归并排序、快速排序的时间复杂度都是$O(nlogN)$。 2、$ O(m+n)、O(m*n) $我们再来讲跟前面都不一样的时间复杂度，代码的时间复杂度由两个数据规模来决定。 123456789101112int func(int m, int n)&#123; int sum1 = 0; for(int i=1; i&lt;=m; i++)&#123; sum1 += i; &#125; int sum1 = 0; for(int j=1; j&lt;=m; j++)&#123; sum1 += j; &#125; return sum1+sum2;&#125; 从代码中看出，m和n表示两个不同的数据规模，我们无法事先评估m和n的量级大小，所以我们在分析复杂度时，就不能简单用加法法则忽略一个，因此上面代码的时间复杂度为$O(m + n)$， 针对这种情况，加法原则就不正确了，我们将加法原则改为：$ T1(m) + T2(n) = O(f(m) + g(n)) $，但是乘法法则继续有效：$ T1(m) + T2(n) = O(f(m) * f(n)) $。 空间复杂度 前面讲过，时间复杂度的全称是渐近时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度的全称就是渐进空间复杂度，表示算法的存储空间与数据规模的增长关系。 还是拿具体的例子说明(仅供测试,一般没人这么写) 12345678void func(int n)&#123; int i = 0; int[] a = new int[n]; for(i; i&lt;n; i++)&#123; a[i] = i*1; print(a[i]); &#125;&#125; 和分析时间复杂度一样，我们看到第二行申请了一个空间变量i，但是它是常量阶的，跟数据规模n无关，所以可以忽略，第三行申请了一个大小为n的int数组，除此之外，该代码没有占据更多的空间O(n). 我们常见的空间复杂度就是$O(1)、O(n)、O(n^2)$，像$ O(logN)、O(nlogN) $ 这样的对数阶复杂度平时都用不到。空间复杂度分析相对时间复杂度要简单得多。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>复杂度分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-安装及配置]]></title>
    <url>%2Fposts%2F2018-09-07-hexo-%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[前言曾几何时，你是否也想有个自己的博客，抒发自己的心情，总结自己的得失，与人分享喜悦、哀伤、愤怒、忧愁，那么这篇文章你就必须看了，非常简单搭建一个自己的开源博客。 (adsbygoogle = window.adsbygoogle || []).push({}); 一、预备1、安装Nodejs及npm Nodejs下载地址： 官网下载地址：https://nodejs.org/zh-cn/download/ 2、安装Git Git下载地址： 官网下载地址：https://git-scm.com/download/ 安装完成后，执行如下命令，可以显示版本号就算安装成功了 12345678$ node -vv9.11.1$ npm -v6.3.0$ git --versiongit version 2.17.0.windows.1 二、安装hexo进入命令行，执行如下命令: 1234567891011121、全局安装hexo$ npm install hexo -g2、创建hexo工作目录$ mkdir hexo-blog$ cd hexo-blog3、初始化工作目录$ hexo init4、本地启动hexo$ hexo serve 到此一个hexo博客已经搭建完成了，可以访问 http://localhost:4000/ 查看博客的效果。 当然现在你就可以开始写博客了，默认的配置足够你写作、发表文章了，但是默认的东西有些并不符合自己的要求和审美。所以下面对hexo进行一些配置，以符合自己的要求。 三、hexo配置hexo的配置文件在根目录下_config.yml文件中。本文仅列举几项，其余配置可以参照hexo官网文档进行配置，当然，有兴趣可以参照我的配置 网站配置：12345678# Sitetitle: Aries' blog 网站标题subtitle: 副标题description: 我不生产知识，我只是知识的搬运工。 网站一句话描述keywords: 关键词author: 无名万物 作者language: zh-CN 语言timezone: Asia/Shanghai 时区 文章配置：1234url: http://blog.renhj.org 网站urlroot: / 文章根路径permalink: posts/:year-:month-:day-:title.html 文章urlpermalink_defaults: 四、创建新文章你可以通过以下命令来创建一篇新文章1hexo new [layout] &lt;title&gt; 命令中指令文章的布局，默认为post，可以通过修改_config.yml中的default_layout来修改默认布局，当然也可以在文章Front-Matter上添加布局. 当然也可以新建一个草稿： draft，这种布局在建立时会保存到source/_drafts文件夹，也可以通过publish来将草稿移动到正式文件夹。 12345# 新建草稿文章$ hexo new draft &lt;title&gt;# 将文章正式发布$ hexo publish [layout] &lt;title&gt; Front-matter Front-matter是文章最上方以--- 分割的区域，用于指定个别文件的变量 12345678910---layout: 指定文章的布局属性title： 文章标题data：建立日期updated： 更新日期comments： 是否开启文章的评论功能(如果有的话)tags： 标签categories：分类permalink： 覆盖文章的网址--- 修改美化默认的主题是有点丑，可以去hexo的主题商店 找一个自己喜欢的、漂亮的主题。 本人找的是网上比较流行的nexT的主题，即本博客所使用的主题：hexo nexT主题，更多的配置可以参照nexT官网的配置或者其他文章进行配置。本文就不再这里赘述的，具体效果可以看本博客的。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>nexT</tag>
        <tag>Github Pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Markdown来写文章]]></title>
    <url>%2Fposts%2F2018-09-06-%E7%94%A8Markdown%E6%9D%A5%E5%86%99%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[MarkdownMarkdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成丰富的HTML页面。 Markdown用一些简单的符号标识不同的标题，将某些文章标记为”粗体“或者斜体，下面就来一起学习一下。 语法1、标题 不同的标题采用不等个数的#号来进行标记，如下所示： 1234# 一级标题## 二级标题### 三级标题#### 四级标题 2、代码块 在需要高亮的代码块的前一行及后一行使用三个反引号“`”，同时第一行反引号后面表面代码块所使用的语言, 如下： ```pyhtonprint (“Hello World!”)``` 3、特殊字符 123**粗体***斜体*&gt; 引用内容 (adsbygoogle = window.adsbygoogle || []).push({});]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
